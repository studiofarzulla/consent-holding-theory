# Consent-Holding: An Axiomatic Framework for Legitimacy, Friction, and Political Stability

## Abstract

This paper develops a unified analytical framework for measuring political legitimacy across heterogeneous governance domains. Building on insights from constitutional political economy, social choice theory, and institutional analysis, the framework establishes consent-holding—the mapping from decision domains to those with authority over them—as a structural necessity of collective action. We formalize this intuition through five axioms and five theorems, demonstrating that legitimacy can be operationalized as stakes-weighted consent alignment α(d,t), while friction F(d,t) measures the deviation between outcomes and stakeholder preferences. The framework bridges normative democratic theory and empirical prediction, generating testable hypotheses about institutional stability. Historical validation examines suffrage expansion, abolition movements, labor rights, and contemporary platform governance, demonstrating how misalignment between stakes and voice generates observable instability. Unlike existing approaches that prescribe ideal institutions, this framework provides analytical tools for measuring legitimacy within any governance structure, enabling systematic comparison across democratic, technocratic, and algorithmic systems. Computational validation through Monte Carlo simulation confirms that stakes-weighted consent mechanisms minimize friction while maintaining performance. The framework's domain-specific approach resolves the apparent tension between consent and competence, showing both as complementary dimensions of institutional legitimacy.

**Keywords**: legitimacy, consent, political stability, social choice, institutional design, friction, stakes-weighting

**JEL Codes**: D70 (Social Choice), D71 (Social Choice; Clubs; Committees; Associations), P16 (Political Economy)

---

## 1. Introduction

Political legitimacy presents a fundamental puzzle: how can we measure whether authority is rightfully held across radically different governance domains? A state legislature, corporate board, algorithmic content moderation system, and common-pool resource management regime all make consequential decisions affecting stakeholders, yet existing frameworks struggle to provide unified analytical tools for assessing their legitimacy. Democratic theory emphasizes popular sovereignty (Rawls, 1971; Habermas, 1984), public choice highlights constitutional constraints (Buchanan & Tullock, 1962), while recent work on algorithmic governance introduces new challenges to consent-based legitimacy (Grimmelikhuijsen et al., 2022). What remains elusive is a framework capable of both normative evaluation and empirical prediction that applies consistently across domains.

This paper addresses this gap by developing consent-holding theory, an axiomatic framework that treats legitimacy as a structural property of decision-making systems rather than a binary classification. The central insight is deceptively simple yet powerful: in any domain where collective decisions produce shared consequences, someone must hold the authority to decide. This consent-holder mapping H_t(d)—identifying who decides in domain d at time t—is not a normative choice but a logical necessity arising from the structure of collective action itself. The framework's contribution lies not in prescribing who should hold consent, but in providing rigorous tools for measuring the consequences of any particular allocation.

The framework makes three distinct contributions to political theory and institutional analysis. First, it establishes a formal connection between consent alignment and observable political friction. While democratic theorists have long argued that excluding affected stakeholders undermines legitimacy (Dahl, 1989; Estlund, 2008), existing accounts lack operational metrics for testing these claims. We define consent alignment α(d,t) as the stakes-weighted share of decision power held by affected parties, and friction F(d,t) as the stakes-weighted deviation between outcomes and stakeholder preferences. The framework predicts that persistent misalignment generates measurable instability—protests, non-compliance, institutional breakdown—making legitimacy empirically falsifiable rather than purely philosophical.

Second, the framework resolves the apparent tension between consent and competence through a competence-consent trade-off theorem (T4). Epistemic democrats argue that inclusive decision-making produces better outcomes through cognitive diversity (Landemore, 2013; Hong & Page, 2004), while critics worry that expanding consent sacrifices technical expertise (Brennan, 2016). Our framework shows these concerns reflect different positions on the legitimacy frontier: some domains optimally weight performance highly (nuclear safety, pandemic response), while others prioritize consent alignment (constitutional amendments, community norms). Rather than declaring one approach universally superior, the framework provides tools for identifying domain-appropriate balances.

Third, this approach enables systematic historical and comparative analysis. By operationalizing legitimacy as α(d,t) and friction as F(d,t), we can trace institutional evolution quantitatively. Franchise expansion emerges not as discrete events but as gradual increases in α(d) driven by the accumulating friction F(d) from excluding high-stakes populations. Women's suffrage movements, abolition struggles, labor organizing, and contemporary platform governance rebellions all exhibit the same underlying dynamic: groups with high stakes sᵢ(d) but zero consent power Cᵢ generate sustained friction until incorporation or suppression occurs. This pattern, predicted by the framework's core theorems, provides empirical validation across centuries and continents.

The framework proceeds from seven minimal axioms about collective decision-making (Section 2) to five theorems establishing structural necessities (Section 3). Theorem 1 demonstrates consent-holding necessity: wherever decisions occur, some mapping H_t(d) must exist. Theorem 2 establishes inevitable friction: plural preferences guarantee that someone's interests will be compromised unless perfect alignment obtains. Theorem 3 defines legitimacy as stakes-weighted consent alignment, providing an operational metric. Theorem 4 formalizes the competence-consent trade-off, showing legitimacy as a weighted combination L = w₁·α + w₂·P. Theorem 5 derives a minimal absolutism from value relativism: even if content-level values are frame-dependent, the existence of consent-holding structures remains invariant.

Section 4 situates this framework within nine research traditions—constitutional political economy (Buchanan & Tullock, 1962), social choice theory (Arrow, 1951; Sen, 2017), stakeholder theory (Freeman, 1984), common-pool resource governance (Ostrom, 1990), deliberative democracy (Fishkin, 2018; Habermas, 1990), algorithmic governance (Barocas et al., 2019), epistemic democracy (Estlund, 2008; Landemore, 2013), relational autonomy (Mackenzie, 2014), and legitimacy theory (Scharpf, 1999; Schmidt, 2013). Rather than competing with these approaches, consent-holding theory provides a unifying analytical architecture: each tradition contributes insights about how consent should be allocated or what constitutes legitimate use of authority, while our framework offers measurement tools applicable regardless of normative commitments.

Section 5 operationalizes the framework for empirical application, specifying proxy variables for consent power Cᵢ (voting weights, agenda control, board representation), stakes sᵢ(d) (exposure measures, capability impacts, revealed preferences), and friction F(d) (protest incidence, litigation rates, policy reversals). This operationalization enables econometric identification strategies using panel data with institutional variation as instruments for consent alignment. Monte Carlo simulations (Section 6) validate the framework's predictions: stakes-weighted mechanisms minimize friction while maintaining aggregate welfare under diverse preference distributions.

Historical validation (Section 7) examines six cases spanning two centuries: women's suffrage (1890s-1970s), abolition (1780s-1860s), labor rights (1850s-1930s), civil rights (1950s-present), LGBT inclusion (1969-present), and platform governance (2010s-present). Each case demonstrates the predicted pattern: high sᵢ(d) combined with zero Cᵢ generates rising F(d) until elites respond through suppression or incorporation. When α(d) rises sufficiently, F(d) collapses; when suppression succeeds temporarily, F(d) accumulates until system breakdown. This regularity across radically different contexts—political enfranchisement, economic organization, identity-based movements, digital governance—suggests the framework captures fundamental dynamics of institutional legitimacy.

Section 8 addresses seven major objections, from concerns about infinite regress in consent structures to worries that stakes-weighting enables plutocracy. Each objection is addressed through clarifying the framework's scope: we describe consent-holding's structure and consequences, not prescribe optimal allocations (a task for normative theory informed by these tools). Section 9 concludes by outlining the research agenda this framework enables: cross-national legitimacy indices, institutional experiments varying α(d) systematically, and applications to emerging domains (AI governance, climate policy, platform regulation) where consent structures remain contested.

The framework's title—"consent-holding" rather than "consent theory"—reflects its analytical focus. This is not another account of why consent matters normatively, but a systematic investigation of how consent operates structurally. Just as markets emerge from property rights and contracts regardless of normative justifications for capitalism, consent-holding structures emerge from the necessity of collective decision-making regardless of democratic commitments. The framework's power lies in making these structures visible, measurable, and comparable, enabling rigorous analysis of legitimacy claims that have historically remained philosophically contested but empirically elusive.

---

## 2. Literature Review and Theoretical Foundations

The consent-holding framework synthesizes and extends insights from nine distinct research traditions. This section reviews each tradition's core contributions, identifies limitations the framework addresses, and demonstrates how operationalizing legitimacy as α(d,t) and friction as F(d,t) enables empirical validation of long-standing theoretical claims.

### 2.1 Constitutional Political Economy

Buchanan and Tullock's (1962) seminal work establishes constitutional choice as a distinct analytical problem requiring different decision rules than ordinary politics. Their framework rests on several foundational insights that anticipate the consent-holding approach. First, they distinguish between constitutional rules—rarely changed frameworks establishing decision procedures—and political decisions made within those rules. This maps directly onto our concept of nested consent-holding: H_t(d_meta) represents the consent-holders for constitutional domains, while H_t(d) operates within constraints established at the meta-level. Second, Buchanan and Tullock argue that rational agents behind a "veil of uncertainty" would unanimously consent to rules benefiting all. Once constitutional structures are established, majority rule becomes acceptable for routine decisions. This anticipates our Theorem 1: consent-holding exists at every level, from object-level policy to constitutional design to amendment procedures. Third, their exchange paradigm treats politics as mutual exchange of consent rather than top-down command. Government achieves legitimacy when citizens "purchase" its services consensually through constitutional agreement. The consent-holding framework formalizes this metaphor rigorously through stakes-weighted alignment metrics. Finally, Buchanan and Tullock model optimal decision rules as minimizing total costs combining external costs (harm from decisions affecting you without your consent) and decision costs (time and effort required to reach agreement). Our friction metric F(d) captures external costs precisely as stakes-weighted deviations from stakeholder ideal points.

The framework extends Buchanan and Tullock in four crucial respects. First, we introduce stakes-weighting sᵢ(d), recognizing that individuals are heterogeneously affected by policies. Buchanan assumes rough equality of interests at the constitutional stage; we model variable impacts explicitly, enabling analysis of distributive conflicts. Second, while Buchanan focuses on one-time constitutional founding moments, we model consent-holding as continuously operating through H_t(d), tracking legitimacy dynamically as institutional configurations evolve. Third, Buchanan discusses "the" social contract; we specify that consent-holding varies across domains d, with different optimal structures for taxation, criminal justice, environmental regulation, and community norms. Fourth, Buchanan provides normative theory; we operationalize concepts through α(d,t) and F(d,t), enabling empirical validation of constitutional designs rather than purely philosophical justification.

This operationalization also addresses standard critiques of constitutional political economy. Critics argue that Buchanan's unanimity requirement is unrealistic—actual constitutional processes rarely achieve consensus. The framework accommodates this by allowing F(d) > 0 even in legitimate systems; perfect consent alignment is impossible given plural preferences (Theorem 2). Similarly, concerns about status quo bias—that unanimous consent rules entrench existing distributions—are addressed through stakes-weighting: those harmed by the status quo possess high sᵢ(d) in reform domains, raising the stakes for maintaining dysfunctional arrangements. Finally, objections about unequal bargaining power map onto explicit modeling of Cᵢ, allowing analysis of how power asymmetries affect legitimacy rather than assuming them away.

### 2.2 Social Choice Theory and Impossibility Results

Arrow's (1951) impossibility theorem establishes that no ranked voting system can simultaneously satisfy four seemingly minimal desiderata: Pareto efficiency, non-dictatorship, independence of irrelevant alternatives, and unrestricted domain. This result demonstrates that perfect democratic aggregation is mathematically impossible, not merely practically difficult. The Gibbard-Satterthwaite theorem (Gibbard, 1973; Satterthwaite, 1975) extends this impossibility to strategy-proofness: any non-dictatorial voting mechanism over three or more alternatives is manipulable. Recent quantitative versions show these aren't merely theoretical concerns but quantifiably common: Keller (2012) demonstrates that any voting rule far from dictatorship has significant probability of generating intransitive collective preferences, while Mossel et al. (2012) show that random manipulation succeeds with non-negligible probability under realistic conditions.

Sen's (2017) expanded treatment of collective choice integrates economics and ethics, introducing the capability approach that maps directly onto our effective voice concept. Sen argues that development should be measured not by utility or resources alone but by capabilities—freedoms to achieve valued functionings like health, education, and political participation. This provides theoretical grounding for our eff_voiceᵢ(d) term: possessing formal consent power Cᵢ > 0 without resources, education, or political freedom represents low capability. Sen's information basis of judgment—what variables enter the social welfare function—validates the framework's emphasis on measurement. Different stake conceptions (material exposure vs. capability impact vs. existential threat) reflect different information bases, each legitimate in appropriate domains.

The consent-holding framework relates to social choice theory as meta-analysis rather than competitor. Where Arrow and Gibbard-Satterthwaite ask "which aggregation rule is best?", we ask "how legitimate is any given aggregation rule?" This shift has three implications. First, our framework doesn't compete with impossibility results; it builds on them by providing tools for measuring consequences of unavoidable trade-offs. Since perfect rules don't exist, we need metrics for comparing imperfect options. Second, stakes-weighting sᵢ(d) isn't present in classical social choice theory, which typically assumes equal weights. This extension allows domain-specific analysis: simple majority may be optimal for low-stakes routine legislation, while supermajority or even consensus becomes appropriate when stakes concentrate heavily. Third, the framework generates empirical predictions about friction F(d) under different rules, making legitimacy testable rather than purely axiomatic.

This approach also addresses strategic manipulation concerns raised by Gibbard-Satterthwaite. If stakes are self-reported, agents can strategically inflate sᵢ(d) to capture disproportionate Cᵢ—the plutocracy objection addressed in Section 8. Our response employs revealed preference: measure stakes through behavioral and material proxies (tax exposure relative to capability, health outcomes affected, land threatened) rather than claims. A billionaire cannot falsely claim housing insecurity from tax policy; revealed consumption patterns contradict it. Additionally, friction F(d) provides empirical falsification: if claimed high α(d) still generates high observed friction (protests, non-compliance, instability), stakes were misweighted. The system self-corrects through observable political dynamics.

### 2.3 Epistemic Democracy and the Competence-Consent Tension

Estlund (2008) advances epistemic proceduralism: democracy is justified both procedurally (fairness through equal voice) and epistemically (truth-tracking through collective judgment). He argues against pure proceduralism—if only fairness mattered, coin flips would suffice—while rejecting epistocracy on two grounds. First, no uncontroversial method exists for identifying political experts; any proposed test remains contestable. Second, concentrating power in supposed experts lacks democratic legitimacy even if they govern competently. This sets up the central puzzle: how can we honor both epistemic quality (good decisions) and democratic legitimacy (consent-based authority)?

Landemore (2013) resolves this tension through cognitive diversity: groups of diverse problem-solvers outperform homogeneous high-ability groups because diversity prevents local optima traps (Hong & Page, 2004). Inclusive democracy beats technocratic elites not despite but because of diversity. Different stakeholders bring distinct heuristics, perspectives, and information. Excluding them doesn't just violate democratic norms; it loses epistemic resources. Anderson (2006) extends this social epistemology framework, arguing that democracy enables learning through feedback, experimentation, and distributed knowledge under radical uncertainty. Citizens possess local knowledge experts lack; equal participation empowers diverse perspectives to correct errors.

The consent-holding framework operationalizes these insights through Theorem 4, the competence-consent trade-off. We define legitimacy as L(d,t) = w₁·α(d,t) + w₂·P(d,t), where α represents consent alignment, P represents performance/competence, and w₁, w₂ reflect society-specific weights. This formulation makes three contributions to epistemic democracy debates. First, it shows the consent-competence distinction isn't dichotomous but reflects positions on a legitimacy frontier. Some domains rationally prioritize competence (nuclear safety, pandemic response) while others prioritize consent (community norms, constitutional values). Rather than declaring one universally superior, the framework provides tools for domain-appropriate balance.

Second, friction F(d) provides epistemic feedback. High friction signals that stakeholders resist outcomes harming them—a valuable information source about policy errors. Estlund and Anderson argue abstractly that democracy tracks truth; we specify the mechanism: stakes ensure epistemic motivation. Agents with high sᵢ(d) possess strong incentives to contribute accurate information about consequences. Excluding them doesn't just violate consent; it loses crucial knowledge about likely impacts.

Third, the framework generates testable predictions connecting consent alignment to performance outcomes. If epistemic democrats are correct that inclusion improves decisions through cognitive diversity, we should observe:
H1: Higher α(d) correlates with lower policy error rates (controlling for domain difficulty)
H2: Cognitive diversity (measured as variance in sᵢ perspectives) predicts better outcomes
H3: Friction F(d) at time t predicts policy reversals at t+1 (mistakes generating backlash)

Empirical evidence from participatory budgeting, worker board representation, and citizens' assemblies supports these predictions. Fauver and Fuerst (2011) show German codetermination—50% worker board representation—correlates with sustained long-term value creation despite higher short-term volatility. This validates that broader α(d) can improve performance, not merely satisfy fairness norms. Fishkin's (2018) deliberative polling studies demonstrate that informed deliberation shifts preferences toward expert consensus while maintaining legitimacy—raising both α (participants feel heard) and P (outcomes improve).

### 2.4 Stakeholder Theory and Corporate Governance

Freeman's (1984) stakeholder approach argues that firms should create value for all stakeholders—employees, suppliers, communities, customers, shareholders—not just maximize shareholder returns. This challenges Milton Friedman's (1970) shareholder primacy doctrine, which treats profit maximization as the sole corporate responsibility. Freeman's position rests on both normative and empirical claims: business exists for human flourishing beyond profit, and long-term success requires attending to all affected parties. The 2019 Business Roundtable statement endorsing stakeholder capitalism, signed by 200 CEOs, marks mainstream acceptance of this view.

The framework operationalizes Freeman's insights by defining stakeholders precisely as agents with sᵢ(d) > 0 in corporate domains. Current governance structures grant consent power almost exclusively to shareholders: they elect boards, approve major transactions, and receive residual claims. Employees, despite high stakes in employment security, working conditions, and workplace norms, hold negligible Cᵢ in most Anglo-American firms. This generates low α(d_corporate) when stakes are calculated comprehensively. The framework predicts such misalignment produces friction F(d): labor disputes, regulatory pressures, reputation damage, difficulty attracting talent. Freeman intuits this; we quantify it.

Comparative corporate governance research validates these predictions. Vitols (2011) documents how German codetermination—mandatory worker representation on supervisory boards—constrains hostile takeovers and maintains stakeholder orientation. Workers' voice (high α_workers(d)) prevents zero-sum shareholder maximization strategies. Bosch and Weinkopf (2013) show codetermined firms invest more in worker training and career development; higher α produces performance improvements in human capital domains. Doyle's (2020) comprehensive study of Nordic worker-board representation demonstrates sustained profitability alongside low inequality—direct validation of consent-holding predictions that higher α need not sacrifice performance.

These empirical regularities suggest that stakeholder exclusion represents not merely an ethical failure but an institutional inefficiency. When workers possess high sᵢ(d_workplace) but zero Cᵢ, firms lose access to distributed knowledge about operational improvements, safety hazards, and productivity bottlenecks. Goranova and Ryan (2014) document that when excluded stakeholders' α is low, they pursue external activism—shareholder resolutions, public campaigns, regulatory lobbying—to force voice. This validates the F(d) → instability prediction: low consent alignment generates friction channeled through available mechanisms.

The framework also clarifies debates about stakeholder weighting. Critics worry that "balancing" stakeholder interests provides no operational guidance—how should boards trade off worker wages against customer prices? The consent-holding approach suggests measuring stakes empirically: who is most affected by each decision? For workplace safety domains, workers have extreme sᵢ (health at risk); shareholders have lower sᵢ (indirect via productivity). For capital allocation decisions, shareholders may have higher sᵢ; for community environmental impacts, local residents dominate. Domain-specific H_t(d) mappings can then be designed to weight voice by measured stakes rather than assuming uniform stakeholder influence.

### 2.5 Common-Pool Resource Governance

Ostrom's (1990) groundbreaking work on common-pool resources challenges both "tragedy of the commons" pessimism and top-down state solutions. Through field studies of fisheries, forests, irrigation systems, and groundwater basins across continents, she demonstrates that resource users frequently develop effective self-governance without privatization or centralized authority. Her eight design principles for successful commons management include particularly relevant insights for consent-holding theory. Design Principle 3 requires that "most individuals affected by the operational rules can participate in modifying the operational rules"—essentially mandating high α(d_rules) for those with high sᵢ(d_resources). Design Principle 8 specifies nested enterprises for larger systems, enabling polycentric governance with consent-holding at multiple scales.

The framework formalizes Ostrom's intuitions. Her "collective choice arrangements" represent H_t(d) mappings where users participate in rule modification. Her design principles can be reinterpreted as conditions enabling high α(d): clear boundaries (defining who holds sᵢ), local monitoring (ensuring Cᵢ holders possess information), graduated sanctions (responses to low-α violations), and conflict resolution mechanisms (managing F(d) when it arises). Successful commons maintain high consent alignment; failed commons exhibit persistent misalignment between stakes and voice.

Recent empirical work validates this interpretation quantitatively. Yadav et al. (2021) analyze 83 Amazonian communities managing arapaima fisheries, showing that Ostrom's design principles predict ecological outcomes systematically. Communities exhibiting collective choice arrangements (high α) maintain sustainable fish stocks; those lacking such arrangements experience depletion. This demonstrates that α(d) isn't merely about legitimacy in an abstract sense but predicts real-world performance in resource domains. Gibson et al. (2005) show that commons with local enforcement and graduated sanctions maintain sustainability whereas centralized enforcement fails, documenting empirically that legitimacy (local α) enables compliance without coercion.

Dietz et al. (2003) conduct comprehensive review of 80+ documented commons cases, confirming Ostrom's design principles generalize across fisheries, forests, water systems, and grazing lands. The meta-finding: consent-holding structures aligned with stakes predict success; misalignment predicts failure. Berkes et al. (2006) document collapse when external actors ("roving bandits") gain access without local α—stakes become diffuse, consent-holding breaks down, and resources deplete. This validates that α(d) at local level depends on boundary exclusivity (Ostrom's first design principle).

The framework extends Ostrom in three ways. First, we generalize from resource commons to all collective decision domains—not just fisheries and forests but also corporate governance, platform moderation, and constitutional design. Second, we quantify concepts she describes qualitatively: α(d,t) provides continuous measurement of "collective choice arrangements"; F(d,t) tracks when graduated sanctions become necessary. Third, we generate predictions about institutional evolution: commons experiencing rising F(d) should adapt through expanding α(d) or face degradation.

### 2.6 Deliberative Democracy and Mini-Publics

Habermas's (1984, 1990) communicative action theory distinguishes strategic action (oriented toward achieving one's goals) from communicative action (oriented toward mutual understanding through reasoned argument). Legitimate norms are those acceptable to all affected parties through rational discourse free from coercion. His discourse principle holds that "only those norms can claim validity that could meet with the acceptance of all concerned in their capacity as participants in a practical discourse." This maps onto consent-holding directly: "all concerned" represents our affected set S_d = {i | sᵢ(d) > 0}, while "acceptance" requires Cᵢ > 0 in decision procedures H_t(d). Habermas's communicative ideal provides the normative aspiration; our α(d) measures empirical proximity to it.

Fishkin's (2009, 2018) deliberative polling research operationalizes these theoretical commitments. By convening randomly selected representative samples, providing balanced information, facilitating structured deliberation, and measuring preference changes, deliberative polls demonstrate that informed public judgment shifts significantly through discourse. Participants report increased political efficacy, greater willingness to compromise, and more nuanced policy positions. Fishkin shows deliberation isn't merely normatively desirable but empirically consequential: it changes minds, reduces polarization, and produces stable informed preferences.

Citizens' assemblies extend deliberative innovation to consequential policy domains. The Irish Citizens' Assembly (2016-2018) addressed abortion and climate change through 99 randomly selected citizens deliberating after expert input, ultimately recommending constitutional referendum on abortion. The proposal passed 66-34% in 2018—remarkable in historically Catholic Ireland. The French Citizens' Convention on Climate (2019-2020) generated 149 proposals from 150 randomly selected participants, many subsequently adopted. Scotland established permanent deliberative infrastructure, institutionalizing high-α mechanisms for contested domains.

The framework interprets these innovations as institutional experiments raising α(d) through sortition and deliberation. Random selection approximates equal Cᵢ for participants; demographic stratification can approximate stakes-weighting if groups correlate with sᵢ(d). Learning phases improve eff_voiceᵢ through information provision; deliberation structures enable preference refinement. The resulting recommendations typically achieve supermajority support (80%+), indicating high α(d) within the assembly. When governments implement these recommendations, broader political α(d) rises.

Empirical research validates predicted benefits. Wells et al. (2021) study UK climate assemblies, finding deliberative processes increase public mandate for action but impact on actual policy remains contingent on institutional embedding—precisely what the framework predicts. Assemblies achieving high internal α still face low political α if recommendations are ignored. Niemeyer (2011) documents psychological effects: deliberation increases preference stability, reduces polarization, and increases willingness to accept minority positions, all suggesting higher α reduces F(d) through preference clarification. Hendriks (2014) shows mini-publics reduce but don't eliminate influence inequalities, validating that α(d) > 0 doesn't imply uniform voice distribution as the framework models explicitly through heterogeneous Cᵢ.

Challenges these innovations face illuminate consent-holding dynamics. Empowerment problems arise when assemblies have high internal α but political systems ignore recommendations—low coupling between assembly H_t(d_assembly) and governmental H_t(d_policy). Scaling difficulties emerge in extending 50-150 person deliberations to millions. Our framework suggests nested consent-holding as solution: local assemblies feed into regional, then national structures, each layer maintaining high α within its domain. Representation concerns question whether demographic representativeness captures stakes: should climate assemblies stratify by age (future stakes), geography (regional impacts), or ideology (value heterogeneity)? The framework's answer: measure sᵢ(d_climate) empirically and design selection accordingly.

### 2.7 Algorithmic Governance and Platform Legitimacy

Grimmelikhuijsen et al. (2022) identify three legitimacy dimensions for algorithmic decision-making: input (did citizen preferences inform design?), throughput (does the algorithm follow fair procedures?), and output (do outcomes align with public values?). Their framework extends Scharpf's (1999) input-output legitimacy distinction, recognizing that procedural quality matters beyond democratic authorization and performance. Current algorithmic governance exhibits severe deficits across all three dimensions. Citizens rarely participate in algorithm design (low input legitimacy), decision-making processes remain opaque black boxes (low throughput legitimacy), and outcomes often replicate historical discrimination (questionable output legitimacy).

Empirical findings document legitimacy challenges. Waldman and Johnson (2022) show that high-stakes algorithmic decisions (healthcare allocation, criminal sentencing) are perceived as less legitimate than human decisions even when outcomes are identical. Human-in-the-loop oversight increases legitimacy significantly; purely automated systems face resistance. Algorithmic errors are penalized more heavily than human errors—a troubling asymmetry suggesting people distrust systems they can't understand or contest. Governance benefits decrease as decision importance increases, precisely when legitimacy matters most.

The consent-holding framework diagnoses these challenges structurally. Algorithmic decision-making creates domains d_algorithm where algorithms or their designers hold C ≈ 1 while affected citizens have C ≈ 0 despite high sᵢ(d). Credit scoring algorithms determine loan access (high sᵢ for applicants); hiring algorithms control employment opportunities (high sᵢ for candidates); content moderation algorithms shape speech norms (high sᵢ for platform users). In each case, current α(d_algorithm) ≈ 0 because high-stakes populations are excluded from H_t(d). The framework predicts this generates rising F(d): user protests, regulatory backlash, platform exodus.

This prediction is validated empirically. GDPR (2018) and the Digital Services Act (2022) represent regulatory friction—states channeling user dissatisfaction into binding constraints on platform authority. The #DeleteFacebook movement (2018) following Cambridge Analytica exemplifies user friction through threatened exit. Twitter's transformation under Elon Musk (2022-2024) demonstrates friction dynamics: unilateral changes without user consultation (α → 0 made explicit) generated mass exodus to Mastodon, Bluesky, and Threads. Platforms with sustained low α face user migration when alternatives emerge—exactly as the framework predicts.

Platform responses attempting to raise α reveal understanding of legitimacy deficits. Meta's Oversight Board provides independent content moderation appeals, slightly raising α(d_moderation) by giving users contestation rights. YouTube Creator Councils consult high-profile creators, extending partial Cᵢ to stakeholders whose sᵢ is highest (professional creators depending on platform income). These remain largely tokenistic—Cᵢ still overwhelmingly concentrated in executives and algorithms—but directionally consistent with friction-reduction incentives.

The framework prescribes more thoroughgoing reforms. For input legitimacy, stakeholder participation in algorithm design expands H_t(d_design) to include affected groups. Participatory algorithm design studies (Koshimizu et al., 2020) show that including stakeholders in fairness criterion selection produces higher legitimacy beliefs than top-down technical optimization—direct validation that α(d) matters for perceived legitimacy. For throughput legitimacy, algorithmic transparency and appeals increase eff_voiceᵢ by enabling meaningful contestation. For output legitimacy, stakes-weighted outcome evaluation ensures high-sᵢ groups aren't systematically harmed, which current "fairness" metrics often ignore by optimizing for overall accuracy rather than minimizing maximum harm.

Costanza-Chock's (2020) design justice framework provides practical methodology: systematic inclusion of affected communities in technical system design, explicit accounting for power dynamics, and redistributive approaches prioritizing marginalized groups. This operationalizes consent-holding for algorithmic domains: identify stakeholders (sᵢ > 0), ensure meaningful participation (Cᵢ > 0), and weight influence by stakes (toward high α). Barocas et al. (2019) demonstrate that fairness cannot be achieved through technical optimization alone—it requires specifying whose values matter, an inherently political choice that our α(d) framework makes explicit.

### 2.8 Voting Power Indices and Coalition Analysis

The power indices literature demonstrates that voting weight ≠ voting power. Banzhaf (1965) measures critical voter frequency: how often removing your vote changes outcomes from win to loss. Shapley and Shubik (1954) measure pivotal voter frequency in sequential coalition formation. These indices often diverge dramatically from nominal weights—Germany holds the most European Council votes but doesn't possess proportional power due to coalition dynamics. Similar phenomena arise in corporate boards (blockholders vs. minority shareholders), legislatures (swing voters vs. party leaders), and qualified majority systems (Security Council veto players).

These insights directly inform consent-holding operationalization. Naive approaches measure Cᵢ as voting weight (shares held, seats controlled). Sophisticated approaches use power indices accounting for coalition structures. In weighted voting contexts (shareholders, federalism), qualified majority rules (constitutional amendments), and veto player systems (UN Security Council), indices capture actual influence more accurately than nominal weights.

The framework integrates power indices into legitimacy measurement: α(d,t) = (Σᵢ sᵢ(d)·PowerIndexᵢ(d,t)) / (Σᵢ sᵢ(d)), where PowerIndexᵢ represents Banzhaf, Shapley-Shubik, or domain-appropriate measures. This refinement matters most when vote concentration enables blocking coalitions. Consider corporate governance: a minority shareholder with 20% equity plus veto rights over major transactions wields power far exceeding their ownership share. Measuring Cᵢ = 0.20 understates influence; calculating Banzhaf index accounting for veto power provides accurate assessment.

Recent extensions analyze endogenous coalition formation (Aumann & Myerson, 1988), showing how equilibrium structures emerge from bargaining. This connects to consent-holding's dynamic aspect: H_t(d) evolves as agents form alliances, shifting power distributions. Nash bargaining solutions (Nash, 1950) maximize products of utility gains subject to Pareto efficiency—structurally similar to stakes-weighted consent maximization. Kalai and Smorodinsky (1975) propose alternative axiomatizations highlighting trade-offs between equality (proportional gain-sharing) and efficiency (Pareto optimality), demonstrating solution multiplicity absent unique normative commitments—precisely what Theorem 5 predicts.

### 2.9 Relational Autonomy and Consent Capacity

Mackenzie's (2014) three-dimensional autonomy framework distinguishes self-determination (choosing one's own life path), self-governance (regulating one's actions), and self-authorization (taking responsibility for choices). Traditional liberal autonomy assumes atomistic individuals; relational approaches recognize that autonomy emerges from social structures and relationships. Oppressive systems constrain capacity for self-governance—gender oppression limits women's educational access, economic opportunities, and freedom from violence, directly undermining autonomous choice.

Koggel (2022) extends this to global justice, arguing that respecting autonomy requires enabling threshold capabilities, not merely non-interference. Autonomy necessitates freedom conditions: political liberties (speech, association, conscience) and personal liberties (movement, bodily autonomy, freedom from violence). Agents lacking these conditions cannot exercise meaningful consent even if formally included in H_t(d).

These insights address the framework's handling of eff_voiceᵢ. Relational autonomy = effective voice in our legitimacy equation. Simply granting Cᵢ > 0 (voting rights) without resources, education, or freedom produces low eff_voiceᵢ—formal authority without capacity to exercise it. Oppressive structures systematically reduce both stakes recognition (dominant groups deny subordinated groups' sᵢ) and consent power (exclusion from H_t(d) even for high-stakes domains).

This perspective addresses three framework challenges. First, it resolves circularity concerns: "Who decides who's in H_t(d)?" Answer: those with stakes + capacity, considering relational constraints that may undermine apparent consent. Second, it handles vulnerable populations ethically. Proxy consent becomes necessary when capacity is impaired, but structures should enable gradual inclusion as capability develops rather than permanent exclusion. Third, it enables justice analysis: systematic exclusion of groups with high sᵢ but low eff_voiceᵢ constitutes legitimacy deficit diagnosable through α(d) measurement.

Application to research ethics illustrates these dynamics. Standard approaches grant legal guardians consent authority over cognitively impaired individuals. Relational approaches recognize impaired persons retain partial capacity and value particular relationships beyond legal guardianship—an older sibling may understand needs better than distant legal guardians. The framework prescription: allocate partial Cᵢ based on measured capacity + expand H_t(d) to include chosen trusted relationships, raising α(d_research) for the affected individual.

---

## 3. Formal Framework: Primitives, Axioms, and Theorems

This section establishes the framework's formal foundations through precise definitions, minimal axioms, and structural theorems. The approach proceeds deductively: from spare assumptions about collective decision-making to necessary conclusions about consent-holding's existence, friction's inevitability, and legitimacy's measurement.

### 3.1 Primitives and Definitions

We begin with foundational concepts requiring no prior theoretical commitment. An **agent** is any entity capable of selecting among actions, indexed i ∈ A = {1, ..., N}. Agents may be individuals, organizations, algorithms, or collective bodies—the framework remains agnostic about internal composition. A **domain** represents a decision-relevant sphere—a policy area, firm process, household choice, or any context requiring action selection. The set of domains is D = {d₁, ..., d_M}. Each domain d admits a set of possible actions X_d, from which one action x_d ∈ X_d must be selected.

**Outcomes** represent realized states resulting from action vectors x = (x_d₁, ..., x_d_M) through an environment mapping E: ∏_d X_d → O, where O denotes the outcome space. This formulation accommodates complex interdependencies: outcomes in domain d may depend on actions in other domains d' through E's structure. An agent i's **stake** in domain d, denoted sᵢ(d) ≥ 0, quantifies sensitivity to outcomes in that domain. Stakes may reflect material exposure, legal consequences, capability impacts, or existential threats. The framework requires only that stakes be measurable in principle; Section 5 operationalizes measurement through proxy variables.

Each agent possesses **preferences** over outcomes, represented either as complete orderings ≿ᵢ or utility functions Uᵢ: O → ℝ. Preferences induce ideal points x*ᵢ,d in each domain—the action agent i most prefers given others' anticipated choices. The framework remains neutral regarding preference formation, requiring only that preferences exist and agents can (imperfectly) articulate them.

**Consent** represents the normative right to decide in a domain—who may authoritatively say "yes" or "no" to proposed actions. The **consent-holder mapping** H_t(d) ∈ Δ(C) specifies the distribution of decision authority over possible holders C at time t. This distribution may be concentrated (H_t(d) assigns probability 1 to a monarch), distributed uniformly (direct democracy with equal votes), or weighted continuously (shareholder voting, stakes-weighted systems). The mapping evolves over time, capturing institutional change through H_t(d) → H_t+1(d).

Individual **consent power** Cᵢ,d ∈ [0,1] represents agent i's effective share of decision authority in domain d, with Σᵢ Cᵢ,d = 1. In simple voting systems, Cᵢ,d = 1/N_voters. In shareholder meetings, Cᵢ,d = shares_i / total_shares. In technocracies, Cᵢ,d = 1/|E| if i belongs to expert set E, zero otherwise. In algorithmic systems, consent power concentrates in designers and code, with Cᵢ,d ≈ 0 for most affected users. This formalization accommodates arbitrary distributions while maintaining interpretability.

Finally, we distinguish **de jure** vs. **de facto** consent-holding: who is officially recognized as rightful authority versus who actually determines outcomes. Misalignment drives instability—when formal H_t(d) diverges from actual decision-making loci, legitimacy crises emerge. Revolutionary moments often involve de facto authorities (military, revolutionary councils) supplanting de jure structures (deposed monarchies, overthrown governments).

### 3.2 Axioms

The framework rests on seven axioms representing minimal commitments about collective decision-making. These axioms are intentionally sparse; denying any removes the analysis from realistic social contexts.

**A1. Action Precedence**: Every non-null outcome in a domain is produced by some action (including "do nothing"). This rules out outcomes materializing without decisions, grounding all consequences in choices. Even defaults represent prior actions establishing conventions.

**A2. Decision Requirement**: Every action is selected by some decision procedure (choice, rule, randomization, delegation). This axiom asserts that actions don't occur spontaneously—some mechanism, however implicit, generates selections. The mechanism may be individual choice, collective voting, algorithmic optimization, or random selection, but its existence is necessary.

**A3. Shared Reality**: Outcomes alter a world co-occupied by multiple agents; externalities exist. This axiom rules out pure individualism where each agent's actions affect only themselves. Collective decision-making becomes necessary precisely because A3 holds—we inhabit overlapping consequence spaces.

**A4. Finitude**: Agents have finite time, attention, and cognitive capacity; no single agent can decide everything alone. This motivates specialization and delegation in consent-holding. Even if one agent theoretically possessed sufficient knowledge, time constraints necessitate distributed decision-making as domains multiply.

**A5. Plurality**: Agents' preference orderings differ on at least some domains. This assumption generates the core tension driving consent-holding analysis. If all agents shared identical preferences, collective choice would be trivial—any mechanism would produce unanimous outcomes. Plural preferences guarantee conflicts requiring resolution.

**A6. Salience**: For each domain, at least one agent has sᵢ(d) > 0. This rules out domains where nobody cares about outcomes, focusing analysis on consequential decisions. Trivial domains raise no consent-holding questions.

**A7. Fallibility/Subjectivity**: Perception and valuation are frame-dependent; no universal content-level value ordering is logically forced. This axiom embraces value pluralism: agents may reasonably disagree about goodness, justice, or priority even with perfect information. Reasonable pluralism (Rawls, 1993) is structural, not resolvable through better argumentation alone.

These axioms jointly characterize human social organization. Societies violating A1-A2 wouldn't exhibit recognizable politics; violations of A3-A4 would enable autarky; violations of A5-A7 would permit algorithmic value determination. The axioms' minimalism strengthens derived theorems—conclusions follow from sparse commitments.

### 3.3 Theorem 1: Consent-Holding Necessity

**Theorem 1 (Consent-Holding Necessity)**: In any domain d where a non-null outcome occurs, there exists a consent-holder mapping H_t(d).

*Proof Sketch*: By A1-A2, any outcome resulted from an action selected through some procedure. A procedure implies a locus of control—the entity/entities choosing the action, establishing the choice rule, or delegating to randomization. This locus constitutes H_t(d). Formally, if outcome o ≠ null occurred, then some x_d was selected. Selection requires a chooser C or a rule R selecting based on C's preferences/delegation. Even if x_d was chosen randomly, someone authorized randomization—that authorization decision had a locus. Therefore, denying H_t(d)'s existence contradicts A2. ∎

**Corollary 1.1**: "Letting chance decide" doesn't eliminate consent-holding—it reveals who held consent to permit chance. Lottery-based selection (sortition, random policy experiments) represents concentrated consent to randomize, not absence of consent structures.

**Corollary 1.2**: The regress "who consents to the consent rules?" is virtuous, not vicious. Each meta-level n has its own H_t(d^n), from object-level policy (d⁰) to constitutional rules (d¹) to amendment procedures (d²) and beyond. The chain terminates pragmatically through founding acts, revolution, or ongoing practice, but continues logically as nested consent relations. This structure *is* politics; demanding non-consent foundations commits a category error akin to asking "what causes causation?"

### 3.4 Theorem 2: Inevitable Friction

**Theorem 2 (Inevitable Friction)**: If there exist agents i, j with divergent preferences on domain d and sᵢ(d), s_j(d) > 0, then unless H_t(d) exactly reproduces stakes-weighted unanimity, at least one agent experiences moral/political friction.

We formalize **friction** in domain d as:

F(d,t) = Σᵢ sᵢ(d) · δ(x_d(t), x*ᵢ,d)

where x*ᵢ,d represents agent i's ideal action and δ measures divergence. For discrete choices, δ(x, x*) = 0 if x = x*, else 1. For continuous policy spaces, δ(x, x*) = |x - x*| captures distance from ideal points.

*Proof*: Given A5 (plural preferences), there exist i, j with x*ᵢ,d ≠ x*_j,d. Any realized action x_d must equal some agent's ideal or lie between/outside ideals. If x_d ≠ x*ᵢ,d, then δ(x_d, x*ᵢ,d) > 0, contributing sᵢ(d)·δ > 0 to F(d,t). Stakes-weighted unanimity (impossible under plural preferences except by coincidence) is the only configuration yielding F = 0. Therefore, F(d,t) > 0 generically. ∎

**Behavioral Extension**: Introducing tolerance thresholds τᵢ yields:

F_τ(d,t) = Σᵢ sᵢ(d) · max(0, δ(x_d, x*ᵢ,d) - τᵢ)

This refinement captures that agents don't mobilize against every deviation—they tolerate "good enough" governance within zones of acceptability. Only deviations exceeding tolerance generate observable friction (protests, defection, resistance). Tolerance may correlate with stakes (higher sᵢ → lower τᵢ as vulnerable populations are less forgiving) or inversely (privileged agents tolerate suboptimal policy more easily). Empirically, τᵢ can be identified through mobilization thresholds where friction manifests observably.

### 3.5 Theorem 3: Legitimacy as Consent Alignment

**Theorem 3 (Legitimacy as Alignment)**: Define the **affected set** S_d = {i | sᵢ(d) > 0}. **Consent alignment** is:

α(d,t) = (Σᵢ∈S_d sᵢ(d) · eff_voiceᵢ(d,t)) / (Σᵢ∈S_d sᵢ(d))

where eff_voiceᵢ represents agent i's effective decision power in H_t(d), accounting for formal authority Cᵢ, capability constraints, and relational factors (Section 2.9). A minimal procedural legitimacy condition requires α(d,t) ≥ τ for society-specific threshold τ. Persistent α < τ predicts observable friction through unrest, exit, sabotage, or normative decay.

This theorem doesn't *define* legitimacy philosophically—that remains contested across moral traditions. Instead, it operationalizes a measurable legitimacy proxy: stakes-weighted consent alignment. High α indicates that decision power concentrates among those most affected; low α indicates that the most impacted have little voice.

**Interpretation**: Legitimacy is consent aligned with consequence. This formulation avoids binary classifications (legitimate/illegitimate) in favor of continuous measurement enabling comparative analysis. A system with α = 0.7 exhibits higher legitimacy than α = 0.3, though both may face friction if societal τ = 0.8. The threshold τ represents context-dependent expectations—established democracies may require τ > 0.6, while autocracies maintain stability at τ = 0.2 through coercion or performance compensation.

**Empirical Prediction**: Cross-sectional analysis should find negative correlation between α(d) and friction metrics F(d) (protests, strikes, litigation, policy reversals). Longitudinal analysis should show that increasing α(d) causes declining F(d) with lags reflecting institutional adjustment. Panel regressions with institutional reforms as instruments can test causality.

### 3.6 Theorem 4: Competence-Consent Trade-Off

**Theorem 4 (Competence-Consent Trade-Off)**: Overall legitimacy combines consent alignment and performance:

L(d,t) = w₁ · α(d,t) + w₂ · P(d,t)

where α(d,t) represents stakes-weighted consent alignment (Theorem 3), P(d,t) denotes performance/competence metrics (outcome quality relative to benchmarks), and w₁, w₂ ≥ 0 reflect society-specific weights on voice versus results.

This formulation makes several insights explicit. First, different systems optimize different points on the legitimacy frontier. Technocracies maximize P, often sacrificing α by concentrating consent in experts. Direct democracies maximize α through universal suffrage, potentially reducing P on technical domains where distributed knowledge is sparse. Authoritarian systems frequently achieve neither—concentrated consent-holding without stakes-weighting produces low α, while performance legitimation remains uncertain. Sophisticated systems (weighted voting, citizens' assemblies, codetermination) attempt Pareto improvements, raising both α and P simultaneously.

Second, even maximally competent governance (P → 1) remains illegitimate if consent alignment vanishes (α → 0) whenever w₁ > 0. This addresses technocratic fantasies: benevolent dictatorship by perfectly informed experts still generates illegitimacy if affected populations lack voice. The magnitude depends on w₁—societies valuing autonomy highly set w₁ >> w₂, making high α necessary regardless of performance. Societies facing existential threats may temporarily elevate w₂, accepting lower α for competent crisis management.

Third, we can extend friction measurement to incorporate performance:

F(d,t) = Σᵢ sᵢ(d) · δ(x_d(t), x*ᵢ,d) - λ · P(d,t)

where λ represents the performance discount factor. Good outcomes reduce friction even when consent alignment is imperfect—successful policies generate acquiescence. However, λ is bounded: extremely low α cannot be fully compensated by high P if w₁ is substantial. Colonial powers frequently achieved moderately high P (infrastructure, order, administrative competence) yet generated enormous F(d) because α(d_governance) → 0 for indigenous populations with existential stakes in self-determination. Competence doesn't purchase legitimacy when autonomy weights w₁ are large.

Fourth, the framework generates domain-specific predictions. High-stakes technical domains (nuclear safety, pandemic response, monetary policy) may rationally set w₂ >> w₁—expertise matters more than universal participation. High-stakes value domains (constitutional rights, community norms, religious practice) rationally set w₁ >> w₂—consent matters more than efficiency. Routine low-stakes administration balances both—moderate α suffices if P is high. These aren't arbitrary choices but reflect stakes distributions and uncertainty structures.

**Empirical Test**: Estimate w₁, w₂ by regressing observed friction F(d) on α(d) and P(d) across domains. Coefficients reveal societal legitimacy weights. Cross-national comparisons identify cultural variation: Scandinavian democracies may exhibit high w₁ (prioritizing inclusion), while technocratic Singapore shows high w₂ (prioritizing performance). Within-country variation across domains tests whether weights adjust appropriately—do nuclear safety decisions indeed show higher w₂ than constitutional amendments?

### 3.7 Theorem 5: Minimal Absolutism from Relativism

**Theorem 5 (Relativism ⇒ Minimal Absolutism)**: Given A7 (value frame-dependence), the claim "all value judgments are frame-relative" is coherent only if the **structure** enabling frames is invariant. Therefore, at least one absolute exists: the necessity of consent-holding over shared outcomes wherever A1-A6 hold.

*Proof Sketch*: Suppose all value claims are frame-dependent (A7). Frame-dependence presupposes frames exist—perspectives from which valuations occur. Frames belong to agents inhabiting shared reality (A3) with plural preferences (A5). These agents make decisions affecting each other (A1-A2). Such decisions require consent-holder mappings H_t(d) (Theorem 1). Therefore, relativism about content-level values doesn't extend to structural necessities. If someone objects "but who says consent-holding must exist?", the answer is: logic, given axioms describing collective decision contexts. Denying consent-holding's necessity requires denying A1-A6, exiting the domain of social analysis. ∎

**Interpretation**: When values are relative, only structure is absolute. This resolves apparent tension between value pluralism and moral realism. Content-level disagreements (what policies are just? which outcomes are good?) may admit no universal resolution—different frames yield different verdicts. But the structural fact that *someone decides* remains invariant. This is the framework's core philosophical move: extracting structural absolutism from value relativism.

**Corollary 5.1**: Traditional moral theories (utilitarian, deontological, virtue-based) occupy the evaluation layer (Layer 3 in Section 4) while consent-holding operates at the ontological layer (Layer 1). Utilitarianism prescribes maximizing Σᵢ Uᵢ given H_t(d); deontology constrains permissible x_d by rights/duties; virtue ethics evaluates consent-holders' character. None of these eliminate H_t(d)—they judge its use differently. The framework provides common infrastructure across moral traditions.

**Implication for Metaethics**: Metaethical debates about moral truth conditions (realism vs. anti-realism) don't undermine consent-holding analysis. Even if moral realism holds—some moral truths are stance-independent—agents remain bounded and plural (A4-A5), necessitating consent structures over shared outcomes (A3). Conversely, moral anti-realism strengthens the framework: if no objective values exist, structural analysis of decision authority becomes the only tractable approach to political philosophy.

---

This completes the formal foundations. Theorems 1-5 establish that consent-holding is structurally necessary (T1), friction is inevitable under plural preferences (T2), legitimacy is measurable as stakes-weighted alignment (T3), competence and consent trade off in domain-specific ways (T4), and this structural analysis remains valid across value frameworks (T5). Section 4 applies these tools to social contract theories, Section 5 operationalizes for empirical testing, and Section 6 validates through computational simulation.

## 5. Operationalization: Empirical Measurement and Identification

The theoretical framework developed in Sections 2-3 provides analytical tools for understanding consent-holding structures and their consequences. This section bridges theory and empirical application by specifying how the framework's core concepts can be measured, how causal relationships can be identified econometrically, and what testable predictions emerge. The operationalization proceeds in four steps: first, formalizing the consent matrix and stakes-weighted alignment metrics; second, developing friction measures including tolerance-weighted extensions; third, specifying identification strategies for estimating legitimacy effects; and fourth, addressing methodological challenges inherent in measuring politically contested concepts.

### 5.1 Formal Measurement Framework

We operationalize consent-holding through a consent matrix C ∈ [0,1]^(N×M), where each element C_i,d represents agent i's effective decision share in domain d, subject to the normalization constraint Σ_i C_i,d = 1. This matrix captures both de jure authority and de facto power. In simple majority voting systems with equal suffrage, C_i,d = 1/N_voters for all enfranchised i and C_i,d = 0 for excluded populations. In shareholder governance, C_i,d = shares_i / total_shares, reflecting proportional ownership. In technocratic systems, C_i,d = 1/|E| if agent i belongs to the expert set E, zero otherwise. Algorithmic governance concentrates consent power in system designers and code, yielding C_i,d ≈ 0 for most affected users despite their often substantial stakes in platform policies.

The matrix formulation accommodates arbitrary distributions while preserving interpretability. Consent power may concentrate in single agents (monarchy, dictatorship), distribute uniformly (direct democracy), or weight by various criteria (wealth, expertise, demonstrated stakes). The framework takes no position on which distribution is normatively preferable but provides tools for measuring the consequences of any particular allocation. This agnosticism enables comparative analysis across radically different governance systems without presuming democratic superiority.

Complementing the consent matrix, the stakes vector s(d) ∈ ℝ^N_≥0 quantifies each agent's exposure to consequences in domain d. Stakes measurement presents both conceptual and practical challenges. Conceptually, stakes may reflect material exposure (tax burden relative to income, property value threatened by regulation), capability impacts (health outcomes affected, educational opportunities shaped), or existential threats (survival risks from climate policy, bodily autonomy implications of reproductive regulation). These conceptions often correlate but may diverge substantially. A billionaire faces large absolute tax exposure measured in dollars but negligible capability impact given remaining wealth; a working family faces modest absolute exposure but severe capability consequences if taxation forces housing or healthcare tradeoffs.

The framework's flexibility regarding stakes definition reflects substantive disagreement about what "being affected" means normatively. Different domains may legitimately employ different stakes conceptions. Corporate dividend policy might reasonably weight by material stakes (capital at risk), while healthcare policy should prioritize capability stakes (health outcomes affected), and climate policy must account for existential stakes (territorial submersion, uninhabitability). Rather than prescribing universal stakes metrics, we propose domain-specific measurement strategies validated through friction outcomes: if measured stakes yield low observed friction F(d), the measurement captures consequential impacts; if high friction persists despite claimed high alignment α(d), stakes were likely mismeasured.

Preferences x*_i,d represent each agent's ideal action in domain d given anticipated actions in other domains and expected environmental responses. These ideal points induce a preference ordering over feasible actions X_d. The framework accommodates both discrete choice domains (approve/reject a policy, elect a candidate) and continuous policy spaces (tax rates, emissions targets, interest rates). For analytical tractability, we often assume single-peaked preferences over continuous domains, though extensions to multi-dimensional policy spaces and non-single-peaked preferences are straightforward.

Combining these elements, consent alignment in domain d at time t is measured as:

α(d,t) = (Σ_i∈S_d s_i(d) · eff_voice_i(d,t)) / (Σ_i∈S_d s_i(d))

where S_d = {i | s_i(d) > 0} denotes the affected set and eff_voice_i represents agent i's effective decision power accounting for both formal authority C_i,d and capacity constraints. This refinement recognizes that possessing formal consent power without resources, education, political freedom, or institutional access produces low effective voice. A citizen with voting rights but lacking information, facing voter suppression, or deterred by registration barriers exercises less influence than the nominal C_i,d = 1/N_voters suggests.

The effective voice adjustment connects directly to the relational autonomy literature reviewed in Section 2.9. Oppressive social structures systematically reduce eff_voice_i below formal C_i,d through multiple mechanisms: economic deprivation limits time and resources for political participation; educational disparities reduce capacity to process policy information; discriminatory enforcement raises costs of mobilization for marginalized groups; and cultural subordination undermines political efficacy beliefs. Measuring eff_voice_i empirically requires proxying these capacity constraints through literacy rates, poverty rates, discrimination indices, and political efficacy survey data. Where such data permits, researchers can construct refined alignment measures:

α_effective(d,t) = (Σ_i s_i(d) · C_i,d · capacity_i(t)) / (Σ_i s_i(d))

where capacity_i ∈ [0,1] represents estimated capability to exercise formal consent power. This three-way interaction among stakes, formal authority, and capacity provides a comprehensive legitimacy metric sensitive to multiple dimensions of political exclusion.

### 5.2 Friction Metrics and Tolerance-Weighted Extensions

Political friction represents the stakes-weighted aggregate deviation between realized outcomes and stakeholder preferences. In its basic form, friction in domain d at time t is:

F(d,t) = Σ_i s_i(d) · δ(x_d(t), x*_i,d)

where x_d(t) denotes the implemented action and δ measures divergence from ideal points. For discrete choices, the divergence function takes binary values: δ(x, x*) = 0 if x equals the ideal action x*, and δ(x, x*) = 1 otherwise. For continuous policy spaces, Euclidean distance δ(x, x*) = |x - x*| captures proximity to ideal points. In multidimensional policy spaces (d ∈ ℝ^k), the Euclidean norm generalizes naturally: δ(x, x*) = ||x - x*||_2.

This formulation makes explicit that friction is inevitable under plural preferences (Theorem 2). Any realized action satisfying some agents necessarily disappoints others when ideal points diverge. Perfect alignment F(d) = 0 requires either unanimous preferences (violating Axiom A5) or actions exactly matching the stakes-weighted ideal Σ_i s_i(d)·x*_i,d / Σ_i s_i(d), which generically differs from all individual ideal points. Friction thus provides a cardinal measure of the welfare losses imposed by collective decision-making's necessity.

The basic friction metric treats all deviations from ideal points as equally consequential. Behavioral reality suggests a more nuanced pattern: agents tolerate "good enough" governance within zones of acceptability, mobilizing only when deviations exceed tolerance thresholds. This insight motivates a tolerance-weighted friction measure incorporating agent-specific tolerance parameters τ_i ≥ 0:

F_τ(d,t) = Σ_i s_i(d) · max(0, δ(x_d(t), x*_i,d) - τ_i)

Under this specification, deviations smaller than τ_i contribute zero friction; only deviations beyond tolerance generate political resistance. The tolerance-weighted metric captures several empirically important phenomena. First, it explains why stable democracies often implement policies far from many citizens' ideal points without generating instability: so long as deviations remain within tolerance zones, F_τ(d) stays low despite potentially high F(d). Second, it predicts threshold effects in mobilization: gradual policy shifts may produce minimal observable friction until crossing critical tolerance boundaries, whereupon resistance explodes discontinuously. Third, it generates heterogeneous friction contributions depending on tolerance distributions: even identical policy deviations affect agents differently if their τ_i values vary.

The distributional properties of tolerance parameters τ_i raise important empirical and normative questions. One possibility is positive correlation between stakes and intolerance: agents with high s_i(d) maintain lower tolerance τ_i because consequences are severe, making precision more critical. Under this specification, vulnerable populations facing existential stakes exhibit hair-trigger resistance to adverse policy shifts, while privileged populations with low stakes tolerate substantial deviations. An alternative pattern involves inverse correlation: privileged agents maintain high tolerance because they can absorb suboptimal policy consequences through private alternatives (private schools substituting for public education policy, private healthcare mitigating health policy failures), while vulnerable populations tolerate poor governance because mobilization costs exceed their capacity despite low τ_i preferences.

These competing accounts generate testable predictions. If high stakes correlate with low tolerance, we should observe that policy domains affecting vulnerable populations (housing, healthcare, food security) exhibit steeper mobilization gradients: small adverse changes produce large friction increases. If privilege enables tolerance, we should instead observe that elites facing moderate policy losses rarely mobilize (high τ_i despite resources for action), while poor populations tolerate severe deprivation until absolute thresholds are crossed. Empirical identification of tolerance parameters proceeds through mobilization threshold analysis: estimate τ_i by observing the deviation magnitude at which agent i transitions from acquiescence to active resistance (protest participation, legal challenges, exit threats).

### 5.3 Alignment Measurement and Interpretation

Consent alignment α(d,t) provides a continuous legitimacy measure ranging from zero (complete exclusion of high-stakes populations) to unity (perfect proportionality between stakes and consent power). This metric improves upon binary legitimate/illegitimate classifications by enabling comparative analysis across institutional configurations and temporal evolution. A system with α = 0.7 exhibits higher legitimacy than one with α = 0.3, though both may generate friction if societal expectations set legitimacy thresholds τ_legitimacy above these values.

The alignment metric's interpretation depends on understanding what perfect alignment α = 1 represents. This occurs when consent power C_i,d exactly equals normalized stakes s_i(d) / Σ_j s_j(d) for all agents i. In such configurations, influence over outcomes scales precisely with exposure to consequences. Perfect alignment is rarely achieved in practice: electoral systems grant equal votes despite heterogeneous stakes; corporate governance weights by capital contribution rather than employment stakes; technocracies concentrate power in experts independent of their exposure to policy consequences. The framework's contribution lies not in demanding α = 1 universally but in measuring deviations from this ideal and predicting their consequences.

Empirically, alignment measurement requires constructing both numerator and denominator components. The numerator Σ_i s_i(d)·C_i,d aggregates stakes-weighted consent power across agents. This requires measuring individual stakes s_i(d) through domain-specific proxies (detailed in Section 5.4) and consent power C_i,d through institutional analysis. For voting systems, C_i,d follows directly from franchise rules and vote weights. For corporate governance, C_i,d can be measured using Shapley-Shubik or Banzhaf power indices that account for coalition dynamics rather than naive ownership shares (Shapley & Shubik, 1954; Banzhaf, 1965). For algorithmic governance, C_i,d measurement requires identifying effective control over decision rules: engineers writing code, executives setting objectives, and users possessing exit rights all hold partial consent power measurable through organizational structure and platform architecture analysis.

The denominator Σ_i s_i(d) represents total stakes, providing normalization ensuring α ∈ [0,1]. This total stakes measure serves an important interpretive function: it identifies the aggregate exposure to consequences in domain d, quantifying the decision's importance. Domains with large Σ_i s_i(d) are consequential for many agents or extremely consequential for some; domains with small totals affect few agents weakly. Legitimacy deficits (low α) in high-total-stakes domains predict greater instability than equivalent α in low-stakes domains, suggesting an interaction term between alignment and aggregate stakes:

F_predicted(d,t) = (Σ_i s_i(d)) · f(α(d,t))

where f(α) is a decreasing function capturing that higher alignment reduces friction. This specification predicts that even moderate misalignment in high-stakes domains generates substantial friction, while severe misalignment in trivial domains produces minimal observable resistance.

Beyond point estimates, alignment measurement should quantify uncertainty through confidence intervals accounting for measurement error in both stakes and consent power. Stakes proxies involve estimation error from incomplete data and conceptual ambiguity about what counts as "being affected." Consent power measurements face uncertainty from coalition formation dynamics, informal influence channels, and the gap between de jure and de facto authority. Bayesian hierarchical models can jointly estimate s_i(d) and C_i,d from multiple proxies while propagating uncertainty through to final α estimates, providing honest assessments of measurement precision.

### 5.4 Empirical Identification Strategies

Translating the framework's theoretical concepts into measurable quantities for econometric analysis requires specifying proxy variables, data sources, and identification strategies for causal inference. This subsection details operational approaches for measuring consent power, stakes, friction outcomes, and legitimacy effects, then addresses endogeneity concerns and instrumental variable strategies.

#### 5.4.1 Proxies for Consent Power (C_i,d)

Consent power measurement strategies depend on governance structures. For electoral democracy domains, franchise breadth provides a first-order proxy: the share of affected population S_d holding voting rights. This can be measured historically through suffrage laws and contemporary cross-national comparisons using democratic quality indices (Polity IV scores, V-Dem democracy indices). More refined measures account for vote weighting schemes: equal suffrage versus property-weighted voting, one-person-one-vote versus qualified majorities, and district malapportionment creating divergence between population shares and effective representation.

Constitutional veto point counts (Tsebelis, 2002) proxy for distributed consent power: systems with multiple veto players distribute C_i,d more broadly than majoritarian unicameral systems. Veto players include legislative chambers, executive authorities, constitutional courts, and federal subdivisions. Each additional veto point raises the threshold for policy change, effectively increasing C_i,d for status quo beneficiaries. The framework predicts that veto point proliferation without stakeholder alignment generates high friction: excluded populations face compounded barriers to incorporating their preferences.

For corporate governance, board control indices measure consent power distribution. The Herfindahl index of directorship concentration H = Σ_i (seats_i / total_seats)² captures whether board control disperses broadly or concentrates in blockholder coalitions. Low H indicates distributed consent power; high H suggests concentrated control. Complementary measures include voting share requirements for major decisions (simple majority, supermajority, unanimity), cumulative voting rules enabling minority representation, and worker board representation mandates (German codetermination) expanding C_i,d beyond shareholders.

Algorithmic governance presents novel measurement challenges. Consent power concentrates in those controlling decision rules: platform executives setting algorithmic objectives, engineers implementing models, and operators tuning parameters. User consent power approximates zero in most current systems despite high stakes in content moderation, recommendation algorithms, and platform governance. Proxy measures for algorithmic C_i,d include governance transparency (whether decision rules are disclosed), contestation mechanisms (appeals processes granting users voice), participatory design structures (stakeholder inclusion in algorithm development), and exit costs (switching costs to alternative platforms reducing user leverage through exit threats).

Agenda control provides another dimension of consent power distinct from voting weight. Actors controlling which decisions reach formal voting possess outsized influence independent of vote shares. Agenda power can be proxied through committee chair positions, proposal rights in legislative procedures, and institutional rules governing amendment processes. Romer and Rosenthal (1978) demonstrate formally that agenda setters can induce their preferred outcomes even when holding minority voting power, suggesting that comprehensive C_i,d measurement must account for procedural authority beyond vote tallies.

Network centrality measures from social network analysis offer additional consent power proxies, particularly for informal influence. Agents occupying central network positions (high betweenness centrality, high eigenvector centrality) exercise disproportionate influence through information brokerage and coalition facilitation even without formal authority. Corporate interlock networks, legislative cosponsorship networks, and deliberative assembly interaction patterns can reveal de facto consent power exceeding de jure authority.

#### 5.4.2 Proxies for Stakes (s_i(d))

Stakes measurement requires domain-specific approaches reflecting varied consequence types. For fiscal policy domains, proportional tax exposure captures material stakes: tax burden as a share of income or wealth rather than absolute dollar amounts. This proportional approach prevents plutocratic weighting where billionaires' large absolute tax payments generate claims to outsized consent power despite negligible capability impacts. Complementary measures include benefit incidence analysis allocating government spending to affected populations, effective tax rate calculations accounting for loopholes and evasion, and lifetime tax exposure estimates incorporating age-earnings profiles.

Environmental policy stakes can be proxied through physical exposure measures: land area threatened by sea level rise for climate policy, air quality exposure indices for pollution regulation, and water security indicators for resource management. The Environmental Justice Screening Tool developed by the US EPA provides census-tract-level vulnerability indices combining pollution exposure, demographic sensitivity (age, health status), and socioeconomic capacity to adapt. Such indices can be aggregated to agent-level stakes estimates s_i(d_environment) reflecting heterogeneous consequence distributions.

Healthcare policy stakes reflect health outcome sensitivity to policy changes. Measures include disease burden indices (disability-adjusted life years affected), healthcare utilization patterns (frequency of care, chronic condition management), insurance dependency (share of healthcare costs covered vs. out-of-pocket), and geographic access (distance to facilities, provider availability). The stakes in healthcare reform differ dramatically between chronically ill populations dependent on subsidized care (high s_i) and healthy wealthy populations with private coverage (low s_i despite potential absolute cost exposure).

Labor policy stakes can be measured through employment concentration indices: workers in industries facing regulatory changes have higher stakes than those in unaffected sectors. Industry-specific measures include union density (higher stakes in collective bargaining regulation for unionized workers), workplace safety incident rates (higher stakes in safety regulation for dangerous industries), and wage distribution positions (higher stakes in minimum wage policy for low-wage workers). Geographic concentration of affected employment amplifies stakes for local communities depending heavily on regulated industries.

Existential stakes present the highest measurement priority but greatest conceptual challenges. Climate policy poses existential threats to low-lying island nations facing territorial submersion; reproductive regulation poses existential stakes for pregnant people whose bodily autonomy and health hang in balance; religious freedom policy poses existential stakes for persecuted minorities. These stakes don't reduce to material exposure or capability impacts in conventional terms—they involve survival, identity, and fundamental autonomy. Proxy measurement strategies include binary indicators for populations facing existential risks, surveys measuring perceived existential threat, and revealed preference from migration patterns when exit is feasible.

Cross-domain aggregation of stakes raises additional complications. An agent may hold high stakes in multiple domains d₁, d₂, ..., d_k, but should these sum additively or exhibit complementarities? An impoverished climate-vulnerable single mother faces compounded stakes across income support, environmental protection, healthcare access, and education policy. The framework accommodates both specifications: additive stakes s_i(total) = Σ_d s_i(d) or interactive stakes reflecting that combined vulnerabilities exceed component sums. Empirically, this distinction matters for alignment measurement: additive specifications permit domain-by-domain α(d) assessment, while interactive specifications require assessing α across the full policy portfolio jointly.

#### 5.4.3 Friction Outcome Measures

Friction manifests through observable political instability that can be quantified using event data, institutional records, and behavioral indicators. Protest incidence provides a direct friction measure: the frequency, size, and intensity of demonstrations reveal stakeholder dissatisfaction with realized outcomes. Cross-National Time-Series Data Archive, Social Conflict Analysis Database, and Armed Conflict Location & Event Data Project (ACLED) provide standardized protest event data enabling cross-national and temporal comparisons. Measures include protests per capita (accounting for population), participant counts (scaling friction by mobilization magnitude), and protest violence (distinguishing peaceful assembly from riots indicating extreme friction).

Strike activity offers another friction indicator, particularly for labor relations domains. Days lost to strikes per thousand workers, strike frequency, and strike duration all proxy for F(d_labor). The International Labour Organization compiles strike statistics enabling comparative analysis, though measurement consistency varies across countries with different reporting standards. Wildcat strikes occurring outside official union channels indicate especially high friction, suggesting that even incorporated stakeholders' tolerance thresholds τ_i have been exceeded.

Litigation volume captures friction channeled through legal contestation rather than street mobilization. Case filing rates in administrative challenges, constitutional litigation, and civil suits related to specific policies reveal stakeholder resistance to implemented actions. The friction interpretation depends on legal system accessibility: in systems with low litigation costs and strong judicial review, high case volumes may reflect moderate friction channeled efficiently; in systems with prohibitive access barriers, even modest litigation indicates severe underlying friction given the mobilization threshold required.

Policy reversal rates and instability provide indirect friction measures. Frequent policy changes, oscillating between opposed positions across electoral cycles, suggest persistent failure to achieve stable equilibrium reflecting stakeholder preferences. Panel data on policy adoption and repeal across jurisdictions and time periods can measure this volatility, with higher variance indicating greater friction. Similarly, implementation gaps—differences between legislated policy and actual enforcement—reveal friction manifesting as non-compliance or bureaucratic resistance.

Noncompliance rates directly measure behavioral friction: tax evasion levels for fiscal policy, regulatory violation frequency for business regulation, and shadow economy size for labor market regulation. High noncompliance indicates that realized policy x_d diverges sufficiently from affected populations' preferences that they resist through evasion despite sanctions. Empirical measures include tax gaps (difference between owed and collected revenue), workplace safety violation citations, and informal employment shares.

Exit options provide market-analog friction measures. Capital flight from heavily taxed jurisdictions, emigration from oppressive regimes, and platform exodus from algorithmic governance changes all reveal friction through exit rather than voice (Hirschman, 1970). Migration data, foreign direct investment flows, and user retention rates on digital platforms proxy for F(d) when stakeholders possess mobility. The Tiebout sorting model (Tiebout, 1956) predicts that heterogeneous jurisdictions enable preference matching through migration, reducing friction; empirically testing this prediction requires measuring both α(d) variation across jurisdictions and migration flows responding to legitimacy deficits.

#### 5.4.4 Estimating Legitimacy Effects

The core empirical prediction connecting alignment to friction generates testable hypotheses through panel regression specifications. Consider a baseline model:

F_{d,t} = β₀ + β₁·α_{d,t} + β₂·P_{d,t} + γ·X_{d,t} + μ_d + λ_t + ε_{d,t}

where F_{d,t} represents friction in domain d at time t (measured through protest incidence, litigation, or other proxies), α_{d,t} denotes consent alignment, P_{d,t} captures performance outcomes (policy effectiveness, economic growth, public goods provision), X_{d,t} includes control variables (GDP per capita, urbanization, media freedom), μ_d represents domain fixed effects absorbing time-invariant domain characteristics, λ_t represents time fixed effects absorbing common shocks, and ε_{d,t} is the error term.

The framework's theoretical predictions constrain coefficient signs: β₁ < 0 (higher alignment reduces friction), β₂ < 0 (better performance reduces friction), with magnitudes reflecting society-specific weights w₁ and w₂ from Theorem 4's legitimacy function L = w₁·α + w₂·P. If alignment matters more than performance for legitimacy, we expect |β₁| > |β₂|; if performance dominates, the inequality reverses. Cross-national heterogeneity in these coefficients can be explored through interaction terms:

F_{d,t} = β₀ + (β₁ + θ₁·Democracy_d)·α_{d,t} + (β₂ + θ₂·Democracy_d)·P_{d,t} + ...

where Democracy_d indexes democratic quality. The framework predicts θ₁ > 0: democratic societies weighting voice highly (large w₁) exhibit stronger alignment-friction relationships. Conversely, autocracies prioritizing performance may show θ₂ < 0: performance matters more for stability when voice is suppressed.

Endogeneity threatens causal interpretation of these specifications through multiple channels. Reverse causality arises if high friction F_{d,t} causes changes in alignment α_{d,t+1} rather than vice versa—precisely the dynamic the framework predicts in Section 7's historical cases, where friction drives institutional reform expanding consent-holding. Simultaneity occurs if unmeasured factors simultaneously affect both alignment and friction: economic crises may reduce both performance (raising F) and state capacity to maintain inclusive institutions (reducing α). Measurement error in alignment and stakes proxies attenuates coefficient estimates through classical errors-in-variables bias.

Instrumental variable strategies address these endogeneity concerns by exploiting exogenous variation in consent structures. Historical franchise expansions driven by international diffusion provide quasi-experimental variation: countries adopting suffrage extensions following neighbors' reforms experience alignment increases plausibly orthogonal to contemporaneous friction. Regression discontinuity designs can estimate alignment effects in settings with population thresholds triggering institutional rules: Swiss cantons requiring minimum populations for direct democracy initiatives, corporate governance requirements applying above shareholder count thresholds, or algorithmic governance regulations triggered by platform user counts.

Difference-in-differences specifications compare jurisdictions adopting alignment-increasing reforms to control jurisdictions, identifying treatment effects under parallel trends assumptions:

F_{j,t} = β₀ + β₁·Treated_j·Post_t + μ_j + λ_t + ε_{j,t}

where Treated_j indicates jurisdictions adopting reforms (expanded franchise, worker codetermination, participatory budgeting) and Post_t indicates post-reform periods. The coefficient β₁ estimates the causal effect of alignment increases on friction, identified by comparing treated jurisdictions' pre-post friction changes to control jurisdictions' contemporaneous changes. Validity requires that absent treatment, treated and control jurisdictions would have exhibited parallel friction trends—an assumption testable through pre-treatment period analysis and falsifiable through placebo timing tests.

Synthetic control methods (Abadie et al., 2010) improve upon difference-in-differences when suitable control groups are unavailable by constructing weighted combinations of control units matching treated units' pre-treatment characteristics. This approach has been successfully applied to comparative policy evaluation (Brexit effects, German reunification, tobacco control) and naturally extends to consent-holding reforms. Estimate a synthetic control unit matching the treated jurisdiction's pre-reform friction trajectory, then compare post-reform actual friction to synthetic counterfactual friction. Large divergences identify alignment treatment effects, with inference conducted through permutation tests comparing observed gaps to placebo treatment gaps in control units.

Panel vector autoregressions (VARs) enable analysis of dynamic feedback between alignment, friction, and performance without requiring exogeneity assumptions. Specify:

F_t = α₁·F_{t-1} + β₁·α_{t-1} + γ₁·P_{t-1} + ε_{F,t}
α_t = α₂·F_{t-1} + β₂·α_{t-1} + γ₂·P_{t-1} + ε_{α,t}
P_t = α₃·F_{t-1} + β₃·α_{t-1} + γ₃·P_{t-1} + ε_{P,t}

This system allows friction to affect future alignment (β₂ coefficient), alignment to affect future friction (β₁ coefficient), and both to interact with performance dynamics. Granger causality tests assess whether alignment Granger-causes friction (β₁ ≠ 0 even after controlling for lagged friction and performance), providing evidence for predictive relationships. Impulse response functions trace out the time path of friction following an exogenous alignment shock, identifying both immediate effects and longer-term adjustment dynamics as institutions and stakeholder expectations adapt.

#### 5.4.5 Methodological Challenges and Extensions

Several methodological challenges complicate empirical implementation of the framework and warrant acknowledgment. Revealed versus stated preferences in stakes measurement presents a fundamental tension: should we measure s_i(d) through behavioral proxies (tax exposure, health outcomes) or survey responses (perceived importance, reported concern)? Behavioral measures avoid strategic misreporting but may miss subjective valuations diverging from objective exposure. Survey measures capture subjective stakes but suffer from hypothetical bias, social desirability effects, and manipulation incentives when stakes determine consent power allocation.

The framework's theoretical structure suggests privileging revealed preference where feasible, supplemented by stated preference validation. If behavioral stakes proxies s^{revealed}_i correlate strongly with survey stakes s^{stated}_i, both likely capture true underlying stakes. If they diverge, behavioral measures provide conservative estimates resistant to strategic inflation, while divergence magnitude informs measurement uncertainty. Elicitation methods from contingent valuation and discrete choice experiments can improve stated preference accuracy through incentive compatibility, though perfect truth-telling remains elusive when stakes allocate power.

Endogeneity of stakes and consent power presents a deeper conceptual challenge. The framework treats s_i(d) as exogenous exposure to consequences and C_i,d as institutional allocation of authority. But consent power may causally affect stakes: possessing political voice enables policy influence that reduces one's exposure to adverse consequences. Wealthy elites holding high C_i,d through campaign finance and lobbying influence subsequently face low s_i(d_tax) because they shape tax policy favorably. This reverse causality implies that observed correlations between stakes and consent power reflect equilibrium outcomes of dynamic processes rather than exogenous alignments.

Addressing this endogeneity requires modeling institutional change explicitly. Specify a dynamic system where today's friction F_t feeds back into tomorrow's consent allocation C_{t+1} through political mobilization, and consent power affects future stakes through policy influence. Equilibrium analysis then characterizes steady states where friction-driven consent expansion balances consent-enabled stakes reduction. Empirically, this suggests estimating structural models with simultaneous equations for stakes, consent, and friction rather than reduced-form regressions treating these as independent.

Cross-national comparability of stakes and friction measures raises external validity concerns. Protests in liberal democracies with protected assembly rights may reflect lower mobilization thresholds (smaller τ_i) than protests in autocracies where participation risks severe repression. Observing higher protest rates in democracies doesn't necessarily indicate higher friction—it may reflect lower costs to friction manifestation. Controlling for institutional freedom indices and protest repression partially addresses this, but ideal comparisons would estimate latent friction F* adjusted for manifestation costs, distinguishing true friction from observed resistance.

Temporal dynamics and institutional memory complicate longitudinal analysis. The framework's static formulation assumes preferences x*_i,d and stakes s_i(d) are contemporaneously determined, but both likely depend on historical experiences. Populations that suffered severe policy failures develop higher stakes in preventing recurrence (post-Weimar German constitutional design prioritizing stability, post-2008 financial regulation sensitivity). Preferences adapt to institutional possibilities: citizens in long-stable democracies may have ideal points x*_i,d centered near status quo through endowment effects, while those experiencing recent institutional change maintain more radical preferences. Incorporating institutional memory through distributed lag models or Bayesian updating frameworks could enrich the temporal analysis.

Measurement of effective voice (eff_voice_i) versus formal consent power (C_i,d) requires rich data on capacity constraints often unavailable in cross-national datasets. While formal voting rights follow from constitutional analysis, effective voice depends on literacy, political knowledge, media access, associational freedoms, and psychological political efficacy—variables measured inconsistently across contexts. Survey data from World Values Survey, Afrobarometer, and Latinobarómetro provide partial coverage, but missing data and comparability concerns limit comprehensive eff_voice_i estimation. Researchers must often proceed with formal consent power proxies, acknowledging that capacity constraints imply estimated α(d) overstates true effective alignment.

Aggregation of domain-specific alignment into overall legitimacy indices presents normative and technical challenges. Should we weight α(d₁) and α(d₂) equally when aggregating across domains, or should weights reflect aggregate stakes Σ_i s_i(d)? Equal weighting treats all domains as equally important regardless of consequence magnitude; stakes-weighting prioritizes alignment in high-impact domains. The framework takes no position but enables both specifications:

α_{equal} = (1/M)·Σ_d α(d)
α_{stakes-weighted} = Σ_d (Σ_i s_i(d))·α(d) / Σ_d Σ_i s_i(d)

Empirically comparing these aggregation rules' friction predictions can inform which better captures legitimacy dynamics. If stakes-weighted aggregation predicts observed friction F(d) more accurately, this validates that legitimacy deficits in high-stakes domains destabilize systems more than equivalent deficits in trivial domains.

### 5.5 Testable Predictions and Empirical Hypotheses

The operationalization framework generates precise empirical predictions testable through the identification strategies above. These hypotheses span cross-sectional patterns, longitudinal dynamics, and causal mechanisms, providing multiple angles for empirical validation or falsification.

**Hypothesis 1 (Alignment-Friction Relationship)**: Across domains and time periods, higher consent alignment α(d,t) predicts lower friction F(d,t+k) with lags k reflecting institutional adjustment speeds. Specifically:

∂F(d,t+k)/∂α(d,t) < 0 for k ≥ 0

This prediction follows directly from Theorem 3: alignment measures the extent to which decision power concentrates among high-stakes populations; when such populations possess voice, realized outcomes x_d shift toward their preferences x*_i,d, reducing deviation δ(x_d, x*_i,d) and thereby lowering friction F(d,t).

**Hypothesis 2 (Stakes-Consent Covariance)**: Institutional reforms increasing the covariance between stakes and consent power—Cov(s_i(d), C_i,d)—reduce friction through alignment improvement. This follows from decomposing alignment:

α(d) = Σ_i s_i(d)·C_i,d / Σ_i s_i(d) = E[s·C] / E[s]

where the numerator benefits from positive correlation between stakes and consent power. Reforms expanding franchise to previously excluded high-stakes populations (suffrage extensions, worker codetermination, participatory budgeting) increase this covariance and therefore alignment.

**Hypothesis 3 (Threshold Effects)**: Domains with alignment below societal tolerance thresholds—α(d) < τ_legitimacy—exhibit discontinuously higher instability. This generates testable predictions of nonlinearity in the alignment-friction relationship:

∂²F/∂α² > 0 near τ_legitimacy (convexity in alignment)

Empirically, this implies that small alignment improvements have modest friction effects when α is well below or above τ, but large effects when α crosses the legitimacy threshold.

**Hypothesis 4 (Temporal Dynamics)**: Persistent friction F(d,t) predicts future alignment increases α(d,t+k) through institutional reform pressure. This feedback relationship generates predictable institutional evolution:

∂α(d,t+1)/∂F(d,t) > 0

The mechanism operates through elite responses to instability: sustained friction raises the costs of maintaining exclusionary institutions, eventually inducing incorporation of excluded stakeholders or system collapse (Section 7 historical analysis).

**Hypothesis 5 (Performance Interactions)**: The alignment-friction relationship weakens in domains with high performance P(d,t), as competent governance partially compensates for voice deficits:

∂F/∂α < 0, but |∂F/∂α| decreases as P increases

This interaction follows from Theorem 4's legitimacy function L = w₁·α + w₂·P: performance and alignment substitute imperfectly. Empirically, this predicts that technocratic systems achieve stability at lower α(d) than poorly performing democracies, though the substitution is bounded by minimum voice requirements when w₁ > 0.

**Hypothesis 6 (Domain Heterogeneity)**: High-stakes technical domains (nuclear safety, pandemic response) exhibit weaker alignment-friction relationships than high-stakes value domains (constitutional rights, community norms):

|∂F/∂α|_{technical} < |∂F/∂α|_{value}

This reflects appropriate domain-specific weighting: technical domains rationally set w₂ > w₁ (prioritizing competence), while value domains set w₁ > w₂ (prioritizing consent). Stakeholders tolerate lower α in technical domains if performance is high.

**Hypothesis 7 (Capacity Constraints)**: The effective alignment measure α_effective incorporating capacity constraints predicts friction more accurately than formal alignment α_formal using nominal consent power:

R²(F ~ α_effective) > R²(F ~ α_formal)

This validates the importance of distinguishing formal authority from effective voice: oppressive structures that suppress capacity reduce legitimacy even when formal inclusion obtains.

**Hypothesis 8 (Exit Options)**: In domains where exit is feasible (mobile capital, skilled labor, platform users), misalignment generates friction through exit rather than voice:

∂(Exit_rate)/∂(1-α) > 0 when mobility_costs are low

This Hirschman prediction suggests that measuring friction purely through protest and resistance understates true friction in competitive environments where dissatisfied stakeholders can defect.

These hypotheses collectively provide a comprehensive empirical agenda for testing the consent-holding framework. Confirming these predictions across diverse contexts would validate the framework's core insights about legitimacy structure; falsification would identify boundary conditions or necessitate theoretical revision.

## 7. Historical Validation: Seven Case Studies in Consent Alignment Dynamics

The framework's predictive power rests on historical validation. We examine seven domains where consent alignment α(d) evolved over time, generating observable friction F(d) when misaligned and stability when aligned. Each case demonstrates the framework's core prediction: persistent low α(d) generates escalating friction until institutional reform raises alignment above threshold τ, or suppression temporarily contains mobilization at unsustainable cost. These cases span two centuries, multiple continents, and radically different governance contexts—political enfranchisement, economic organization, identity-based movements, colonial structures, digital platforms, and intergenerational policy. The regularity of dynamics across such heterogeneous domains suggests the framework captures fundamental institutional processes rather than domain-specific particularities.

### 7.1 Abolition Movements (1780s-1865): Maximum Stakes, Zero Consent

Enslaved people in the Atlantic world possessed the highest conceivable stakes in the freedom domain—sᵢ(d) approached infinity as the difference between chattel bondage and human liberty represents an existential binary. Yet consent power remained exactly zero: enslaved persons held no legal personhood, no voice in legislative processes determining their status, no recognized authority over their own bodies or labor. This configuration—maximum stakes combined with absolute exclusion from H_t(d)—produced α(d_slavery) approaching zero, the most severe consent misalignment structurally possible. Enslavers, conversely, held Cᵢ ≈ 1 within their local domains through property law, enforcement mechanisms, and legislative representation weighted toward slave states. The framework predicts such catastrophic misalignment generates friction proportional to stakes magnitude; with existential stakes, friction becomes unbounded.

Friction manifested across multiple channels with escalating intensity. Slave revolts erupted throughout the Americas: the Haitian Revolution (1791-1804) successfully overthrew French colonial rule, Nat Turner's rebellion (1831) in Virginia killed sixty whites before brutal suppression, and countless smaller uprisings punctuated plantation regimes. Maroon communities—escaped slaves forming autonomous societies in remote areas—represented sustained exit from the system, geographic friction beyond elite control. The Underground Railroad constituted systematic mass non-compliance with Fugitive Slave Laws, demonstrating that low α(d) generates not merely individual defection but organized resistance infrastructure. Abolitionist movements channeled friction through those with moral stakes (religious groups viewing slavery as sin) and economic stakes (Northern industrialists competing with slave-produced goods). Each friction channel reflected the same underlying dynamic: when those with maximum sᵢ(d) possess zero Cᵢ, the system generates forces proportional to the stakes-consent gap.

The American Civil War represents friction overflow—when α(d) remains near zero despite escalating pressure, the system breaks catastrophically rather than reforming incrementally. Six hundred thousand deaths demonstrate that existential-stakes domains cannot sustain near-zero consent alignment indefinitely; either α(d) rises or the polity fractures. The 13th Amendment (1865) formally abolished slavery, technically raising α(d) by granting legal personhood to formerly enslaved people. However, effective consent power remained suppressed through Black Codes, convict leasing systems, sharecropping arrangements that replicated bondage economically, and systematic disenfranchisement. This reveals a critical framework distinction: de jure α(d) increases without corresponding de facto Cᵢ gains merely displace friction to new domains rather than resolving underlying legitimacy deficits. Jim Crow laws perpetuated low α(d) for nearly a century post-abolition, generating sustained friction that eventually erupted in the Civil Rights Movement.

The abolition case illuminates why gradual compensation schemes failed. Some proposed buying slaves' freedom incrementally over decades to reduce economic disruption and lower immediate friction. The framework explains failure: when stakes are existential (freedom versus slavery represents a non-negotiable binary), affected agents cannot accept extended timelines that leave α(d) ≈ 0 for generations. Tolerance thresholds τᵢ collapse toward zero as stakes approach infinity; gradual transitions appear indistinguishable from permanent exclusion. This contrasts with lower-stakes domains where incremental α(d) increases prove acceptable—abolition required immediate, substantial consent expansion because sᵢ(d) magnitudes admitted no compromise. The case also demonstrates Theorem 2's prediction: even "benevolent" enslavers claiming paternalistic concern for enslaved people could not eliminate friction, because F(d) derives from stakes-weighted preference divergence, not consent-holders' intentions. Only α(d) increase—actual redistribution of consent power—resolves friction when stakes are high.

### 7.2 Women's Suffrage (1890s-1970s): Meta-Consent Power via Voting Rights

Pre-1890 institutional arrangements excluded women from H_t(d_voting) across nearly all jurisdictions, despite women possessing sᵢ(d) > 0 in every policy domain. Laws governing property rights, employment conditions, educational access, family structure, taxation, and war mobilization affected women directly, yet women held Cᵢ = 0 in determining these policies. This produced α(d) ≈ 0.5 across all domains if calculated by population share—half of all affected stakeholders systematically excluded. The severity compounds when recognizing that voting rights constitute meta-consent: those holding Cᵢ in electoral domains indirectly control H_t(d) for all subordinate policy domains through legislative authority. Excluding women from voting thus excluded them from the primary mechanism for influencing consent allocations across the entire governance structure. Initial α(d_voting) therefore approximated zero for a population segment with universal positive stakes.

Friction manifestations varied by jurisdiction but exhibited common patterns of escalating intensity. In Britain, the Women's Social and Political Union (WSPU) employed increasingly militant tactics after decades of unsuccessful petitioning: window smashing campaigns, arson targeting unoccupied buildings, hunger strikes during imprisonment, and forced feeding by authorities that generated public outrage. Emily Davison's death in 1913—stepping in front of the King's horse at the Epsom Derby—represented friction overflow from an individual unable to tolerate persistent exclusion. In the United States, the National Woman's Party picketed the White House continuously beginning in 1917, with participants enduring imprisonment, abuse, and forced feeding similar to British suffragettes. The sustained multi-decade mobilization across both nations—requiring coordination, resource mobilization, and willingness to endure state violence—demonstrates that high sᵢ(d) combined with zero Cᵢ generates friction sufficient to overcome collective action problems and state suppression efforts.

Consent alignment rose incrementally across jurisdictions from 1893 to 1971, following a pattern of pioneers followed by rapid diffusion. New Zealand granted women's suffrage in 1893, establishing proof of concept that women's voting wouldn't collapse social order (a common elite fear). Finland (1906) and Norway (1913) followed during periods of broader democratic reform. Britain extended partial suffrage in 1918 to women over 30 who met property requirements—a partial α(d) increase reflecting elite efforts to incorporate women while maintaining class-based restrictions—before achieving equal franchise in 1928. The United States ratified the 19th Amendment in 1920 after intensive state-by-state campaigns, though Southern states systematically disenfranchised Black women through the same mechanisms excluding Black men. Switzerland's delayed suffrage until 1971 reveals that even highly democratic systems can sustain low α(d) in particular domains when cultural norms support exclusion; friction eventually forced change even in this outlier case.

The framework insights from suffrage movements center on threshold effects and friction collapse patterns. Once α(d_voting) crossed a critical threshold—approximately when one-third to one-half of affected populations gained formal Cᵢ—friction declined precipitously. Pre-suffrage decades featured constant protests, civil disobedience, hunger strikes, and state repression. Post-suffrage, this friction largely evaporated as women's political energy channeled into conventional electoral participation and policy advocacy. However, F(d) didn't vanish entirely; it merely shifted to domains where women remained excluded from H_t(d), particularly corporate governance and religious leadership hierarchies. This demonstrates Theorem 3's prediction: legitimacy is domain-specific, and raising α(d_voting) doesn't automatically raise α(d_corporate) or α(d_religious). Contemporary gender-based friction in workplace governance and religious institutions represents the same underlying dynamic—high sᵢ(d) with low Cᵢ generates instability until consent alignment rises above τ. The suffrage case also illustrates how voting rights function as meta-consent: expanding H_t(d_voting) enables affected populations to subsequently reshape H_t(d) across multiple subordinate domains through legislative action, creating cascading legitimacy improvements across the institutional landscape.

### 7.3 Labor Rights (1811-Present): Capital Versus Workers in Workplace Domains

The Industrial Revolution created workplace domains where workers possessed extreme stakes—sᵢ(d_wages), sᵢ(d_hours), sᵢ(d_safety) determined survival, health, and life prospects—while capital held Cᵢ → 1 through property rights and unilateral employment contracts. Factory owners set wages, hours, and conditions without worker input; employment remained "at will" in most jurisdictions, enabling dismissal without cause. This generated α(d_workplace) ≈ 0 across industrial economies: those bearing the consequences of workplace governance possessed no voice in determining outcomes. Workers' existential dependence on wages—inability to survive without employment—meant tolerance thresholds τᵢ were correspondingly low. Even minimal deviations from subsistence wages or tolerable conditions generated friction because workers' capability constraints left no margin for error. The framework predicts such configurations produce sustained instability proportional to stakes magnitude; early industrial capitalism featured precisely this pattern.

Friction erupted in waves of escalating intensity and sophistication. The Luddite uprisings (1811-1816) targeted machinery directly, destroying the capital goods enabling worker displacement and wage suppression. This represented friction channeled through available mechanisms when formal political Cᵢ remained zero—if you cannot vote on factory policy, destroy the factory. State response through the Frame Breaking Act (1812) made machine-breaking a capital offense, executing seventeen Luddites in 1813. Suppression without consent expansion merely displaced friction: the 1819 Peterloo Massacre killed fifteen protesters demanding parliamentary reform, while the Combination Acts criminalized worker organizing. Early unions formed despite legal prohibition, demonstrating that when sᵢ(d) is existential, affected agents accept substantial personal risk to raise collective Cᵢ. Strikes constituted coordinated friction: workers' primary leverage derived from withholding labor, imposing costs on capital proportional to production disruption.

Collective bargaining rights emerged incrementally across Western economies between 1870 and 1940, raising α(d_workplace) substantially. Britain's Trade Union Act (1871) legalized unions, granting them legal standing and contract enforcement capacity. This didn't grant workers Cᵢ = 0.5 but moved α(d) from near-zero to perhaps 0.2—unions could negotiate but lacked binding authority over workplace governance. The Wagner Act (1935) in the United States protected organizing rights and established mandatory collective bargaining, further raising α(d) by compelling employers to negotiate with certified unions. International Labour Organization Convention 87 (1948) codified freedom of association globally, embedding labor rights in international law. The period 1945-1975 represents peak α(d_workplace) in Western economies: union density approached 35% in the United States and exceeded 50% in many European nations, workers gained board representation in German codetermination systems, and wages tracked productivity growth closely. Friction during this period declined markedly—strikes occurred but represented negotiation tactics within accepted frameworks rather than revolutionary violence challenging the system's legitimacy.

Modern dynamics reveal α(d) evolution is non-monotonic; consent alignment can decline as well as rise, generating renewed friction through novel channels. Union membership collapsed from 1980 onward: United States density fell from 35% in the 1950s to 10% by 2020, reflecting policy changes (right-to-work laws), sectoral shifts (manufacturing decline), and coordinated anti-union campaigns. This represents α(d_workplace) declining as workers' collective Cᵢ erodes. The framework predicts declining α(d) generates rising F(d) with lags; contemporary friction manifests as "quiet quitting" (minimal effort matching minimal compensation), "lying flat" movements in China (rejecting intensive work culture entirely), and the Great Resignation (2021-2022) where millions exited unsatisfactory employment. These represent friction channels adapted to contexts where traditional strikes prove ineffective—when workers lack unions (low collective Cᵢ), individual exit becomes the primary friction mechanism. Labor scholars note rising dissatisfaction, declining employee engagement, and increasing strikes in 2023-2024 after decades of quiescence. The framework interprets this as F(d) rising in response to declining α(d), predicting either renewed consent expansion (new organizing models, worker board representation, employee ownership) or continued instability. The stakes haven't declined—workers still depend on wages for survival—but consent power has contracted, reopening legitimacy gaps that mid-century institutions temporarily closed.

### 7.4 Civil Rights Movements (1950s-1970s): Race-Based Disenfranchisement

The post-Civil War constitutional amendments theoretically granted Black Americans political equality: the 13th abolished slavery (1865), the 14th guaranteed equal protection (1868), and the 15th prohibited race-based voting restrictions (1870). Reconstruction (1865-1877) briefly raised α(d) as Black men voted, held office, and participated in governance across Southern states. However, the 1877 Compromise ending Reconstruction withdrew federal enforcement, enabling systematic re-imposition of exclusion through legal mechanisms (literacy tests, poll taxes, grandfather clauses), extra-legal violence (lynching, intimidation, Ku Klux Klan terrorism), and social norms (Jim Crow segregation). By 1900, α(d_voting) for Black Americans in the South had returned to near-zero despite formal constitutional protections, demonstrating that de jure rights without enforcement produce hollow legitimacy gains. Across all policy domains—education, employment, housing, criminal justice, public accommodations—Black Americans possessed high sᵢ(d) while holding Cᵢ ≈ 0. This configuration persisted for ninety years post-abolition, generating accumulated friction awaiting catalytic release.

Friction exploded during the 1950s-1960s through mass mobilization that made consent deficits nationally visible. The Montgomery Bus Boycott (1955-1956) demonstrated sustained collective action capacity: 40,000 Black residents refused segregated public transit for 381 days despite economic hardship and violent retaliation, generating sufficient friction to force policy change through economic pressure when political Cᵢ remained zero. The sit-in movement (1960) spread rapidly across Southern states as students occupied segregated lunch counters, forcing confrontations that revealed the violence sustaining low α(d). Freedom Rides (1961) deliberately provoked violent responses to interstate bus desegregation, generating national attention through televised brutality. The Birmingham Campaign (1963) strategically escalated friction until it overflowed on national television: police dogs, fire hoses, and mass arrests of children created legitimacy crises for the broader American polity. The Selma to Montgomery marches (1965) culminated in "Bloody Sunday," where state troopers attacked peaceful marchers on the Edmund Pettus Bridge—friction visualized so starkly that federal intervention became politically necessary.

Legislative responses raised α(d) formally but enforcement remained contested. The Civil Rights Act (1964) prohibited discrimination in public accommodations and employment, technically raising α(d) by granting Black Americans legal standing to contest exclusions. The Voting Rights Act (1965) proved more consequential by eliminating literacy tests and imposing federal oversight (preclearance requirements) on jurisdictions with histories of discrimination. This raised α(d_voting) substantially: Black registration rates in Mississippi rose from 6% (1964) to 59% (1967), demonstrating that consent alignment can shift rapidly when enforcement mechanisms back formal rights. However, friction didn't vanish; it shifted to domains where α(d) remained low—criminal justice (mass incarceration rose dramatically from 1970 onward), housing (redlining and segregation persisted), education (school funding disparities), and economic opportunity (wealth gaps widened). This illustrates the framework's domain-specificity: raising α(d_voting) doesn't automatically raise α(d_criminal_justice) or α(d_housing), requiring separate institutional reforms in each domain.

Modern regression demonstrates α(d) can decline even in previously reformed domains, regenerating friction predictably. Shelby County v. Holder (2013) struck down the Voting Rights Act's preclearance formula, removing federal oversight of jurisdictions with discrimination histories. States immediately enacted voter ID laws, polling place closures in minority neighborhoods, voter roll purges, and registration restrictions—all mechanisms reducing Cᵢ for populations with high sᵢ(d_voting). Research documents differential impacts: strict ID laws reduce Black turnout 2-3 percentage points more than white turnout, polling place closures increase wait times disproportionately in minority precincts, and registration purges disproportionately affect Black and Hispanic voters. This represents α(d_voting) declining from its 1970-2010 peak, as those with high stakes lose effective voice through procedural barriers. The framework predicts rising F(d) should follow declining α(d) with institutional lags; contemporary evidence supports this through increased litigation (friction channeled through courts when electoral friction is suppressed), protest movements (Black Lives Matter mobilizations), and heightened political polarization around voting access. The case demonstrates Theorem 3's warning: legitimacy is dynamic, requiring sustained enforcement to maintain α(d) above threshold τ. When enforcement lapses, consent alignment declines, friction rises, and the cycle of struggle resumes.

### 7.5 LGBT Rights (1969-2015): Non-Linear Path from Criminalization to Recognition

Pre-1969 institutional arrangements criminalized homosexual conduct in most jurisdictions, rendering LGBT individuals not merely excluded from H_t(d) but actively persecuted by state authority. Sodomy laws made sexual orientation an imprisonable offense; police raids on gay establishments constituted routine harassment; employment discrimination, housing exclusion, and family law (child custody denial) systematically marginalized LGBT people. Across all relevant domains—marriage, employment, housing, anti-discrimination protections, criminal law, family formation—LGBT people possessed extreme sᵢ(d) (these policies determined fundamental life prospects and physical safety) while holding Cᵢ ≈ 0 (political invisibility through forced closeting, no legal recognition, no organized advocacy infrastructure). This generated α(d) → 0 across all LGBT-relevant domains. The stakes were existential in multiple senses: criminalization threatened literal imprisonment, social ostracism destroyed economic opportunities, and the AIDS crisis (1980s) made state policy a matter of life and death. Yet those facing these stakes remained systematically excluded from governance processes determining their fate.

Friction erupted dramatically during the Stonewall Riots (June 1969), representing a friction overflow event where accumulated pressure from persistent low α(d) exploded in response to routine police harassment. A raid on the Stonewall Inn gay bar—a typical enforcement action sustaining the exclusionary system—triggered spontaneous multi-day uprisings as patrons and community members fought back rather than accepting arrest. The riots weren't strategically planned but emerged organically from intolerable consent deficits: when α(d) approaches zero and sᵢ(d) is existential, even minor provocations generate disproportionate friction. Within weeks, the Gay Liberation Front formed, transforming spontaneous resistance into organized political mobilization. The AIDS crisis intensified stakes dramatically during the 1980s: as thousands died while government officials ignored or mocked the epidemic, sᵢ(d_health_policy) became literally existential. ACT UP (AIDS Coalition to Unleash Power) channeled this friction through radical direct action—die-ins blocking traffic, occupying the FDA headquarters demanding drug approval, disrupting St. Patrick's Cathedral mass. Each action demonstrated Theorem 2's prediction: when stakes are existential and consent approaches zero, affected populations will mobilize despite extreme personal costs.

Consent alignment rose incrementally through legal battles and legislative victories, though the path proved highly non-linear. Sodomy law decriminalization proceeded state-by-state from 1962 onward but didn't achieve national elimination until Lawrence v. Texas (2003)—forty-one years after Illinois's repeal. Employment discrimination protections remained absent in most jurisdictions until Bostock v. Clayton County (2020) extended Title VII protections, a sixty-year lag from initial organizing. Marriage equality followed an accelerating trajectory: Denmark allowed registered partnerships (1989), Massachusetts legalized same-sex marriage (2004), and Obergefell v. Hodges (2015) established nationwide recognition in the United States. This timeline reveals that α(d) can rise rapidly once threshold τ is crossed: marriage went from zero-state recognition to national right in eleven years (2004-2015), demonstrating that friction accumulation eventually produces cascading institutional change. Estimated α(d_marriage) rose from 0 (2000) to 0.3 (2010, reflecting state-level variation) to approaching 0.9 (2020) in liberal democracies, one of the fastest legitimacy expansions in modern history.

Framework insights center on non-linear friction dynamics and domain heterogeneity. LGBT rights demonstrate that friction spikes often precede breakthroughs rather than smooth incremental improvement: Stonewall (1969) preceded rapid organizing; ACT UP militancy (1987-1990) preceded treatment access expansions; marriage litigation campaigns generated backlash (Defense of Marriage Act, 1996) before ultimate victory (Obergefell, 2015). This suggests F(d) must exceed some visibility threshold before elites respond—low-level chronic friction can persist indefinitely, but overflow events force institutional adaptation. The case also reveals persistent low α(d) in subdomains despite overall progress: trans rights currently exhibit the same pattern LGBT rights showed in the 1970s—high sᵢ(d) in healthcare access, sports participation, bathroom use, and legal recognition, combined with Cᵢ ≈ 0 in many jurisdictions and substantial hostile mobilization. Contemporary trans-related friction (legislative battles, protest movements, policy reversals) represents the framework's predictions operating in real-time. The question becomes whether α(d_trans_rights) will follow the inclusion trajectory that gay/lesbian rights achieved, or whether suppression will succeed in maintaining low consent alignment. Historical patterns suggest friction from high stakes eventually forces either incorporation or unsustainable repression costs, making inclusion the likely equilibrium despite current contestation.

### 7.6 Platform Governance (2000-Present): Digital Colonialism and Algorithmic Authority

Digital platforms create governance domains affecting billions of users who possess high stakes in content moderation policies (sᵢ(d_moderation) determined by speech freedoms and harassment exposure), algorithmic curation (sᵢ(d_algorithms) determined by information access and attention manipulation), data use practices (sᵢ(d_data) determined by privacy and autonomy), and monetization rules (sᵢ(d_monetization) for creators dependent on platform income). Yet consent power concentrates almost entirely in platform executives and algorithmic systems: users hold Cᵢ ≈ 0 in determining the policies governing their digital lives. Facebook's 3 billion users collectively possess negligible voice in moderation standards, algorithmic timeline construction, or data collection practices despite these policies fundamentally shaping their information environment and social interactions. Twitter/X's governance decisions—content policy changes, verification systems, algorithmic adjustments—are made unilaterally by executives without user consultation. TikTok's algorithmic curation determines content visibility for hundreds of millions, yet users have zero input into ranking criteria or recommendation systems. This generates α(d_platforms) ≈ 0 across all major platform governance domains: those most affected by policies hold no meaningful decision authority.

Friction manifests through user exodus, regulatory backlash, and protest movements that channel dissatisfaction through available mechanisms. The Cambridge Analytica scandal (2018) revealed Facebook's data practices to be more invasive than users assumed, triggering the #DeleteFacebook movement where users threatened platform exit en masse. However, network effects limited exit effectiveness—platforms derive value from user bases, creating lock-in that reduces exit viability as friction channel. This demonstrates a framework limitation: when exit costs are prohibitive, even high F(d) may not force consent expansion if elites can ignore user dissatisfaction. Regulatory friction proved more consequential: the European Union's GDPR (2018) mandated consent requirements, data portability, and deletion rights, effectively forcing α(d_data) increases by granting users legal standing and procedural rights even without direct governance participation. The Digital Services Act (2022) imposed transparency obligations and appeals processes, further raising α(d) incrementally. These represent friction channeled through state authority—when direct user F(d) proves insufficient due to platform market power, affected populations petition governments to impose constraints, raising α(d) indirectly through regulation rather than internal governance reform.

Platform-specific friction dynamics demonstrate the framework's predictions about sustained low α(d). Twitter's transformation under Elon Musk's ownership (2022-2024) made consent deficits explicit: unilateral policy changes without user consultation, elimination of content moderation infrastructure, verification system overhauls, and brand changes occurred through pure executive authority. This represented α(d) → 0 made visible, as the fiction of user input was abandoned entirely. User migration followed predictably: millions joined Mastodon (decentralized alternative emphasizing user governance), Bluesky (designed with user-controlled algorithms), and Threads (Meta's Twitter competitor). The exodus validates Theorem 3's prediction that persistent low α(d) generates instability; when alternatives emerge offering higher α(d), users migrate despite switching costs. Meta's Oversight Board constitutes a minimal attempt to raise α(d_moderation) by creating independent appeals for content decisions, extending partial Cᵢ to users contesting removals. YouTube's Creator Councils consult high-profile content producers on policy changes, recognizing that creators possess extreme sᵢ(d_monetization) (professional creators depend on platform income for livelihood) and require some voice to prevent exodus. However, these remain largely tokenistic—Cᵢ still concentrates in executives and algorithms, with user input advisory rather than binding.

The framework generates clear predictions about platform stability based on consent alignment trajectories. Scenario one: platforms raise α(d) substantially through user councils with binding authority over moderation policies, algorithmic transparency enabling informed consent, participatory algorithm design including affected communities, and democratic governance structures. This would lower F(d), improve retention, and ease regulatory pressure as legitimacy deficits narrow. Estimated α(d) would need to reach approximately 0.4-0.5 (matching moderate representative democracy) to achieve stability; current α(d) ≈ 0.1 suggests substantial reforms required. Scenario two: platforms maintain current low α(d), relying on network effects and market power to retain users despite dissatisfaction. This generates rising F(d) manifesting as continued regulatory fragmentation (jurisdictions imposing conflicting requirements), exodus to decentralized alternatives (Mastodon, Bluesky adoption), and advertiser backlash from brand safety concerns. Historical precedent—MySpace's collapse, Tumblr's exodus following content policy changes—demonstrates that platforms sustaining low α(d) face eventual decline when alternatives emerge. The colonial parallel proves instructive: platforms extract value (attention, data) from populations while excluding them from governance, claim benevolent intentions ("connecting the world"), and face rising friction as affected populations mobilize. Colonial empires couldn't indefinitely maintain α(d) ≈ 0 despite military superiority; platforms similarly cannot sustain zero consent alignment indefinitely despite network effects. The question is whether α(d) rises through internal reform or external regulation, not whether it rises.

### 7.7 Climate Governance (1990-Present): Temporally Distributed Stakes

Climate change presents a unique challenge to consent-holding frameworks: those with the highest stakes—future generations facing catastrophic warming, sea level rise, ecosystem collapse, and resource scarcity—do not yet exist and therefore cannot participate in H_t(d_climate). Current generations hold Cᵢ → 1 in all climate-relevant decisions (emissions policies, energy infrastructure, land use, industrial regulation) while future generations hold Cᵢ = 0 by definition—they are literally absent from decision-making bodies. The stakes distribution is radically asymmetric: current generations face moderate costs from climate action (energy transition expenses, consumption changes) while future generations face existential threats (agricultural collapse, mass migration, conflict over resources). Estimating stakes quantitatively: current sᵢ(d_climate) might average 0.3 (significant but not immediately survival-threatening for most), while future sᵢ(d_climate) approaches 1.0 (survival directly at stake). This generates α(d_climate) ≈ 0.3 if weighted by current stakeholders only, but α(d_climate) ≈ 0.05 if future generations are included proportionally. Such severe misalignment predicts policy failures even when current actors possess good intentions.

Friction manifests primarily through proxies since future generations cannot mobilize directly. Youth climate movements (2018-present) represent the nearest proxy: young people whose lifespans overlap substantially with climate impacts organize strikes (Fridays for Future), protests demanding policy action, and civil disobedience (Extinction Rebellion, Ende Gelände). Greta Thunberg's activism demonstrates that even minors—who possess high sᵢ(d_climate) despite lacking voting rights—generate significant friction when consent deficits become visible. Intergenerational lawsuits attempt to channel friction through legal systems: Juliana v. United States (filed by youth plaintiffs) argues government climate inaction violates constitutional rights of future generations. These represent creative efforts to raise effective Cᵢ for those currently lacking formal voice. Indigenous communities function as another proxy, possessing both current high stakes (climate impacts on traditional lands) and cultural orientations emphasizing intergenerational responsibility. Their activism channels future stakes into present political processes despite numerical minority status.

Policy failures persist despite scientific consensus because low α(d_climate) produces systematically biased decision-making. Even well-intentioned current decision-makers face coordination problems: future generations cannot reciprocate current sacrifices, cannot enforce agreements, and cannot punish defection. This generates high discount rates in policy evaluation—future costs are weighted less heavily than present costs, reflecting the low Cᵢ of those bearing future consequences. International climate negotiations exhibit these dynamics clearly: the Paris Agreement (2015) relies on voluntary national commitments without enforcement mechanisms, emissions continue rising despite pledges, and adaptation finance remains far below estimated needs. The framework diagnoses this as structural rather than merely a failure of will: when α(d) ≈ 0.05 (future stakes massively outweigh current stakes but future voices have zero weight), policies optimizing current preferences inevitably underweight future harms. Fossil fuel interests hold substantial current Cᵢ through lobbying, campaign finance, and regulatory capture, while future generations hold zero Cᵢ—this asymmetry guarantees policy drift toward present interests even when long-term costs vastly exceed short-term benefits.

Proposed solutions attempt to raise α(d_climate) by granting institutional voice to future interests. Wales's Future Generations Commissioner (established 2015) provides formal Cᵢ > 0 to a guardian explicitly tasked with representing long-term interests in policy evaluation. New Zealand's environmental and intergenerational guardians serve similar functions. These representatives don't possess binding authority but can publicize when policies harm future generations, raising friction costs for present-focused decisions. Constitutional amendments embedding intergenerational equity—Ecuador's Rights of Nature (2008), France's Charter for the Environment (2005)—constrain permissible H_t(d) uses to protect future interests through meta-level rules. Mandatory climate impact assessments raise implicit weighting of sᵢ(future) in policy evaluation by forcing explicit consideration of long-term consequences. However, current α(d_climate) remains far below the threshold τ needed for stability. The framework predicts continued friction manifestations: youth radicalization as those inheriting climate consequences recognize their exclusion from H_t(d), intergenerational conflict over resource allocation (pensions versus climate spending), climate migration generating demographic pressure in destination countries, and eventual systemic collapse if α(d) remains near zero. Historical patterns from other domains suggest friction escalates until either incorporation occurs (raising α(d) through institutional reforms granting future interests substantial weight) or system breakdown (climate tipping points rendering policy irrelevant). The temporal dimension adds complexity but doesn't eliminate the fundamental dynamic: high stakes without voice generates instability proportional to the misalignment magnitude.

### 7.8 Synthesis: Common Patterns Across Historical Cases

These seven cases spanning two centuries, multiple continents, and radically different governance contexts exhibit striking regularities validating the framework's core predictions. First, every case demonstrates the predicted relationship between consent alignment and friction: when α(d) approaches zero despite high sᵢ(d), friction F(d) rises proportionally until either incorporation (α(d) increases through institutional reform) or suppression (elites use coercion to contain mobilization temporarily). Abolition, suffrage, labor rights, civil rights, LGBT rights, platform governance, and climate policy all follow this pattern despite heterogeneous domains, actors, and historical periods. This suggests the framework captures fundamental institutional dynamics rather than period-specific or domain-specific phenomena.

Second, a legitimacy threshold τ ≈ 0.6 emerges consistently across cases as the point where friction collapses from sustained mobilization to routine politics. Women's suffrage: pre-1920 featured constant protests and civil disobedience; post-1920 this friction vanished as women channeled energy into conventional electoral participation. Civil rights: Voting Rights Act (1965) raised α(d_voting) above 0.6 in many Southern jurisdictions, causing dramatic friction decline from mass mobilizations to normal interest group politics. LGBT rights: marriage equality reaching majority support (α(d) ≈ 0.6) preceded rapid legal victories and friction collapse. Labor rights: 1945-1975 peak union density (35%+ in US, 50%+ in Europe) maintained α(d_workplace) ≈ 0.6-0.7, correlating with industrial peace despite earlier decades of violent strikes. Platform governance currently operates at α(d) ≈ 0.1-0.2, well below τ, explaining persistent user dissatisfaction and regulatory friction. Climate governance at α(d) ≈ 0.3 (current generation only) or 0.05 (including future generations) similarly falls far below threshold, explaining policy failures despite scientific consensus. The consistency of τ ≈ 0.6 across such varied contexts suggests this represents a genuine legitimacy threshold rather than arbitrary convention—systems requiring supermajority consent (60%+) for major decisions implicitly recognize this threshold.

Third, consent alignment evolution proves non-monotonic; α(d) can decline as well as rise, regenerating friction predictably. Labor rights demonstrate this clearly: α(d_workplace) rose from near-zero (1850s) to 0.6-0.7 (1950s-1970s) before declining to current 0.3-0.4 as union density collapsed. Friction rose correspondingly through new channels (quiet quitting, Great Resignation). Voting rights exhibit regression: Shelby County v. Holder (2013) enabled α(d_voting) decline in jurisdictions implementing restrictive policies, generating renewed friction through litigation and protest. Trans rights currently experience backlash despite broader LGBT rights gains, demonstrating that α(d) in one subdomain (gay/lesbian rights) doesn't guarantee α(d) maintenance in related subdomains (trans rights). Platform governance shows that α(d) can start near zero and remain there indefinitely if market power enables elite resistance to user demands. These patterns validate the framework's dynamic modeling: α(d,t) as time-varying function captures real institutional processes better than static classifications.

Fourth, the framework successfully predicted friction channels and intensity across domains. When formal political Cᵢ equals zero (disenfranchised groups), friction channels through exit (Underground Railroad, maroon communities), violence (slave revolts, Stonewall riots), civil disobedience (suffragette window-smashing, lunch counter sit-ins), and proxy mobilization (abolitionist movements, youth climate strikes). When partial Cᵢ exists (labor unions), friction becomes negotiation-focused (strikes as bargaining tactics rather than revolutionary violence). When α(d) exceeds τ, friction channels into routine politics (lobbying, electoral campaigns, litigation). The framework also predicts friction intensity: existential stakes (abolition, AIDS crisis) generate friction willing to accept extreme personal costs including death, while moderate stakes (platform policies) generate lower-intensity friction like selective exodus and regulatory complaints. This granular predictive accuracy across diverse cases demonstrates the framework's empirical validity beyond mere conceptual elegance.

The historical validation establishes that consent-holding dynamics operate as structural regularities across governance domains. Legitimacy measured as α(d) predicts stability; misalignment generates friction proportional to stakes; threshold effects near τ ≈ 0.6 produce regime transitions; and evolution proves path-dependent but responsive to friction pressures. These findings enable applications to emerging domains—artificial intelligence governance, cryptocurrency regulation, pandemic response coordination—where consent structures remain contested and legitimacy deficits generate observable instability. The framework provides analytical tools for diagnosing institutional problems and predicting trajectories based on stakes distributions and consent allocations, moving legitimacy analysis from philosophical speculation to empirical social science.

## 8. Objections and Replies

While the framework rests on minimal axioms and derives its conclusions deductively, several substantive objections merit careful consideration. We address eight critiques that challenge either the framework's internal coherence (objections 1-3, 6), its empirical applicability (4-5, 8), or its normative implications (7). Each objection receives a response demonstrating that apparent difficulties either misunderstand the framework's structure or reveal productive extensions rather than fatal flaws.

### 8.1 Normativity Smuggled Into Descriptive Model

A critic might argue that despite claims to provide structural analysis, the framework smuggles normative commitments into ostensibly descriptive machinery. The very terminology—"legitimacy," "friction," "consent alignment"—appears evaluative rather than neutral. When we define legitimacy as α(d,t) and claim that low alignment produces friction, aren't we implicitly endorsing consent-based governance over alternative justifications? If the framework truly remained value-neutral, it would simply describe power distributions without labeling some configurations "legitimate" and others "illegitimate."

This objection misidentifies where normativity enters the framework. Theorems 1-2 make purely descriptive claims about decision structures under axioms A1-A6. T1 asserts that wherever collective decisions occur, some consent-holder mapping H_t(d) must exist—this is a logical necessity given that actions require choosers (A1-A2) in shared environments (A3). No normative claim appears here; we merely identify the structural fact that someone decides. Similarly, T2 establishes that plural preferences (A5) guarantee some stakeholders will experience outcomes diverging from their ideal points, generating friction F(d) > 0. This remains descriptive: stating that agents prefer different outcomes and cannot all be satisfied simultaneously makes no claim about which preferences deserve satisfaction.

Normativity enters explicitly and transparently at Theorem 3, where we introduce legitimacy as stakes-weighted consent alignment. Here the framework transitions from pure ontology (decision structures exist) to evaluation (some structures exhibit higher legitimacy than others). Crucially, this transition is marked and justified: we argue that legitimacy can be operationalized as α(d,t) because this metric captures the extent to which decision power aligns with consequence exposure. This operationalization enables empirical measurement while remaining agnostic about ultimate normative foundations—utilitarians, deontologists, and virtue ethicists can all utilize α(d) and F(d) metrics while disagreeing about which configurations count as truly just.

The framework's strength lies precisely in separating three analytical layers: (1) ontological claims about consent-holding's structural necessity, (2) operational definitions enabling empirical measurement, and (3) normative evaluation of particular H_t(d) allocations. Critics who conflate these layers mistake the framework's domain. We provide tools for measuring legitimacy within any governance structure, not a comprehensive moral theory declaring which structures are ultimately justified. The claim "consent-holding exists and misalignment produces friction" is value-neutral and empirically testable—societies with persistent low α(d) do exhibit observable instability, regardless of philosophical commitments about whether such instability constitutes genuine injustice.

### 8.2 Randomness and Markets Eliminate Consent-Holders

One substantive challenge concerns domains where decisions appear to emerge from impersonal mechanisms rather than identifiable consent-holders. Consider lottery-based policy selection (sortition, randomized controlled trials) or market allocation (price mechanisms coordinating decentralized choices). In these cases, no single agent or collective body "decides" in the conventional sense—outcomes result from chance or emergent aggregation of individual transactions. Doesn't this undermine Theorem 1's claim that every domain requires H_t(d)?

The objection reveals an important clarification about levels of decision-making rather than a genuine counterexample. Randomization and market mechanisms don't eliminate consent-holding; they relocate it to a meta-level. Someone must decide to permit chance to decide, and someone must establish the rules governing market operation. These meta-decisions possess their own consent-holder mappings H_t(d_meta) that remain analytically prior to the object-level mechanisms they authorize.

Consider sortition-based citizens' assemblies, which randomly select participants to deliberate on policy questions. The randomization itself requires prior collective decisions: which population to sample from, what demographic stratification to employ, which domains fall under assembly jurisdiction, and whether recommendations are advisory or binding. Each of these choices represents a domain with its own H_t(d). The legislature or constitutional convention authorizing sortition holds consent power over the meta-level decision "should this domain be decided randomly?" Similarly, when outcomes are determined by lot, the choice to randomize reflects a prior decision by those holding H_t(d_procedural) that equal treatment via chance is preferable to other allocation methods.

Market mechanisms exhibit the same structure. Prices emerge from decentralized transactions, but markets operate within institutional frameworks establishing property rights, contract enforcement, permissible transaction types, and regulatory constraints (Buchanan & Tullock, 1962). Someone decides these rules—legislatures, regulatory agencies, constitutional conventions—and their authority constitutes H_t(d_rules). Even ostensibly "free" markets represent a particular allocation of consent power: agents hold Cᵢ > 0 proportional to their wealth and property, while the state holds authority to define and enforce those property rights. The apparent absence of centralized decision-making masks a prior decision to decentralize through market mechanisms, itself requiring a consent-holder.

This clarification generalizes: delegation to algorithms, expert committees, or procedural rules always presupposes a meta-level decision to delegate. The meta-chooser remains the consent-holder for that delegation decision, validating T1 at a higher level of abstraction. Rather than undermining the framework, such cases demonstrate its recursive structure—consent-holding exists at every layer, from object-level policies to procedural rules to constitutional foundations. The regress doesn't terminate in some non-consent mechanism; it continues as nested consent relations, each layer possessing its own H_t(d^n). This is a feature, not a bug—politics is precisely this infinite nesting of decisions about decision-making authority.

### 8.3 Moral Realism Invalidates Value Relativism

A philosophically sophisticated objection challenges Axiom 7's claim that values are frame-dependent. If moral realism holds—if some normative truths are stance-independent facts about what justice requires or what human flourishing demands—then the framework's value relativism collapses. Suppose it is objectively true that all persons possess equal moral worth, or that suffering should be minimized, or that autonomy is intrinsically valuable. These truths would not be "frame-relative" but universal, discoverable through moral inquiry. Doesn't moral realism undermine the framework's foundational commitment to value pluralism, and with it Theorem 5's derivation of structural absolutism from value relativism?

The objection misunderstands the relationship between value relativism and consent-holding necessity. Even if moral realism holds—if stance-independent normative truths exist—agents remain bounded (A4), informationally limited, and plural in their perspectives (A5). The existence of objective moral facts doesn't eliminate disagreement about their content, nor does it provide a non-controversial mechanism for identifying them (Estlund, 2008). Reasonable people examining the same evidence can reach different conclusions about justice, goodness, and obligation, not because no truth exists but because cognitive limitations and evaluative complexity preclude universal convergence.

Moreover, even granting that moral realism provides true answers to normative questions, the structural necessity of consent-holding persists. Consider an analogy: objective facts about epidemiology exist (viruses spread through specific mechanisms, vaccines provide measurable protection), yet public health policy still requires decision-makers. Someone must translate scientific knowledge into policy actions, weighing competing values when trade-offs arise, and holding authority to implement collective responses. The existence of medical truths doesn't eliminate the need for H_t(d_health)—it merely informs how consent-holders should exercise their authority if they wish to track those truths.

Similarly, if moral truths exist independently of our beliefs, we still face the structural problem of collective action in shared environments (A3). Multiple agents with overlapping consequence spaces must coordinate responses to shared challenges. Some must hold authority to decide binding policies when unanimity proves unattainable. This consent-holding necessity arises from the structure of collective action, not from metaethical commitments about moral truth's nature. Realism refines evaluation (Step 3 in the framework's architecture)—it provides standards for judging whether consent-holders' decisions align with objective justice—but it doesn't invalidate ontology (Step 1). The fact that someone must decide remains invariant.

Furthermore, moral realism arguably strengthens rather than weakens the case for consent-holding frameworks. If objective moral truths exist but remain epistemically inaccessible through uncontroversial methods, inclusive decision-making becomes epistemically valuable. Landemore (2013) demonstrates that cognitive diversity improves truth-tracking under uncertainty; Anderson (2006) shows that democratic processes enable social learning through distributed knowledge. If we seek to approximate objective justice despite our epistemic limitations, stakes-weighted consent alignment α(d) provides a mechanism: those most affected by decisions possess both epistemic motivation (high stakes incentivize accurate information) and evaluative standing (they experience consequences most directly). Realism and consent-holding prove complementary rather than contradictory.

The framework's value relativism at A7 is better understood as evaluative pluralism than moral anti-realism. We remain agnostic about whether objective values exist, recognizing that even if they do, agents will reasonably disagree about their content and application. This agnosticism enables the framework to provide analytical tools useful across metaethical positions. Moral realists can employ α(d) and F(d) to measure whether institutions track objective justice; relativists can use identical tools to assess consistency between claimed values and institutional practice. The structural analysis remains valid regardless of one's metaethical commitments.

### 8.4 Vulnerable Populations Cannot Meaningfully Consent

A pressing ethical objection concerns populations lacking the cognitive or developmental capacity for meaningful consent: young children, individuals with severe cognitive disabilities, non-human animals, and those in temporary states of incapacity. If consent-holding requires that affected parties possess voice proportional to stakes, how can the framework accommodate agents who cannot articulate preferences, deliberate about consequences, or exercise decision-making authority? Doesn't this condemn vulnerable populations to permanent exclusion from H_t(d), or alternatively, doesn't it require us to abandon stakes-weighting when capacity is impaired?

This objection identifies a genuine challenge that the framework addresses through the concept of proxy consent, supplemented by relational autonomy theory (Mackenzie, 2014; Koggel, 2022). When agents lack capacity for direct consent, others hold decision-making authority on their behalf—parents for children, guardians for incapacitated adults, ethical review boards for research subjects. The framework doesn't merely accommodate this arrangement; it predicts guardianship institutions as structural necessities wherever high sᵢ(d) coincides with zero capacity. Someone must decide on behalf of those who cannot decide for themselves, and the framework provides tools for analyzing when such proxy consent aligns with or diverges from vulnerable agents' interests.

Crucially, the framework also predicts characteristic pathologies of guardianship systems. When proxies hold Cᵢ on behalf of vulnerable populations, misalignment between proxy interests and subject interests generates friction F(d). Historical examples validate this prediction systematically. Child custody law historically granted fathers absolute authority regardless of children's welfare; reform movements advocating "best interests of the child" standards represent increases in α(d) as children's stakes gained weight (albeit mediated through judicial evaluation rather than direct voice). Disability rights movements challenged medical model guardianship that concentrated Cᵢ in physicians while disabled individuals possessed highest sᵢ(d) in decisions affecting their autonomy, mobility, and care. Supported decision-making frameworks now emerging—which preserve partial agency for those with cognitive disabilities while providing assistance rather than wholesale substitution—represent institutional innovations raising α(d) by extending partial Cᵢ proportional to actual capacity.

The framework enables sophisticated analysis of when proxy consent is legitimate and when it constitutes oppression. Legitimate proxy relationships exhibit several features: (1) proxies are structurally accountable to vulnerable populations' demonstrated preferences when such preferences are discernible, (2) proxy authority is bounded rather than absolute, with oversight mechanisms preventing abuse, (3) proxies are selected based on genuine knowledge of and concern for subject welfare rather than mere legal authority, and (4) institutions enable gradual expansion of direct consent power as capacity develops (as children mature) or fluctuates (as mental health conditions stabilize or deteriorate).

Research ethics provides a model. Standard approaches grant legal guardians consent authority over cognitively impaired individuals enrolled in research trials. Relational approaches recognize that impaired persons often retain partial capacity and value particular relationships beyond legal guardianship—an older sibling, long-term caregiver, or chosen advocate may understand needs and preferences better than a distant legal guardian appointed by courts. The framework prescription: allocate partial Cᵢ based on measured capacity, expand H_t(d) to include chosen trusted relationships beyond legal authorities, and weight consent by both proxy knowledge and demonstrated subject preference when discernible. This approach raises α(d_research) for vulnerable individuals by ensuring decision power concentrates among those most aligned with subject interests.

Non-human animals present a harder case, as capacity for reasoned deliberation is absent entirely. The framework's response is structurally similar but normatively contingent on whether one assigns moral standing and thus stakes to animals. If sentient animals possess sᵢ(d) in domains affecting their welfare—factory farming, habitat destruction, medical experimentation—current H_t(d) allocations exhibit catastrophically low α(d), as those with highest stakes have zero voice. Animal welfare movements and rights advocacy represent proxy efforts to raise α(d) by extending consideration to animal interests despite their inability to directly consent. The framework doesn't settle whether such efforts are morally required; it provides tools for analyzing their effects: increased animal welfare protections do reduce certain forms of friction (public revulsion at factory farming conditions, consumer boycotts, activist sabotage) even while generating different friction from industry resistance.

The general principle: proxy consent is a second-best solution wherever capacity deficits prevent direct participation. Optimal institutions minimize the gap between proxy decisions and subject interests, provide accountability mechanisms, and enable gradual expansion of direct voice as capacity permits. The framework's contribution lies in making proxy consent failures measurable through friction metrics—when proxies systematically fail to represent subject interests, observable instability results (revolts against institutions, reform movements, defection from established guardianship relationships). This empirical falsifiability distinguishes legitimate proxy relationships from oppressive ones.

### 8.5 Anarchism Lacks Centralized Consent-Holders

Anarchist political theory presents a distinctive objection: horizontal, decentralized organization deliberately avoids concentrating decision-making authority in any identifiable H_t(d). Consensus-based communes, mutual aid networks, and federated collectives make decisions through direct participation without permanent leadership or hierarchical structures. Doesn't this undermine Theorem 1's claim that consent-holder mappings necessarily exist? If anarchist organization succeeds in eliminating concentrated Cᵢ, hasn't it falsified the framework's core structural claim?

The objection misidentifies the level at which consent-holding operates in anarchist arrangements. Decentralization doesn't eliminate H_t(d); it fragments domains D and localizes consent-holding to micro-scales. Rather than a single centralized authority holding Cᵢ ≈ 1 across many domains, anarchist organization distributes decision-making across numerous small-scale H_t(d) mappings, each governing a narrowly circumscribed domain. Someone still decides—the local affinity group, the workers' council, the neighborhood assembly—but consent power is dispersed across many loci rather than concentrated in a state apparatus.

Consider historical anarchist experiments. The Spanish CNT during the 1936-39 revolution collectivized workplaces through worker assemblies holding authority over production decisions, work assignments, and distribution. These assemblies constituted H_t(d_factory), with Cᵢ distributed equally among workers in each collective. Coordination across collectives occurred through federated councils, themselves possessing H_t(d_coordination) with delegates subject to immediate recall. At no point did decision-making authority vanish; it relocated from capitalist owners and state bureaucracies to workers' assemblies and federated structures. Theorem 1 applies: in every domain where decisions occurred (production, distribution, inter-collective coordination), some consent-holder mapping existed.

Similarly, contemporary mutual aid networks during disasters don't eliminate consent-holding; they create temporary H_t(d_relief) through voluntary coordination. When neighbors organize food distribution, someone decides allocation principles (first-come-first-served? prioritize vulnerability? equal shares?), even if that decision is made collectively through consensus. The consensus process itself represents a form of consent-holding where Cᵢ is distributed equally (or weighted by participation intensity) rather than concentrated. Consensus failures—when agreement proves unattainable—force either domain fragmentation (each participant acts independently) or exit (dissenters leave the network), both consistent with the framework's predictions about friction under misaligned consent.

The framework illuminates anarchism's core tension: the relationship between scale and consent alignment. Anarchist theory predicts that smaller domains with higher homogeneity enable higher α(d) through direct participation and consensus. The framework formalizes this intuition: as domain scope shrinks and affected populations become more homogeneous, preference plurality (A5) decreases, making unanimous or supermajority consent feasible. Conversely, large-scale coordination requires either (1) federated structures with nested consent-holding at multiple levels, or (2) acceptance of lower α(d) as heterogeneity increases.

Historical anarchist experiments frequently failed precisely where the framework predicts difficulties. Scaling from factory-level collectives to regional economic planning generated coordination challenges that horizontal structures struggled to resolve efficiently. The CNT's attempt to coordinate Catalonian industry through federated councils without hierarchical authority produced significant friction—agricultural and industrial collectives pursued conflicting priorities, resource allocation disputes lacked clear resolution mechanisms, and external military pressures overwhelmed decentralized decision-making capacity. The framework doesn't judge whether anarchist organization is normatively preferable to alternatives; it predicts empirically that decentralized structures excel in small-scale, high-homogeneity domains while facing friction costs when scaling to heterogeneous populations with diverse preferences.

Importantly, anarchism and the consent-holding framework share a core commitment: legitimacy requires that those affected by decisions participate in making them. Anarchists pursue α(d) → 1 through direct democracy and consensus; the framework provides metrics for assessing when such arrangements succeed or encounter limits. Far from conflicting with anarchist theory, the framework offers analytical tools for diagnosing why some anarchist experiments thrive (high α(d) in circumscribed domains) while others collapse (impossible to maintain α(d) at scale with heterogeneous preferences). The structural analysis remains valid whether consent is concentrated or distributed; what changes is the Cᵢ distribution across agents, not the existence of consent-holding relationships.

### 8.6 Infinite Regress of Consent Undermines Foundations

A philosophically sophisticated objection identifies an apparent infinite regress in the framework's logic. If every decision requires a consent-holder mapping H_t(d), who consents to that mapping? And who consents to the rules determining who consents to the consent rules? Doesn't this generate an infinite chain of meta-level decisions, each requiring its own consent-holder, with no foundation ever reached? Without some non-consent ground—tradition, natural law, divine authorization, or brute power—the framework seems to float free of any stable basis, making legitimacy claims circular and arbitrary.

The response requires embracing rather than evading the regress: it is virtuous, not vicious. The framework's position is that consent-holding relations are indeed recursive all the way down, terminating pragmatically through founding acts, ongoing practice, or revolutionary rupture, but continuing logically without ultimate foundation. This isn't a defect of the theory; it is the theory. Politics is precisely this infinite nesting of decisions about decision-making authority, and demanding non-consent foundations commits a category error analogous to asking "what causes causation?" or "what explains explanation itself?"

Formally, the structure is straightforward. Let d⁰ represent object-level domains (taxation, criminal justice, environmental regulation). Each possesses a consent-holder mapping H_t(d⁰). But the rules establishing those mappings—constitutional provisions, electoral laws, judicial review procedures—themselves constitute meta-level domains d¹, each with H_t(d¹). Similarly, amendment procedures for constitutional rules form d², with H_t(d²), and so forth. The chain continues: at every level n, some consent-holder mapping H_t(d^n) exists for decisions at that level. There is no n where the regress terminates in a non-consent foundation because Theorem 1 applies universally: wherever decisions occur (A1-A2) in shared environments (A3), someone must hold authority to decide.

Pragmatic termination occurs through several mechanisms. Founding acts establish initial consent structures without prior authorization—the American Constitutional Convention of 1787 or the French National Assembly of 1789 simply declared themselves authorized to redesign governance frameworks. These are not logically grounded acts but performative assertions of authority, subsequently legitimated (or delegitimated) through acceptance or resistance. Ongoing practice provides another termination: long-established constitutional rules gain de facto legitimacy through sustained operation, even absent explicit consent from those governed. Revolutionary rupture creates new beginnings when existing consent structures collapse under excessive friction, replaced by new mappings whose ultimate authority is simply the brute fact of their acceptance.

None of these pragmatic terminations provide logical foundations in the traditional philosophical sense—they don't ground consent-holding in something outside consent itself. But this is precisely the framework's insight: there is no outside. When values are relative (A7), only structure is absolute (T5), and that structure is consent-holding relations themselves. Demanding a non-consent foundation is demanding a square circle—if some other principle (divine command, natural law, utilitarian optimization) ultimately grounds political authority, then consent-holding isn't structurally necessary after all, contradicting T1.

The virtue of accepting infinite regress is that it accurately describes political reality. Constitutional democracies regularly face questions about who has authority to change constitutional rules—popular referendum? legislative supermajorities? judicial interpretation? constitutional conventions? Each answer is itself a constitutional choice with its own contested authorization. The United States experienced this during the Civil War: did states have authority to secede? Who held consent power to answer that meta-constitutional question? The war's outcome didn't provide a logical answer but a practical one—the Union's victory established that states lacked that authority, a settlement enforced through violence and subsequently legitimated through constitutional amendments.

This recursive structure also illuminates revolutionary moments. When existing consent structures lose legitimacy (α(d) collapses, F(d) spikes), new movements assert authority to redesign H_t(d) mappings at every level. The French Revolution didn't seek authorization from the Estates-General or the monarchy to abolish those institutions; it simply declared new consent structures and fought to establish them. Revolutionary legitimacy comes not from proper procedural authorization but from subsequent acceptance (high α(d) in the new order, low F(d)) or coercive maintenance (state power enforcing compliance despite low α(d)).

Far from being a weakness, the infinite regress clarifies what political legitimacy claims actually do. They don't point to ultimate non-political foundations (God, Nature, Reason) but assert positions within nested consent structures: "We hold these truths to be self-evident" is a performative declaration establishing a new d^n layer, not a discovery of pre-political facts. The framework makes this transparent: legitimacy is consent aligned with consequence at every level of the nesting, measurable through α(d^n) and F(d^n) at each layer. The chain doesn't terminate in a foundational certainty; it continues as long as politics exists.

### 8.7 Utilitarian Calculus Can Override Consent

A substantive normative objection comes from consequentialist moral theory. If aggregate welfare can be maximized by concentrating decision-making authority in competent hands regardless of stakeholder consent, doesn't utilitarian logic trump consent-holding considerations? Suppose a technocratic elite reliably produces better outcomes—higher GDP, longer lifespans, reduced suffering—than democratic deliberation. Shouldn't we sacrifice consent alignment α(d) for superior performance P(d)? The framework's emphasis on consent seems to privilege a particular moral value (autonomy, participation) over what many ethicists consider genuinely fundamental: human welfare.

The response clarifies that utilitarian optimization and consent-holding analysis operate at different levels of the framework's architecture. Theorem 4 explicitly incorporates this tension through the competence-consent trade-off: L(d,t) = w₁·α(d,t) + w₂·P(d,t). Utilitarian reasoning primarily informs evaluation (Step 3 in the framework's structure)—judging which consent-holder allocations maximize welfare—while consent-holding analysis describes ontology (Step 1-2)—identifying who actually holds decision-making authority and measuring the consequences. The framework explains the conflict between consent and welfare maximization without prejudging its resolution.

This separation has several implications. First, utilitarian critics can fully adopt the framework's analytical tools while rejecting consent alignment as the primary normative criterion. They can measure α(d) and F(d) instrumentally: does high consent alignment happen to produce better welfare outcomes through mechanisms like information aggregation (Hong & Page, 2004), error correction (Anderson, 2006), or political stability (reducing violence and instability)? If so, utilitarians have welfare-based reasons to favor high α(d); if not, they may rationally accept low consent alignment when competent technocracy produces superior welfare.

Second, the framework generates empirical predictions about welfare-consent trade-offs that utilitarian reasoning must address. High-performing technocracies with low α(d) face several welfare costs that pure performance metrics often ignore: (1) friction F(d) itself imposes welfare losses through instability, violence, and resources devoted to suppressing resistance; (2) excluding affected populations from H_t(d) loses distributed knowledge about local conditions and heterogeneous impacts (Ostrom, 1990); (3) persistent low α(d) generates resentment, eroding social trust and cooperation that themselves enable welfare production (Gerver et al., 2024 demonstrate that non-consensual nudges reduce happiness despite behavioral effectiveness). Once these welfare costs are properly accounted for, the utilitarian case for abandoning consent becomes far weaker.

Third, the framework illuminates historical examples where supposed welfare-maximizing rationales for low α(d) proved empirically false. Colonial powers frequently justified imperial rule through competence claims—"we're developing you, building infrastructure, providing order." Yet post-colonial research shows that many extractive institutions actively harmed subject populations (Acemoglu & Robinson, 2012), while the friction F(d) generated by exclusion imposed massive welfare costs through violence, instability, and foregone cooperation. Similarly, mid-20th century "high modernist" development projects that concentrated authority in technocratic planners often failed catastrophically by ignoring local knowledge (Scott, 1998), producing welfare outcomes worse than participatory alternatives would have delivered. The framework predicts this pattern: low α(d) systematically loses information held by high-sᵢ agents, degrading performance P(d) through epistemic failures.

Fourth, even granting that some benevolent dictatorship could maximize welfare in the short run, dynamic considerations complicate the utilitarian calculation. Competent autocrats eventually die or are replaced; institutions concentrating Cᵢ ≈ 1 in single agents or small elites lack mechanisms for ensuring successor competence. Democratic institutions with higher α(d) exhibit self-correcting features—bad leaders can be removed, policies generating high F(d) face electoral pressure to change—that technocracies lack. The expected welfare across time may favor higher α(d) even if some snapshots show technocratic advantage.

Finally, the framework enables a synthesis: domain-specific optimization of the w₁ and w₂ weights in the legitimacy function L = w₁·α + w₂·P. Utilitarians can consistently argue that high-stakes technical domains (pandemic response, nuclear safety, monetary policy) should set w₂ >> w₁, concentrating decision-making authority among competent experts. But even utilitarians should recognize that value-laden domains where preferences vary reasonably (constitutional rights, community norms, distributive justice) rationally set w₁ >> w₂, because welfare in these domains depends heavily on whether outcomes align with diverse value commitments. The framework doesn't force a universal answer; it provides tools for analyzing trade-offs systematically.

The objection ultimately misunderstands the framework's scope. We don't claim consent alignment should lexically dominate welfare considerations; we provide analytical infrastructure for measuring both and assessing their relationship empirically. Utilitarians, deontologists, and virtue ethicists can all employ α(d) and F(d) metrics while disagreeing about normative weights. The framework's value lies in making these disagreements precise and testable rather than resolving them philosophically.

### 8.8 Stakes-Weighting Enables Plutocracy

A final substantive objection challenges the framework's vulnerability to strategic manipulation. If consent power Cᵢ is weighted by stakes sᵢ(d), won't wealthy elites claim extreme stakes to justify outsized political influence? A billionaire might argue that tax policy affects their wealth substantially, granting them disproportionate voice despite representing a tiny fraction of the population. Haven't we merely formalized plutocracy—rule by those with the resources to claim high stakes, perpetuating rather than challenging economic inequality's translation into political inequality?

This objection requires a multi-layered response addressing measurement, institutional design, rights constraints, and empirical falsification. First and most fundamentally, stakes should be measured through revealed preference and objective impact metrics rather than self-reported claims. The distinction between absolute and proportional exposure proves decisive: a 10% tax on a billionaire's $1 billion wealth represents $100 million in absolute terms but leaves $900 million remaining, barely affecting capabilities. A 10% tax on a working family's $30,000 income represents $3,000 in absolute terms but may force housing insecurity or foregone healthcare, severely impacting capabilities. Proportional stakes—measured as impact on welfare, capabilities, or functioning rather than nominal dollars—systematically favor vulnerable populations over wealthy elites.

This capabilities-based approach draws on Sen's (2017) framework: stakes reflect not money changing hands but effects on freedoms to achieve valued functionings. In healthcare domains, stakes should be measured through health outcomes affected, not insurance premiums paid. In environmental policy, through exposure to pollution and climate impacts, not property values. In education policy, through learning opportunities and life prospects, not tuition paid. These objective metrics resist strategic inflation—a billionaire cannot falsely claim housing insecurity from tax policy, as revealed consumption patterns and asset portfolios directly contradict such claims.

Second, even when using material exposure metrics, progressive stake weighting through concave transformations prevents plutocratic capture. Instead of linear sᵢ(d), apply logarithmic (s'ᵢ(d) = log(1 + sᵢ(d))), square root (s'ᵢ(d) = √sᵢ(d)), or capped (s'ᵢ(d) = min(sᵢ(d), threshold)) transformations. These functions preserve the principle that higher stakes warrant greater voice while preventing single "whales" from dominating. Logarithmic weighting, for instance, compresses the ratio between billionaire and working-family stakes from 3,333:1 (linear) to approximately 1.8:1, achieving substantial balance while maintaining stakes-proportionality. The concavity degree can be tuned based on domain norms and empirical testing of friction outcomes.

Third, the framework operates within rights constraints established at meta-levels. Constitutional limits on individual consent power—maximum Cᵢ thresholds, supermajority requirements for fundamental domains, guaranteed minimum voice for vulnerable groups—restrict how stakes translate into authority. The framework describes consent-holding structures and measures their legitimacy; it doesn't claim that institutional design should mechanically translate measured stakes into unconstrained Cᵢ. Just as constitutional rights limit majority rule (free speech cannot be eliminated even with 99% support), stake-weighting operates within boundaries protecting against domination. These constraints themselves represent meta-level consent-holding decisions about H_t(d_constitutional), subject to their own α(d) and F(d) measurement.

Fourth, friction provides empirical falsification of claimed high legitimacy. If plutocrats manipulate stakes measurement to inflate their Cᵢ while excluding genuine high-stakes populations, the resulting low true α(d) manifests as observable friction F(d). Protests, instability, resistance, and defection reveal misallocated consent power. Corporate governance exemplifies this mechanism: shareholders claim high stakes (capital at risk) to justify exclusive board control, while workers claim high stakes (livelihoods at risk) but hold negligible Cᵢ. If friction F(d_workplace) remains high despite shareholder-claimed legitimacy—frequent strikes, high turnover, "quiet quitting," unionization drives—revealed preference demonstrates that workers' stakes were systematically underweighted. The system self-corrects through observable political dynamics.

Historical evidence supports welfare-based rather than material-based stake weighting. Tax systems weighted by material stakes—poll taxes, property requirements for suffrage—generated sustained friction until franchise expansion to welfare-based participation (universal suffrage). The pattern repeats across domains: wherever elites claimed high stakes based on wealth while excluding populations with high welfare stakes, friction accumulated until institutional change (incorporation through rising α(d)) or violent rupture (revolution when elites resisted indefinitely).

Different domains may legitimately weight different stake conceptions. Shareholder voting in purely economic decisions (dividend distribution) might reasonably use material stakes—those who contributed capital bear financial consequences. But policy affecting human welfare (healthcare, education, climate, labor regulation) should weight capability stakes—those whose health, learning, safety, and livelihoods are at risk. The framework predicts that material weighting in welfare domains produces low α(d), high F(d), and instability, while welfare weighting produces higher α(d), lower F(d), and stability. This prediction is empirically testable through comparative analysis of different weighting schemes.

Contemporary applications illuminate these principles. Platform governance currently concentrates Cᵢ in executives and algorithms while billions of users possess high sᵢ(d) in content moderation, data usage, and speech norms. User welfare stakes (privacy, autonomy, social connection) vastly exceed platform financial stakes when measured proportionally. The framework predicts rising F(d) until α(d) increases through user councils, transparency requirements, or regulatory intervention—precisely the pattern we observe with GDPR, Digital Services Act, and user migration to alternatives when platforms impose unilateral changes.

The plutocracy objection ultimately assumes the framework advocates mechanical translation of nominal stakes into consent power. This misreads the framework's structure. We provide analytical tools for diagnosing legitimacy given any particular stakes measurement and Cᵢ allocation. Different measurement approaches (absolute vs. proportional exposure, material vs. capability stakes, linear vs. concave weighting) produce different α(d) values and predicted F(d) levels. These can be compared empirically: which approaches minimize friction while maintaining performance? The framework enables rigorous analysis of these trade-offs rather than prescribing a single answer. Stakes-weighting doesn't inherently favor the wealthy—it depends entirely on how stakes are measured and how institutional constraints are designed. The framework makes these design choices transparent and their consequences testable.

---

These responses demonstrate that the framework's minimal axiomatic foundations prove robust under scrutiny. Objections either misunderstand the framework's limited scope (distinguishing ontology from evaluation, structure from prescription) or reveal productive extensions (proxy consent mechanisms, stakes measurement methods, meta-level iteration, rights constraints) rather than contradictions. The framework's strength lies precisely in its minimal commitments: by describing consent-holding's structural necessity without prescribing optimal allocations, it provides analytical tools useful across diverse normative traditions and empirical contexts. Legitimacy becomes measurable, friction becomes predictable, and institutional design becomes subject to systematic comparative analysis rather than philosophical assertion alone.
## 4. Social Contract Theories as Distribution Mechanisms

The consent-holding framework establishes a four-layer analytical hierarchy distinguishing structural necessities from institutional choices from normative evaluation from metaethical foundations. This section demonstrates how canonical social contract theories—from Hobbes's absolutist Leviathan to contemporary algorithmic governance—represent distinct answers to the distribution question: given that consent-holding must exist (Theorem 1), how should H_t(d) be allocated across agents and domains? By recasting these theories as special cases within our general framework, we reveal both their shared structural assumptions and their divergent normative commitments, while showing how each generates testable predictions about consent alignment α(d) and observable friction F(d).

### 4.1 The Four-Layer Analytical Architecture

The framework's conceptual structure proceeds from ontological necessity through institutional design to normative evaluation to epistemic reflection. Each layer addresses distinct questions while building on foundations established below.

**Layer 1: Consent-Holding as Ontological Structure.** Theorems 1 and 2 establish consent-holding's unavoidability. Wherever collective decisions produce shared consequences (Axiom A3) among agents with plural preferences (Axiom A5), someone must hold authority to decide (Theorem 1), and this allocation will generate friction among those whose preferences differ from realized outcomes (Theorem 2). This layer is not normative prescription but logical deduction from minimal axioms about collective action. Denying consent-holding's existence requires denying that decisions occur—exiting the domain of political analysis entirely. The question is never whether consent-holding structures exist, but rather how they are constituted and what consequences follow.

**Layer 2: Distribution Mechanisms as Institutional Choice.** Given consent-holding's necessity, institutions must specify H_t(d)—mapping domains to decision-holders. Social contract theories occupy this layer, proposing principles for allocating consent authority. Hobbes concentrates H_t(d) in sovereign monopoly; Locke conditions delegation on rights protection; Rousseau seeks alignment with general will; technocracy assigns H_t(d) to epistemic elites; anarchism fragments domains D to localize control; algorithmic systems embed H_t(d) in code. These represent competing institutional designs, each generating different consent alignment α(d) and friction F(d) profiles. The framework provides tools for measuring these consequences empirically rather than adjudicating which distribution is universally optimal.

**Layer 3: Moral Frameworks as Evaluation Criteria.** Once H_t(d) is specified, moral theories evaluate how consent-holders should exercise authority. Utilitarianism prescribes maximizing aggregate welfare Σᵢ Uᵢ given the consent structure; deontology constrains permissible actions x_d through rights and duties irrespective of consequences; virtue ethics evaluates consent-holders' character and motivations. Critically, no moral framework eliminates consent-holding itself—they operate within whatever H_t(d) obtains, judging its uses differently. A utilitarian monarchy maximizes welfare subject to monarchical H_t(d); a deontological democracy respects rights within democratic H_t(d). The distinction between Layers 2 and 3 clarifies debates: arguments about optimal consent distribution (Layer 2) differ categorically from arguments about how distributed consent should be exercised (Layer 3).

**Layer 4: Metaethics as Epistemic Reflection.** This layer interrogates the truth conditions and justification structures of moral claims themselves. Are values objective features of reality (moral realism) or constructed through social practice (anti-realism)? Are moral disagreements resolvable through rational argument or irreducibly plural? Theorem 5 demonstrates that embracing value relativism at this layer—accepting frame-dependence of content-level judgments—actually strengthens rather than undermines consent-holding analysis. If values are relative, the structure enabling value-making becomes the only invariant foundation for political theory. Consent-holding operates regardless of metaethical commitments, providing analytical infrastructure compatible with realist and anti-realist positions alike.

This four-layer architecture clarifies persistent confusions in political philosophy. Critiques of "social contract theory" often conflate Layer 2 (institutional design) with Layer 3 (moral evaluation) or Layer 4 (metaethical foundations). The framework separates these analytically: one can reject Hobbes's proposed distribution (Layer 2) while accepting his insight that consent-holding is structurally necessary (Layer 1); one can embrace democratic consent distribution (Layer 2) while disagreeing about whether democracy should maximize welfare or respect rights (Layer 3); one can accept the entire framework (Layers 1-3) while remaining agnostic about moral truth conditions (Layer 4). We now demonstrate how canonical theories map onto this structure.

### 4.2 Hobbesian Monopoly: Concentrated Consent-Holding

Hobbes's *Leviathan* (1651) represents the paradigmatic case of consent monopoly. The Hobbesian social contract concentrates H_t(d) across virtually all domains d in a single sovereign authority—monarch, assembly, or dictator—with subjects ceding decision power in exchange for security from the state of nature's violence and instability. In our notation, Hobbesian sovereignty implies Cᵢ ≈ 1 for the sovereign across all d, with Cⱼ ≈ 0 for all subjects j ≠ i. The resulting consent alignment α(d) depends entirely on whether sovereign preferences happen to align with subjects' stakes-weighted interests.

The framework reveals why Hobbesian institutions generate persistent friction despite providing order. When sovereign interests align with subjects'—when the ruler benefits from subjects' prosperity or genuinely internalizes their welfare—stakes-weighted alignment can be moderate: α(d) rises not through distributed consent but through preference alignment. Historical examples include benevolent autocracies where rulers' dynastic interests required subject prosperity, or enlightened despots whose philosophical commitments generated welfare-maximizing policies. However, such alignment is fragile and non-structural. The moment sovereign and subject interests diverge—when extractive taxation benefits the ruler at subjects' expense, when foreign adventures serve sovereign glory while imposing conscription costs, when succession conflicts prioritize dynastic claims over stability—consent alignment collapses to α(d) ≈ 0 despite subjects bearing maximal stakes.

Hobbes's innovation lies not in denying this friction but in arguing that alternative arrangements generate worse outcomes. The state of nature—his baseline for institutional comparison—exhibits zero coordination, producing maximal friction as every agent pursues incompatible interests through violence. Even concentrated consent-holding with low α(d) reduces friction relative to anarchic baseline if sovereign provision of order outweighs sovereign predation. This maps directly onto Theorem 4's competence-consent trade-off: Hobbesian subjects accept low α in exchange for high P (performance through security provision), weighting w₂ >> w₁ given existential threats. Legitimacy L = w₁·α + w₂·P can remain positive even when α → 0 if performance P is sufficiently high and agents weight competence heavily.

Yet the framework also formalizes why Hobbesian monopoly faces endemic instability. When subjects' stakes sᵢ(d) in domains like taxation, conscription, or religious practice become sufficiently high, and sovereign decisions impose costs vastly exceeding benefits, tolerance thresholds τᵢ are exceeded and friction manifests observably: tax resistance, conscription evasion, religious dissent, ultimately rebellion. Hobbes anticipates this through his account of sovereign duties—maintaining order, providing security, avoiding tyrannical caprice—which function as friction-management strategies. A sovereign who violates these duties raises F(d) until coordination around regime change becomes possible. This explains the empirical pattern: absolutist regimes sustaining moderate α(d) through benevolent policy survive for generations, while extractive tyrannies generate rising F(d) culminating in revolution or collapse.

Hobbes's framework also demonstrates consent-holding's recursive structure (Corollary 1.2). The sovereign's authority rests on the original covenant through which subjects collectively authorized monopoly. This authorization represents meta-level consent: H_t(d_constitutional) initially resided in the people who established H_t(d_policy) in the sovereign. Hobbes insists this meta-consent is irrevocable—once granted, subjects cannot reclaim it—producing a curious asymmetry. Initial consent alignment α(d_founding) ≈ 1 if authorization was genuinely unanimous, but subsequent α(d_policy) depends entirely on sovereign-subject preference alignment. This structure explains both Hobbesian legitimacy (grounded in original authorization) and instability (when original consent no longer reflects ongoing stakes).

### 4.3 Lockean Conditional Delegation: Consent as Ongoing Authorization

Locke's *Second Treatise* (1689) modifies Hobbesian monopoly through conditional delegation and revocability. Where Hobbes treats sovereign authority as absolute following initial authorization, Locke maintains that consent-holding remains conditional on respect for natural rights—life, liberty, and property. Government holds H_t(d_policy) not through permanent transfer but through ongoing delegation contingent on performance. When governments violate rights systematically, the people retain H_t(d_constitutional)—authority to withdraw consent and reconstitute government. This represents a fundamentally different consent topology: nested consent-holding with meta-level authority permanently retained by the governed.

The Lockean structure generates higher baseline consent alignment α(d) than Hobbesian monopoly through institutional mechanisms translating rights into effective constraints. Representative assemblies, separation of powers, and constitutional limitations distribute C across institutions, raising α by preventing unilateral sovereign action. Even when individual subjects possess negligible Cᵢ,d in routine policy decisions, the revocation threat maintains alignment: governments anticipating withdrawal of consent moderate extractive behavior. This threat credibility depends on coordination capacity—the people's ability to collectively exercise H_t(d_constitutional) through revolution or constitutional convention. Where coordination costs are prohibitive, Lockean conditional delegation collapses into Hobbesian monopoly de facto despite de jure revocability.

Our framework formalizes this dynamic through tolerance-weighted friction F_τ(d). Lockean governments maintaining α(d) within tolerance thresholds avoid mobilization despite imperfect alignment. Rights function as bright-line standards: violations exceeding τᵢ trigger coordinated resistance because agents can verify breaches easily. When government respects property rights, individual deviations from ideal policies remain tolerable; when confiscation begins, tolerance thresholds collapse simultaneously across affected populations, enabling coordination. This explains why rights-based constitutionalism stabilizes consent structures—it provides focal points for identifying legitimacy breaches, reducing coordination costs for collective response.

Empirically, Lockean institutions predict intermediate friction F(d): lower than Hobbesian tyranny due to rights constraints maintaining moderate α(d), but higher than fully democratic systems that maximize α through universal suffrage. Historical validation supports this prediction. English constitutional development following 1689's Glorious Revolution exhibited moderate stability punctuated by friction around franchise expansion—chartism, suffragette movements, labor organizing—as groups with high sᵢ(d) but excluded from C demanded inclusion. Lockean conditional delegation solved the consent problem for propertied classes represented in Parliament while generating sustained friction among disenfranchised populations. Alignment improved gradually through franchise expansion, each extension raising α(d) and reducing friction among newly incorporated groups.

The Lockean framework also illuminates the competence-consent relationship differently than Hobbes. Where Hobbesian legitimacy trades consent for competence (low α justified by high P), Locke argues that consent constraints improve competence by preventing catastrophic errors. Rights protect against the worst governance failures—arbitrary confiscation destroying property security, religious persecution fragmenting society, unlimited power corrupting decision-making. By maintaining moderate α through constitutional constraints, Lockean systems avoid the extremes of both tyrannical caprice (Hobbesian risk) and incompetent direct democracy (populist risk). The framework captures this as optimization along the legitimacy frontier L = w₁·α + w₂·P, with Lockean institutions targeting moderate weights w₁ ≈ w₂ rather than Hobbes's w₂ >> w₁ or direct democracy's w₁ >> w₂.

### 4.4 Rousseauian General Will: Consent Alignment as Collective Self-Rule

Rousseau's *Social Contract* (1762) proposes a radically different distribution mechanism: H_t(d) should reside in the general will—the collective determination of common interest transcending individual preferences. Where Hobbes and Locke model consent as delegation from individuals to government, Rousseau envisions consent-holding as collective self-governance, with citizens simultaneously sovereign and subject. The general will represents not aggregated individual preferences (what Rousseau dismisses as the "will of all") but rather the shared interest discovered through collective deliberation oriented toward common good.

Translating this vision into our framework reveals both profound insights and operational challenges. Ideal Rousseauian consent alignment approaches α(d) → 1 by construction: if H_t(d) genuinely resides in the collective expressing general will, and the general will by definition reflects common interest, then consent power distributes proportionally to stakes. Every citizen participates equally in general will formation (equal Cᵢ,d), but outcomes weight stakes implicitly because deliberation oriented toward common good prioritizes more consequential interests. When successful, this process maximizes consent alignment without explicit stakes-weighting—the general will converges on stakes-weighted optimal policy through collective reason rather than mechanical aggregation.

However, the framework also exposes Rousseauian democracy's vulnerability to implementation failures. The general will exists as normative ideal, not empirical aggregate—it's what citizens *should* collectively desire if properly deliberating toward common good, not necessarily what they *do* desire. This creates severe measurement problems: how do we verify that expressed collective preferences constitute general will rather than mere majority tyranny or faction dominance? Rousseau's answer—that true general will commands near-unanimity, with dissent indicating faction rather than legitimate disagreement—sets impossibly high standards. Our framework offers diagnostic tools: if purported general will generates high friction F(d) among stakeholders with large sᵢ(d), this suggests alignment failure. Genuine general will should minimize friction by incorporating all consequential interests; persistent resistance signals that outcomes reflect majority preference, not collective reason.

This interpretation connects Rousseauian theory to contemporary deliberative democracy (Section 2.6). Fishkin's deliberative polling and citizens' assemblies operationalize general will formation through structured deliberation: random selection approximates equal Cᵢ, information provision enables informed judgment, facilitated dialogue reveals common ground, and supermajority requirements ensure that consensus approximates general will rather than bare majority. Empirical results validate the approach: deliberative processes regularly generate 70-90% agreement on policy recommendations after structured engagement, suggesting that collective deliberation can discover shared interests transcending initial preferences. The framework measures this as rising α(d) through the deliberation process—as participants incorporate others' stakes and converge on mutually acceptable solutions, consent alignment increases.

Yet challenges remain. Rousseau's small-republic assumption—that general will requires direct citizen participation in manageable polities—reflects recognition that scale undermines deliberative capacity. As N grows large, coordination costs explode, preference heterogeneity increases, and deliberation becomes impractical. The framework formalizes this through Axiom A4 (finitude): agents cannot simultaneously deliberate on all domains d at all times t. This necessitates representation, delegation, or domain fragmentation—moves that Rousseau viewed as corrupting general will formation. Contemporary deliberative innovations address this through nested consent-holding: local assemblies deliberate on localized domains (high α through manageable scale), regional bodies coordinate across localities (maintaining moderate α through representation), national institutions synthesize (accepting lower α for coordination benefits). Each layer operates at appropriate scale for its domains, maintaining α(d) above tolerance thresholds while avoiding scale-driven coordination failures.

The Rousseauian framework also demonstrates the competence-consent synthesis differently than Lockean constitutionalism. Where Locke protects competence through rights constraints on consent, Rousseau argues that proper consent-holding *produces* competence—the general will tends toward correct policy because collective deliberation aggregates distributed knowledge, corrects individual errors, and discovers common interests serving long-run welfare. This anticipates epistemic democracy's central claim (Section 2.3): inclusive consent mechanisms improve decision quality through cognitive diversity, not despite popular participation. Our framework operationalizes this by predicting that rising α(d) through genuine deliberation (not mere voting aggregation) should correlate with improving performance P(d), raising both terms in L = w₁·α + w₂·P simultaneously rather than trading them off.

### 4.5 Technocracy: Expertise-Based Consent Concentration

Technocratic governance represents a fundamentally different distribution principle: H_t(d) should reside with those possessing domain-relevant expertise rather than affected populations or their representatives. In our notation, technocracy concentrates consent in expert set E with Cᵢ,d = 1/|E| for i ∈ E and Cⱼ,d ≈ 0 for j ∉ E, regardless of stakes distributions sᵢ(d). This generates low baseline consent alignment α(d) when expertise and stakes correlate weakly—as they typically do, since domain knowledge concentrates through specialized training while stakes distribute according to exposure, identity, or vulnerability.

The technocratic case for low-α institutions rests entirely on Theorem 4's performance term: accepting α → 0 can be legitimate if performance P is sufficiently high and agents weight competence heavily (w₂ >> w₁). Proponents argue that complex technical domains—monetary policy, pandemic response, nuclear safety, climate engineering—require specialized knowledge that popular majorities lack. Delegating H_t(d_technical) to experts maximizes P through informed decision-making, outweighing legitimacy costs from excluding affected populations. Central bank independence exemplifies this logic: monetary policy concentrated in technically trained economists rather than elected politicians, justified by performance gains from credible inflation commitments and apolitical analysis.

However, the framework reveals systematic problems with pure technocracy that empirical evidence validates. First, expertise and stakes systematically diverge in ways that generate friction. Climate policy experts may possess superior knowledge about emissions trajectories and mitigation strategies (high technical competence), but frontline communities facing displacement, agricultural workers experiencing harvest failures, and island nations confronting existential sea-level rise possess maximal stakes sᵢ(d_climate) ≈ 1. Excluding these populations from H_t(d) produces low α regardless of expert competence, generating friction F(d) through climate justice movements, indigenous resistance, and Global South demands for representation. The framework predicts this pattern: whenever expertise and stakes decorrelate, technocracy faces legitimacy deficits uncompensable through performance alone if w₁ > 0.

Second, technocratic concentration of consent eliminates epistemic benefits from cognitive diversity documented in Section 2.3. Experts share training, professional norms, and analytical frameworks—homogeneity that generates blind spots. Landemore's (2013) diversity trumps ability theorem demonstrates formally that diverse problem-solvers outperform homogeneous experts because diversity prevents local optima traps. Excluding high-stakes populations from H_t(d) loses their local knowledge, experiential expertise, and distinct heuristics. This suggests that pure technocracy systematically underperforms hybrid arrangements incorporating both technical expertise *and* stakeholder knowledge, raising both P and α simultaneously. Worker representation on German boards combines employee tacit knowledge with managerial expertise; participatory budgeting integrates community preferences with fiscal analysis; citizens' assemblies inform laypeople before deliberation, creating expert-educated stakeholders.

Third, accountability mechanisms weaken when consent concentrates in unelected experts. Without meaningful α(d), affected populations cannot effectively sanction poor performance—they lack Cᵢ,d to remove failed technocrats or redirect policy. This removes crucial feedback loops that democratic accountability provides. Scharpf's (1999) input-output legitimacy framework clarifies the tension: technocracy maximizes output legitimacy through performance (P term) while sacrificing input legitimacy through participation (α term). But output legitimacy alone proves unstable—when expert policies fail or distribute costs unequally, excluded populations lacking input mechanisms channel dissent through external friction: protests, legal challenges, electoral backlash against institutions supporting technocratic governance.

The framework prescribes domain-specific optimization rather than universal technocracy or democracy. Certain domains rationally prioritize expertise: nuclear reactor operation, epidemic disease modeling, aerospace engineering involve technical complexity where untrained judgment adds noise rather than signal. For these domains, concentrating H_t(d) in expert set E may be optimal even accepting low α, provided performance gains are substantial (high P), competence weights dominate (w₂ >> w₁), and expert decisions face constitutional constraints protecting rights. Other domains rationally prioritize inclusion: community norms, resource allocation reflecting diverse values, constitutional fundamentals involve legitimacy concerns where performance alone cannot compensate for exclusion. Hybrid institutions partition domains or blend expertise with participation: experts provide analysis and options (high P), stakeholders choose among alternatives (high α), deliberation bridges knowledge gaps.

Empirical validation supports domain-contingent approaches. Fauver and Fuerst's (2011) study of German codetermination shows worker board representation improving long-term performance despite increasing short-term volatility—expertise alone misses sustainability considerations that worker voice provides. Fishkin's (2018) deliberative polls demonstrate that informed laypeople converge toward expert consensus after learning, suggesting education can raise both α (through inclusion) and P (through knowledge transfer) rather than trading them off. Conversely, central bank independence correlates with lower inflation and more stable growth—technical monetary policy appears to genuinely benefit from expert concentration, validating technocratic governance for specific domains.

### 4.6 Anarchism and Federalism: Fragmenting Domains to Localize Consent

Anarchist and federalist traditions propose a fundamentally different institutional strategy: rather than concentrating or distributing H_t(d) within fixed domains, fragment the domain space D itself to maximize local control and minimize coercion. In our framework, this means partitioning large domains d_national into localized subdomains {d_local,1, d_local,2, ..., d_local,n} and assigning H_t(d_local,k) to the population directly affected by decisions in that subdomain. The goal is maximizing consent alignment α(d) through matching decision authority to consequences—those bearing stakes hold consent.

This approach reduces friction through two mechanisms. First, localizing control raises α(d) mechanically: smaller populations are more homogeneous in preferences, reducing the plural preference conflicts that Theorem 2 identifies as friction's source. A neighborhood deciding park design faces less preference heterogeneity than a nation deciding constitutional values; a worker cooperative setting production schedules faces more aligned interests than a multinational corporation balancing shareholder returns against worker welfare. By fragmenting D to match natural communities of interest, anarchist institutions minimize the stakes-weighted preference divergence δ(x_d, x*ᵢ,d) that generates friction.

Second, domain fragmentation enables exit as friction-management mechanism. When consent alignment within a jurisdiction becomes intolerable (α < τᵢ for agent i), agents can exit to join communities with better preference matches. Tiebout's (1956) sorting model formalizes this for local public goods: residents "vote with their feet," selecting jurisdictions offering preferred tax-service bundles. Anarchist theory extends this logic to all domains—political organization, economic production, social norms—treating exit as fundamental right. In our framework, exit options function as revealed preference mechanisms for stakes measurement: populations fleeing jurisdictions reveal high sᵢ(d) in domains prompting exit, generating friction F(d) through departure even if voice-based resistance is suppressed.

However, the framework also reveals systematic constraints on anarchist domain fragmentation. Axiom A3 (shared reality) asserts that externalities exist—some decisions inevitably affect populations beyond local jurisdiction. Climate emissions, pandemic control, financial stability, military security generate spillovers that local decision-making cannot internalize. Fragmenting these domains risks coordination failures: each locality optimizes for local stakes, ignoring impacts on others, producing globally suboptimal outcomes. The tragedy of the commons (Hardin, 1968) exemplifies this structure: fragmenting resource governance enables local overextraction that degrades shared stocks.

Ostrom's (1990) polycentric governance synthesis addresses this challenge through nested consent-holding: local autonomy for locally-bounded decisions, coordinated governance for domains with spillovers, with consent alignment α(d) maintained at each level. Her empirical studies document successful commons management through nested enterprises—village-level monitoring embedded in regional coordination embedded in national legal frameworks. Each layer holds H_t(d_layer) for decisions at appropriate scale, maintaining α by including affected stakeholders while avoiding both centralization (which reduces α through scale) and fragmentation (which ignores externalities). The framework interprets this as optimizing across domains: high α(d_local) for localized decisions, moderate α(d_regional) for coordination, accepting somewhat lower α(d_national) only for genuinely national-scale externalities.

Federalist institutions operationalize this structure through constitutional division of powers. The U.S. Constitution's enumerated federal powers with reserved state authority attempts domain partitioning: interstate commerce, foreign policy, and national defense centralized (externality justification); education, family law, and local governance decentralized (local stakes justification). When working well, this maintains α(d) at appropriate levels across domains. Failures occur when centralization extends beyond externality-generating domains (reducing α unnecessarily) or when local control persists despite significant spillovers (enabling exploitation).

The competence-consent relationship operates differently under anarchist structures than centralized alternatives. Where technocracy concentrates expertise at the cost of consent, anarchism distributes consent at potential cost to expertise—local populations may lack technical knowledge for complex decisions. The framework suggests hybrid approaches: subsidiarity principles centralizing only domains where expertise benefits exceed consent costs, with lower levels maintaining autonomy elsewhere. The European Union's subsidiarity doctrine attempts this balance, though implementation remains contested.

Empirically, anarchist predictions about friction reduction through domain fragmentation receive mixed support. Worker cooperatives exhibit higher job satisfaction and lower turnover than hierarchical firms—consistent with friction reduction through high workplace α(d) (Craig & Pencavel, 1992). Intentional communities and ecovillages demonstrate sustained cooperation through self-selection and local control, maintaining high α by attracting preference-compatible members (Sargisson, 2007). However, scaling challenges persist: large-scale coordination failures in anarchist experiments (Spanish Civil War collectives, early kibbutzim) validate concerns about externality management. The framework suggests these aren't anarchism failures but scale-appropriate institutional design: anarchist structures optimize small-scale α(d), requiring supplementation with coordination mechanisms for spillover domains.

### 4.7 Algorithmic Governance: Code as Consent-Holder

Algorithmic governance represents an emergent distribution mechanism where H_t(d) resides neither in human individuals nor representative institutions but in computational code making decisions based on data inputs and programmed rules. Platform content moderation algorithms determine speech permissibility (H_t(d_speech)), credit scoring algorithms control lending access (H_t(d_credit)), hiring algorithms filter employment opportunities (H_t(d_employment)), criminal justice risk assessment tools influence sentencing and parole (H_t(d_justice)). In each case, affected populations possess high stakes sᵢ(d) but negligible consent power Cᵢ,d, which concentrates in algorithm designers, platform executives, and automated decision processes.

The resulting consent alignment α(d_algorithm) approaches zero for most affected populations, generating severe legitimacy deficits according to Theorem 3. Users censored by content moderation hold maximal stakes (their speech suppressed) but cannot vote on moderation rules, appeal to accountable representatives, or meaningfully contest algorithmic decisions. Loan applicants denied credit face material consequences (sᵢ(d_credit) high) without understanding scoring criteria or challenging opaque determinations. This structure violates fundamental transparency norms: not only are affected populations excluded from H_t(d), they cannot even observe the decision rules applied to them.

Section 2.7 documents empirical consequences predicted by the framework. Low α(d_algorithm) generates rising friction F(d): user protests against platform policies (#DeleteFacebook, Twitter exodus), regulatory backlash (GDPR, Digital Services Act), litigation challenging algorithmic discrimination, and mass migration to alternatives when exit becomes feasible. The framework interprets these as friction manifestations: excluded populations with high stakes channel resistance through available mechanisms when voice is denied. GDPR's algorithmic transparency requirements and contestation rights represent regulatory attempts to raise α(d) by expanding affected populations' information access and appeal rights—increasing eff_voiceᵢ even if formal Cᵢ remains low.

However, algorithmic governance also presents opportunities for improving both consent alignment and competence simultaneously—raising both α and P—through institutional innovation. First, transparent and auditable algorithms enable democratic oversight that human decision-makers cannot provide. A public algorithm making welfare eligibility determinations can be examined, debated, and democratically amended in ways that caseworker discretion cannot. If H_t(d_algorithm_design) resides in democratically accountable bodies that design rules transparently, then affected populations gain indirect consent power: Cᵢ,d remains low for individual decisions but Cᵢ,d_meta rises for rule-setting. This nested consent structure improves α while maintaining rule consistency that human judgment may lack.

Second, participatory algorithm design (Koshimizu et al., 2020) demonstrates that including affected stakeholders in fairness criterion selection improves perceived legitimacy substantially. Rather than technical experts unilaterally optimizing for accuracy or efficiency, participatory processes elicit stakeholder values about acceptable trade-offs—false positive vs. false negative rates in criminal risk assessment, breadth vs. safety in content moderation, demographic parity vs. individual calibration in hiring. By raising α(d_design), these processes improve both input legitimacy (inclusion) and output legitimacy (better value alignment). Empirical results show participants accept algorithmic decisions more readily when they participated in design even if outcomes remain imperfect—consent alignment raises tolerance thresholds τᵢ.

Third, algorithmic governance enables stakes-weighted mechanisms at scale that would be impractical through human deliberation. An algorithm could, in principle, weight different populations' input by measured stakes sᵢ(d), approximating optimal α(d) → 1 mechanically. Platform governance might weight content creators more heavily in moderation rule-setting (higher sᵢ due to income dependence) while maintaining user voice; city planning algorithms could weight neighborhood residents more heavily than distant stakeholders in local development decisions. This requires solving stakes measurement challenges (Section 5.2), but computational capacity removes scaling barriers that constrain human deliberation.

The framework also clarifies the competence argument for algorithmic governance. Algorithms can process information volumes, detect complex patterns, and maintain consistency across cases that exceed human cognitive capacity. Credit scoring aggregates thousands of variables identifying default risk more accurately than human judgment; medical diagnostic algorithms integrate research literatures and case databases beyond individual physician knowledge; content moderation at Facebook's scale (billions of daily posts) becomes physically impossible through human review alone. These represent genuine P gains justifying partial algorithmic H_t(d) if properly governed.

Yet the framework simultaneously reveals why purely algorithmic governance—H_t(d) residing entirely in code without human oversight—remains fundamentally illegitimate when w₁ > 0. Algorithms execute programmed objectives; they cannot engage in normative reasoning about whether objectives themselves are just. A perfectly competent algorithm optimizing for profit maximization will systematically harm populations whose welfare doesn't correlate with profit. An efficient content moderation algorithm will enforce whatever speech norms are programmed without evaluating whether those norms respect expressive autonomy appropriately. This structural limitation means algorithms can assist decision-making (raising P) but cannot legitimately monopolize consent-holding (which requires α > threshold) in value-laden domains.

The framework prescribes human-in-the-loop governance: algorithms provide analysis, pattern detection, and recommendation (contributing to P), while humans with democratic accountability make final decisions or set normative criteria (maintaining α). Waldman and Johnson's (2022) finding that human oversight substantially raises algorithmic legitimacy validates this prescription. Purely automated decisions are perceived as illegitimate even when outcomes are identical; involving humans in review or exception-handling raises perceived α by providing contestation opportunities. This suggests optimal algorithmic governance combines computational efficiency with democratic oversight, raising both terms in L = w₁·α + w₂·P rather than trading them off.

### 4.8 Synthesis: Moral Theories as Evaluation Layer

Having mapped major social contract theories onto Layer 2 (distribution mechanisms), we can now clarify Layer 3's role. Moral frameworks—utilitarianism, deontology, virtue ethics, care ethics, contractarianism—do not compete with consent-holding analysis but operate within whatever H_t(d) structure obtains. They evaluate how consent-holders should exercise authority rather than prescribing distribution itself.

Utilitarian evaluation judges outcomes x_d by aggregate welfare Σᵢ Uᵢ regardless of the consent structure producing them. A utilitarian can accept Hobbesian monopoly (concentrated H_t), Lockean constitutionalism (conditional H_t), or Rousseauian democracy (collective H_t) while consistently arguing that consent-holders should maximize total utility. The distribution question—who holds H_t(d)—differs categorically from the evaluation question—how should H_t be used? Classical utilitarianism often supported benevolent monarchy on grounds that enlightened despots maximize welfare better than democratic majorities; modern utilitarians favor democracy because political competition and accountability mechanisms better incentivize welfare-maximizing policy. Both positions share utilitarian evaluation criteria while differing on empirical claims about which H_t(d) distribution optimizes Σᵢ Uᵢ.

Deontological frameworks constrain permissible uses of consent authority through rights, duties, and categorical imperatives. Kantian ethics prohibits treating persons as mere means regardless of consequences; Lockean natural rights limit legitimate government action even when welfare gains obtain; Rawlsian justice requires maximin distribution even under democratic H_t(d). These constraints apply to *any* consent-holding structure. A deontological monarch should respect rights; a deontological democracy should not violate duties through majority vote; deontological algorithms should not optimize efficiency by instrumentalizing individuals. The evaluation layer remains constant across distribution mechanisms.

Virtue ethics evaluates the character and motivations of consent-holders rather than the structures or outcomes directly. An Aristotelian assessment asks whether those exercising H_t(d) exhibit practical wisdom (phronesis), courage, temperance, and justice in their decision-making. This applies to monarchs choosing policy, democratic citizens deliberating collectively, technocratic experts analyzing trade-offs, or algorithm designers encoding values. Virtue ethics provides criteria for selecting and training consent-holders—cultivating civic virtue in democracies, wisdom in monarchs, expertise in technocrats—without prescribing which distribution mechanism is universally optimal.

The framework's crucial insight is that these evaluation frameworks are *compatible* with consent-holding analysis rather than competing alternatives. One can be a utilitarian who uses α(d) and F(d) to measure whether consent structures maximize welfare; a deontologist who employs the framework to diagnose rights violations through low α; a virtue ethicist who analyzes which H_t(d) cultivates better civic character; or a care ethicist who examines whether consent structures maintain relationships appropriately. The framework provides measurement tools; moral theories provide evaluation standards.

This clarifies persistent philosophical debates. Arguments about "democracy vs. expertise" often conflate distribution (Layer 2) with evaluation (Layer 3). Epistemic democrats aren't merely claiming democratic H_t(d) is procedurally fair; they claim it produces better outcomes—a utilitarian-adjacent evaluation. Technocratic critics aren't merely defending competence; they claim expert H_t(d) respects persons better by providing what they truly need—a care-ethical or perfectionist evaluation. By separating analytical layers, the framework shows these arguments involve *both* empirical claims about which distributions optimize evaluation criteria *and* normative commitments about which criteria matter. Progress requires specifying both dimensions clearly.

### 4.9 Metaethics: Structure as Invariant Foundation

Theorem 5 establishes that value relativism at the metaethical layer (Layer 4) strengthens rather than undermines consent-holding analysis. If we accept Axiom A7—that perception and valuation are frame-dependent with no universal content-level ordering logically forced—then the framework's structural analysis becomes the only invariant foundation for political theory.

Consider moral realism first. If moral truths exist stance-independently—if justice, rights, and goodness are objective features of reality—this doesn't eliminate consent-holding structures. Even with perfect moral knowledge, agents remain cognitively bounded (Axiom A4) and preferences remain plural (Axiom A5) because different frames emphasize different moral considerations. Implementing objective moral truths still requires decisions about priority under resource constraints, application to particular cases, and balancing competing claims. These decisions require consent-holding: H_t(d) must exist (Theorem 1) even if used to implement discovered moral truths rather than constructed preferences. Moral realism might constrain *which* H_t(d) distributions are legitimate—perhaps only those respecting objective rights—but doesn't eliminate the analytical task of measuring α(d) and F(d) within legitimate structures.

Conversely, moral anti-realism—the position that moral truths are constructed through practice rather than discovered—makes consent-holding analysis even more central. If values are frame-relative with no universal ordering, then the question "what should we do?" reduces to "what procedure for deciding what to do is legitimate?" Political philosophy becomes primarily structural: investigating consent-holding's properties, measuring alignment and friction, and designing institutions managing plural values appropriately. The framework provides precisely this analytical infrastructure.

This position differs from crude relativism that collapses into "anything goes." Theorem 5 demonstrates that relativism about content-level values (what outcomes are good?) is compatible with—indeed requires—absolutism about structural necessities (consent-holding exists wherever decisions occur). Even if we accept that different moral frames yield incompatible value orderings with no universal resolution, the *structure enabling frame-competition* remains invariant. Someone must decide which frame governs in each domain d; that decision-making authority constitutes H_t(d); and its alignment with affected parties' stakes determines legitimacy α(d) and generates friction F(d) predictably. These structural facts obtain regardless of metaethical commitments.

This move resolves long-standing tensions in political theory. Liberal neutrality doctrines attempt to avoid substantive value commitments by remaining neutral among conceptions of the good. Critics object that such neutrality is impossible or incoherent—any institutional structure privileges some values over others. The framework accepts the critique while maintaining analytical progress: we cannot avoid value-laden decisions (consent-holding is necessary, Theorem 1), but we can analyze *who makes them* (measuring H_t(d)) and *how aligned with consequences* they are (computing α(d)). This provides traction on legitimacy questions without requiring resolution of foundational value disputes.

Similarly, communitarian critiques of liberal individualism argue that values emerge from communities rather than pre-existing as individual preferences. The framework accommodates this by remaining agnostic about preference origins—whether sᵢ(d) reflects individual utility, constitutive community membership, or relational identity doesn't affect the structural analysis. What matters is measuring stakes and consent distribution, not adjudicating metaethical debates about personhood. A communitarian can employ α(d) to measure whether consent-holding aligns with community membership; a liberal individualist can use the same metric to measure preference-weighted authority. The tools work regardless of foundational commitments.

The key methodological insight is that operating system neutrality enables application diversity. The framework provides infrastructure—definitions, axioms, theorems, measurement tools—that supports diverse normative commitments rather than prescribing particular values. Just as an operating system enables multiple applications with incompatible purposes, consent-holding theory enables multiple moral frameworks with incompatible evaluations. This isn't value neutrality in the strong sense—the framework does privilege consent alignment and friction minimization—but it's neutrality about content-level values: *which* outcomes are good remains contestable while *who decides* remains structurally analyzable.

The concluding metaphor captures this architecture: **"Moral theories are apps; consent-holding is the operating system."** Utilitarianism, deontology, virtue ethics, care ethics, and contractarianism represent different applications running on consent-holding infrastructure. Each app uses the OS's features—H_t(d) distributions, α(d) measurement, F(d) tracking—to evaluate outcomes according to its distinctive criteria. The apps may conflict (utilitarian efficiency vs. deontological rights), but they share common infrastructure making their operation possible. Attempting political philosophy without consent-holding analysis is like trying to run apps without an operating system—the evaluative frameworks have no structure to operate on.

This doesn't mean the OS is value-neutral in absolute terms. Consent-holding theory privileges legitimacy (measured as α), stability (inverse of F), and domain-appropriate competence (P) as primary political goods. These aren't arbitrary choices but follow from the axioms: given plural preferences in shared reality with finite agents, minimizing stakes-weighted friction while maintaining performance becomes structurally necessary for sustainable collective action. Moral theories can critique these priorities—perhaps accepting higher F(d) is justified for deontological integrity, or lower α is acceptable for utilitarian gains—but such critiques operate *within* the framework's structure rather than replacing it.

The framework thus establishes minimal absolutism from maximal relativism: when content-level values are relative, structural necessities become absolute. This provides foundations for comparative political analysis across cultures, eras, and moral traditions without requiring resolution of foundational disputes. We can measure Hobbesian, Lockean, Rousseauian, technocratic, anarchist, and algorithmic institutions using common metrics—α(d), F(d), L = w₁·α + w₂·P—while remaining agnostic about which values should dominate evaluation. This analytical capacity, we argue, represents consent-holding theory's distinctive contribution to political philosophy.

---

**End of Section 4**
## 9. Conclusion

This paper has developed consent-holding theory as a unified analytical framework for measuring political legitimacy across heterogeneous governance domains. The framework establishes seven minimal axioms about collective decision-making—action precedence, decision requirement, shared reality, finitude, plurality, salience, and fallibility—from which five structural theorems necessarily follow. These theorems demonstrate that consent-holding is a logical necessity wherever decisions produce shared outcomes (T1), that plural preferences guarantee friction unless perfect stakes-weighted alignment obtains (T2), that legitimacy can be operationalized as consent alignment α(d,t) measuring the stakes-weighted concentration of decision power among affected parties (T3), that overall legitimacy combines consent and competence through domain-specific weighting (T4), and that structural analysis remains valid even under radical value relativism (T5). By synthesizing insights from constitutional political economy, social choice theory, stakeholder governance, common-pool resource management, deliberative democracy, algorithmic governance, epistemic democracy, relational autonomy, and legitimacy theory, the framework provides measurement tools applicable across democratic, technocratic, and algorithmic systems without prescribing which institutional arrangements are normatively superior.

### 9.1 Summary of Contributions

The consent-holding framework makes four distinct contributions to political theory and institutional analysis, each addressing longstanding gaps in existing scholarship while opening new research directions.

**Theoretical contribution**: The framework's primary theoretical achievement is establishing legitimacy measurement as a structural property of decision-making systems rather than a binary philosophical classification. Where democratic theory has argued normatively about who should hold authority and public choice has modeled strategic behavior within given institutions, consent-holding theory provides the intermediate analytical layer connecting institutional configurations to observable political dynamics. The seven axioms are intentionally minimal, requiring only that collective decisions occur in shared environments populated by agents with finite capacity and plural preferences. From these sparse commitments, the five theorems derive necessary conclusions about consent structures, friction generation, and legitimacy measurement that apply regardless of normative commitments about justice, efficiency, or democratic ideals.

The axiomatic approach enables rigorous analysis across radically different governance contexts. A state legislature deciding tax policy, a corporate board allocating capital, an algorithmic content moderation system enforcing community standards, and a common-pool resource regime managing fisheries all instantiate specific consent-holder mappings H_t(d) distributing decision authority over affected stakeholders. Traditional frameworks struggle to compare these domains analytically—democratic theory applies awkwardly to corporate governance, stakeholder theory lacks tools for analyzing state legitimacy, and algorithmic governance remains disconnected from centuries of political philosophy. By abstracting to the level of decision domains, stakes distributions, and consent allocations, the framework provides unified measurement tools while preserving domain-specific institutional variation. The consent alignment metric α(d,t) = (Σᵢ∈S_d sᵢ(d)·eff_voiceᵢ(d,t))/(Σᵢ∈S_d sᵢ(d)) applies equally to suffrage expansion (where historical increases in α tracked expanding voting rights), corporate codetermination (where worker board representation raises α_workers in workplace domains), and platform governance (where current low α predicts user friction absent stakeholder voice).

The framework's formalization of friction as F(d,t) = Σᵢ sᵢ(d)·δ(x_d(t), x*ᵢ,d) connects institutional configurations to empirically observable instability. Political theorists have long argued that excluding affected stakeholders undermines legitimacy, but existing accounts lack operational metrics for testing such claims systematically. By defining friction as stakes-weighted deviation between realized outcomes and stakeholder ideal points, the framework generates testable predictions: persistent low α(d) should correlate with high F(d) manifesting as protests, non-compliance, litigation, policy reversals, or institutional breakdown. This operationalization transforms legitimacy from a purely normative concept to an empirically measurable property enabling quantitative comparative analysis. A system with α = 0.7 exhibits higher legitimacy than α = 0.3, and we can test whether this difference predicts observable friction differences controlling for domain characteristics and external shocks.

**Methodological contribution**: The framework's second major contribution is methodological, providing researchers with precise operational definitions enabling empirical investigation of legitimacy dynamics. Political philosophy has generated centuries of debate about consent, representation, and democratic authority without developing agreed-upon metrics for measuring these concepts in practice. Public opinion surveys ask whether people "trust" institutions or feel "represented," but such subjective measures confound perceived legitimacy with performance satisfaction, personal ideology, and transient political circumstances. The consent-holding framework instead operationalizes legitimacy structurally through measurable institutional features: who holds formal decision authority (consent power Cᵢ), who is affected by decisions (stakes sᵢ(d)), and what outcomes result (policy positions x_d relative to ideal points x*ᵢ,d).

This operationalization enables multiple empirical strategies. Cross-sectional analysis can compare α(d) across countries, policy domains, and institutional configurations, testing whether variation in consent alignment predicts variation in friction metrics. Panel data designs tracking institutional reforms over time enable causal identification: when voting rights expand, labor representation increases, or algorithmic governance introduces appeals mechanisms, does α(d) rise and F(d) fall with measurable lags? Quasi-experimental designs exploit exogenous shocks to consent structures—franchise reforms, corporate governance mandates, platform policy changes—as natural experiments revealing how legitimacy shifts affect political stability. Survey instruments can measure perceived legitimacy and test whether subjective assessments correlate with objective α(d) calculations, validating that structural alignment tracks normative acceptance.

The framework's measurement strategy addresses key identification challenges in political economy research. Endogeneity concerns arise because legitimacy and stability mutually influence each other: low legitimacy may cause instability, but instability may also erode perceived legitimacy. Instrumental variables designs using institutional reforms as exogenous variation in consent structures can isolate causal effects. Selection bias concerns arise if observed consent configurations reflect equilibrium responses to underlying political conditions rather than independent institutional variation. Historical case studies examining non-monotonic evolution—where similar stakes distributions generate different consent structures at different times—help address this concern by showing that H_t(d) responds to accumulated friction rather than purely reflecting fixed preferences. Measurement error in stakes estimation can be addressed through multiple proxies (material exposure, capability impacts, revealed preferences) and sensitivity analysis showing results robust to alternative weighting schemes.

**Empirical contribution**: The framework's third contribution is empirical, demonstrating through historical validation across seven domains—suffrage expansion, abolition movements, labor rights, civil rights, LGBT inclusion, platform governance, and climate justice—that the predicted pattern appears consistently: groups with high stakes but zero consent power generate sustained friction until incorporation or suppression occurs. These case studies span two centuries, multiple continents, and radically different issue domains, yet exhibit the same underlying dynamic. Women possessed extreme stakes in suffrage (affecting political voice, legal status, economic opportunities) while holding zero formal consent power in electoral domains. This misalignment generated observable friction through suffrage movements, civil disobedience, and sustained political mobilization lasting decades. As franchise expansion gradually raised α(d_electoral) by incorporating women into H_t(d), friction collapsed—the predicted pattern precisely.

The labor rights case demonstrates domain-specificity: workers held high stakes in workplace conditions (health risks, wages, hours, dignity) but zero consent power in employment domains governed unilaterally by capital. Friction manifested through strikes, sabotage, union organizing, and occasionally violent conflict. Institutional responses varied: some jurisdictions suppressed friction through state violence maintaining low α(d_workplace), while others raised α through collective bargaining rights, codetermination, and labor standards. Where consent alignment rose sustainably, friction declined; where suppression proved temporary, friction accumulated until system breakdown. Contemporary platform governance exhibits the identical structure: users possess high stakes in content moderation, data practices, and algorithmic curation (affecting speech, privacy, psychological wellbeing) while holding near-zero consent power concentrated in executives and algorithms. The framework predicts rising friction, validated through user protests, regulatory backlash (GDPR, Digital Services Act), and platform exodus when alternatives emerge.

Historical analysis reveals a consistent legitimacy threshold τ ≈ 0.6 across contexts: systems maintaining α(d) above roughly 60 percent of maximum stakes-weighted alignment sustain stability, while those falling persistently below face mounting friction requiring either reform raising α or suppression maintaining concentrated consent through coercion. This empirical regularity, if confirmed through systematic quantitative analysis, would provide guidance for institutional design: governance structures should target α > 0.6 through stakes-weighted voice mechanisms rather than assuming universal suffrage or technocratic concentration achieves legitimacy automatically. Domain-specific variation matters—constitutional amendments may require higher thresholds (τ ≈ 0.8) while routine administration tolerates lower alignment (τ ≈ 0.4) if performance compensates—but the framework provides tools for identifying appropriate thresholds empirically rather than asserting them philosophically.

**Integrative contribution**: The framework's fourth contribution is integrative, synthesizing nine research traditions into a coherent analytical architecture. Constitutional political economy (Buchanan & Tullock) provides the meta-level constitutional choice framework, formalized through nested consent-holding H_t(d_meta) over rule-setting domains. Social choice theory (Arrow, Sen, Gibbard-Satterthwaite) establishes impossibility results and normative criteria, which the framework incorporates not by seeking perfect aggregation rules but by measuring legitimacy of any mechanism through α(d) and predicting friction through F(d). Stakeholder theory (Freeman) identifies affected parties in corporate domains, operationalized through sᵢ(d_corporate) measurement and diagnosis of legitimacy deficits when workers possess high stakes but zero board representation. Common-pool resource governance (Ostrom) demonstrates polycentric arrangements succeeding through local consent-holding, formalized as domain-specific H_t(d) at multiple scales with high α at each level.

Deliberative democracy (Habermas, Fishkin) emphasizes communicative rationality and informed deliberation, operationalized through effective voice eff_voiceᵢ accounting for capability constraints beyond formal consent power. Algorithmic governance research (Grimmelikhuijsen et al., Barocas et al.) identifies legitimacy deficits in automated decision-making, diagnosable through α(d_algorithm) calculation revealing how concentration of consent in designers produces low alignment despite high stakeholder exposure. Epistemic democracy (Estlund, Landemore, Anderson) argues that inclusive decision-making produces better outcomes, formalized through Theorem 4 showing consent and competence as complementary dimensions rather than competing values. Relational autonomy theory (Mackenzie, Koggel) highlights how oppressive structures constrain agency, operationalized through eff_voiceᵢ measurement capturing capability constraints that reduce effective decision power even when formal rights exist. Legitimacy theory (Scharpf, Schmidt) distinguishes input, throughput, and output dimensions, all measurable within the framework: α(d) captures input legitimacy through stakeholder inclusion, procedural fairness affects eff_voiceᵢ as throughput legitimacy, and performance P(d) represents output legitimacy.

Each tradition contributes domain-specific insights about consent allocation, legitimacy sources, or institutional performance, while the consent-holding framework provides the common measurement infrastructure enabling systematic comparison. This integration generates theoretical value beyond mere synthesis: by showing that stakeholder theory, deliberative democracy, and common-pool governance all emerge as special cases of the general framework applied to specific domains, we demonstrate that seemingly disparate approaches actually describe the same underlying consent-holding dynamics in different contexts. The framework thus serves as a meta-theory clarifying relationships among research programs that previously appeared disconnected or even contradictory.

### 9.2 Theoretical Implications

The consent-holding framework generates several significant implications for political philosophy, social choice theory, mechanism design, and democratic theory, each reshaping how we understand institutional legitimacy and political stability.

**For political philosophy**, the framework resolves the longstanding tension between consent-based and competence-based theories of legitimacy by showing them as complementary dimensions rather than competing foundations. Democratic theorists from Rousseau through Rawls have emphasized popular sovereignty, arguing that legitimate authority must derive from the consent of the governed regardless of governance quality. Technocratic critics counter that governance requires expertise, and including uninformed masses in complex decisions sacrifices outcome quality for procedural fairness. The consent-holding framework demonstrates this dichotomy is false: overall legitimacy L(d,t) = w₁·α(d,t) + w₂·P(d,t) combines consent alignment and performance through society-specific weights, with different domains optimally balancing these dimensions based on stakes distributions and uncertainty structures. Constitutional amendments affecting fundamental rights rationally prioritize consent (high w₁) because stakeholders possess extreme sᵢ regarding autonomy and inclusion, making voice non-negotiable even if expert consensus exists. Nuclear safety decisions rationally prioritize competence (high w₂) because technical expertise dramatically affects catastrophic risk, though some minimal α remains necessary for monitoring and accountability.

This formulation operationalizes Rawlsian fairness by ensuring decisions are accountable to those most affected, particularly vulnerable populations bearing greatest consequences of policy failure. When consent power Cᵢ is weighted by stakes sᵢ(d) and stakes correlate with vulnerability—those most harmed by bad governance typically have highest exposure—high α(d) ensures the worst-off aren't systematically excluded from decision-making over domains most affecting their welfare. The framework similarly operationalizes Habermasian legitimacy: the discourse principle that "only those norms can claim validity that could meet with the acceptance of all concerned in their capacity as participants in practical discourse" becomes measurable through α(d), with friction F(d) providing empirical validation of whether affected parties actually accept outcomes. Where Rawls and Habermas argue normatively about what legitimacy requires, the framework provides empirical tools for assessing whether actual institutions achieve it.

The framework also provides structural foundation beneath normative debates about justice, rights, and institutional design. Traditional moral theories—utilitarian, deontological, virtue-based—occupy the evaluation layer, prescribing how consent-holders should use their authority once it exists. Utilitarianism advocates maximizing aggregate welfare Σᵢ Uᵢ given H_t(d); deontology constrains permissible actions x_d through rights and duties; virtue ethics evaluates consent-holders' character and motivations. None of these approaches eliminates the necessity of consent-holding structures—they judge different uses of authority according to different moral criteria. The consent-holding framework operates at a more fundamental ontological layer, establishing that someone must hold decision authority wherever collective action occurs (Theorem 1) and providing tools for measuring consequences of any particular allocation regardless of which moral theory guides evaluation. This architecture allows productive engagement across moral traditions that historically talked past each other: instead of debating whether utilitarian or deontological principles should govern, we can first measure who currently holds consent, how aligned this allocation is with stakes, and what friction results, then evaluate these arrangements through whatever normative lens one endorses.

**For social choice theory**, the framework extends classical results from Arrow, Sen, and Gibbard-Satterthwaite by introducing stakes-weighting and domain-specificity while shifting from mechanism prescription to legitimacy measurement. Arrow's impossibility theorem demonstrates that no ranked voting system satisfies four seemingly minimal desiderata simultaneously—Pareto efficiency, non-dictatorship, independence of irrelevant alternatives, and unrestricted domain. This result assumes equal weighting of preferences; the consent-holding framework relaxes this assumption by weighting agents' influence by stakes sᵢ(d), enabling different aggregation rules to achieve higher legitimacy in different domains. Simple majority rule may be optimal for low-stakes routine legislation where minimizing decision costs matters most, while supermajority or consensus becomes appropriate when stakes concentrate heavily (constitutional amendments, community norms, existential decisions). The framework doesn't prescribe which rule is universally best—Arrow proves no such rule exists—but provides tools for measuring legitimacy and predicting friction under any mechanism.

Sen's capability approach maps directly onto effective voice eff_voiceᵢ(d): possessing formal consent power Cᵢ > 0 without resources, education, or political freedom represents low capability, reducing actual influence over decisions. Stakes measurement should reflect capability impacts beyond material exposure—a policy affecting health capabilities, educational opportunities, or political participation generates higher sᵢ(d) for those whose capabilities are most constrained. This integration enriches both frameworks: Sen's capabilities provide richer stakes measurement, while consent-holding operationalizes capability deficits as legitimacy problems measurable through α(d). The Gibbard-Satterthwaite impossibility result—that any non-dictatorial voting mechanism is strategically manipulable—raises concerns about stakes misrepresentation. Agents might exaggerate sᵢ(d) to capture disproportionate Cᵢ if stakes-weighting relies on self-reports. The framework addresses this through revealed preference measurement (behavioral and material proxies), observational verification (objective impact assessment), and diagnostic friction (high F(d) despite claimed high α(d) signals misweighting requiring correction).

The framework's domain-specificity represents a significant extension of social choice theory, which typically analyzes "voting rules" or "social welfare functions" without explicitly modeling how optimal mechanisms vary across decision contexts. By formalizing domains d as distinct decision spheres with heterogeneous stakes distributions, the framework enables systematic analysis of when different aggregation rules achieve legitimacy. Technical domains with concentrated expertise may rationally employ weighted voting where specialists receive higher Cᵢ reflecting their superior information, provided α(d) remains sufficiently high through stakeholder voice mechanisms preventing pure technocracy. Value-laden domains with distributed stakes may employ equal-weight democracy maximizing α through universal inclusion. The framework provides tools for identifying appropriate mechanisms empirically—estimate w₁ and w₂ by regressing friction F(d) on consent alignment α(d) and performance P(d), revealing societal weights on voice versus results, then design institutions optimizing the estimated legitimacy function.

**For mechanism design**, the framework introduces legitimacy as an explicit constraint on optimal mechanisms rather than assuming efficiency or incentive-compatibility alone determines institutional quality. Classical mechanism design asks: what rules maximize aggregate welfare, minimize manipulation, or achieve truthful revelation of private information? The Myerson-Satterthwaite theorem shows that efficient bilateral trade can't be guaranteed without subsidies; the revelation principle establishes that any equilibrium of any mechanism can be replicated through truthful direct revelation mechanisms; revenue equivalence demonstrates that different auction formats yield identical expected revenue under certain conditions. These results are fundamental but remain silent on legitimacy: a mechanism maximizing welfare or revenue while concentrating decision power among experts or algorithms may achieve high performance P(d) while generating low consent alignment α(d), producing friction F(d) despite technical optimality.

The consent-holding framework demonstrates that high performance is insufficient for institutional stability when consent alignment is low, provided society places non-zero weight on voice (w₁ > 0 in L = w₁·α + w₂·P). Colonial administrations frequently achieved moderately high P through infrastructure development, administrative order, and economic growth, yet generated enormous friction because α(d_governance) → 0 for indigenous populations with existential stakes in self-determination. Contemporary algorithmic systems exhibit similar dynamics: content moderation algorithms may achieve high accuracy (performance) at detecting policy violations while generating user backlash because affected populations have zero voice in rule-setting. The framework predicts such systems face instability requiring either consent incorporation (raising α through participatory algorithm design, stakeholder boards, appeals mechanisms) or suppression (maintaining concentrated control through platform monopoly, regulatory capture, or switching costs preventing exit).

This legitimacy constraint suggests mechanism designers should optimize L(d) = w₁·α(d) + w₂·P(d) rather than P(d) alone, accepting performance trade-offs when necessary to maintain stability through stakeholder voice. Practical implications include stakes-weighted voting as a compromise between equal suffrage and technocratic concentration, participatory algorithm design bringing affected communities into development processes, and accountability mechanisms enabling affected parties to contest decisions without full agenda control. Recent work on "fair machine learning" implicitly recognizes this constraint by defining fairness metrics incorporating distributional concerns beyond aggregate accuracy—prohibiting high error rates for protected groups even if overall performance would improve—but lacks the general framework connecting these fairness criteria to legitimacy theory. The consent-holding approach clarifies that such constraints represent α(d) requirements: ensuring high-stakes groups aren't systematically excluded from decision benefits or systematically subjected to decision harms maintains sufficient consent alignment for stable implementation.

**For democratic theory**, the framework explains both democratic resilience and authoritarian fragility while predicting emerging legitimacy crises in platform governance, artificial intelligence, and climate policy. Established democracies sustain stability despite periodic governance failures, corruption, or unpopular policies because high α(d) through universal suffrage and civil liberties generates tolerance for imperfect performance. When citizens possess meaningful voice, they attribute failures to temporary factors (incumbent incompetence, challenging circumstances) and retain confidence in systems' eventual self-correction through electoral accountability. This loyalty enables democracies to weather crises that would destabilize low-α regimes. Authoritarian systems exhibit the opposite dynamic: concentrated consent-holding produces low α despite potentially high P, making legitimacy dependent entirely on performance. When economic growth falters, external threats emerge, or succession crises loom, friction accumulates rapidly because citizens lack voice-based attachment to institutions. The framework formalizes Lipset's observation that democracies rich in legitimacy can survive poor performance while authoritarian systems require sustained performance to maintain stability.

The framework also predicts specific emerging legitimacy crises in domains where rapid technological or social change has outpaced institutional adaptation. Platform governance exhibits classic low-α dynamics: billions of users possess high stakes in content moderation, algorithmic curation, and data practices affecting speech, privacy, and psychological wellbeing, while decision authority concentrates in executives and algorithms producing α(d_platform) near zero. The framework predicts mounting friction through user protests, regulatory intervention, and platform exodus when alternatives emerge—all empirically validated through #DeleteFacebook movements, GDPR enforcement, and migration to Mastodon/Bluesky following Twitter's governance changes. Artificial intelligence governance faces similar challenges as consequential decisions (hiring, lending, healthcare allocation, criminal sentencing) increasingly rely on algorithmic systems where affected populations have zero voice in design, deployment, or oversight. The framework predicts instability requiring either consent incorporation through participatory AI governance or suppression through regulatory capture preventing public accountability.

Climate policy represents perhaps the most severe legitimacy challenge: future generations possess extreme stakes in emissions trajectories and adaptation investments (their survival depends on current decisions) yet hold zero consent power because they don't yet exist to vote or organize politically. This temporal misalignment produces α(d_climate) → 0 when calculated over full stakeholder sets including future people, predicting either catastrophic friction (civilizational collapse from climate breakdown) or innovative consent-holding mechanisms representing future interests (guardian models, youth councils, constitutional provisions). The framework clarifies why traditional democratic institutions struggle with long-term challenges—electoral cycles optimize for current voters, producing systematic bias against those with deferred stakes—and suggests institutional innovations like futures committees, intergenerational equity audits, or weighted voting for youth in climate domains to raise temporal α(d).

### 9.3 Limitations and Challenges

While the consent-holding framework provides significant analytical leverage, several limitations and unresolved challenges constrain its immediate applicability and require acknowledgment before discussing future research directions.

**Measurement challenges** pose the most significant practical obstacle to empirical implementation. Stakes sᵢ(d) are difficult to observe directly because they represent counterfactual sensitivity to policy variation rather than easily measured exposure. Proposed proxies—material impacts, capability effects, revealed preferences through political behavior—each introduce measurement error and potential endogeneity. Material exposure (tax burden, health risk, land threatened) captures tangible stakes but ignores psychological impacts, identity concerns, and dignity interests that may dominate material considerations in value-laden domains. Capability impacts following Sen require measuring how policies affect freedoms to achieve valued functionings, but identifying which functionings matter and how much constraint occurs demands normative judgments the framework aims to avoid. Revealed preference through political mobilization (protest participation, donation patterns, voting intensity) generates endogeneity because observed behavior depends on both underlying stakes and institutional access to voice—groups excluded from H_t(d) may under-mobilize due to rational despair rather than low stakes.

Consent power Cᵢ measurement faces similar challenges beyond simple voting contexts. Formal voting weights provide clear Cᵢ in one-person-one-vote systems, though even here power indices (Banzhaf, Shapley-Shubik) reveal that coalition dynamics produce divergence between nominal weights and actual influence. Corporate governance introduces further complexity: board seats give directors formal authority, but informal power through expertise, networks, or CEO relationships may matter more for actual decision influence. Algorithmic governance nearly defeats simple measurement—who holds "consent power" in content moderation domains where algorithms enforce rules designed by engineers implementing policies set by executives responding to advertiser pressure and regulatory constraints? The framework requires specifying H_t(d_moderation) across multiple nested levels, each with different consent distributions, before calculating overall α(d).

Effective voice eff_voiceᵢ compounds these measurement challenges by requiring assessment of capability constraints beyond formal rights. An agent with Cᵢ = 1/N in universal suffrage may have near-zero effective voice due to poverty (no time for political engagement), illiteracy (can't process policy information), or oppression (fear of political participation). Measuring such constraints demands detailed survey data or behavioral observation, and even sophisticated measurement misses subtle factors like internalized subordination reducing perceived entitlement to voice. Cross-national comparability becomes nearly impossible when institutional contexts, cultural norms, and baseline capabilities vary dramatically—does α(d) = 0.6 in Sweden represent the same legitimacy as α(d) = 0.6 in Singapore given radically different political cultures?

Endogeneity concerns complicate causal inference throughout empirical application. Does low consent alignment α(d) cause friction F(d), or does observed friction reveal previously latent low alignment? The framework assumes the former—institutional configurations determine legitimacy which generates stability outcomes—but reverse causality seems plausible. High friction from exogenous shocks (economic crisis, external conflict, leadership scandal) may erode perceived legitimacy even when institutional α(d) remains constant, because stakeholders attribute poor outcomes to the consent-holding structure itself. Simultaneous causality likely operates: low α generates friction which further reduces perceived legitimacy which intensifies friction in self-reinforcing spirals. Instrumental variables strategies using institutional reforms as exogenous variation in consent structures can address this, but valid instruments are scarce and identifying assumptions remain contestable.

**Scope conditions** constrain the framework's domain of applicability. The axioms assume agents capable of holding stakes and exercising preferences—humans clearly qualify, sophisticated AI systems may qualify in future, but what about animals, ecosystems, or future generations? Animals affected by agricultural policy possess stakes in a biological sense (their welfare depends on farming practices) but cannot hold or delegate consent in ways the framework models. Environmental domains raise similar challenges: ecosystems cannot hold preferences over conservation policy, yet their "stakes" seem normatively relevant. Future generations pose temporal versions of these challenges: unborn people will be affected by climate policy, making their interests ethically relevant, but they cannot currently exercise consent in H_t(d_climate). The framework's response—allowing guardians, proxy consent, or institutional representatives to channel such interests—works pragmatically but introduces normative judgments about legitimate representation that the structural approach aims to avoid.

Domain boundedness represents another scope condition. The framework assumes we can identify distinct domains d with specifiable stakeholder sets S_d, but real politics often exhibits contested boundaries and overlapping domains. Is immigration policy one domain or several? Arguably it comprises labor market domains (affecting workers' wages and employment), cultural domains (affecting national identity and community composition), fiscal domains (affecting tax burdens and public services), and security domains (affecting crime and terrorism risks), each with different stakeholder sets and stakes distributions. The framework's response—decompose contested "policies" into component domains with distinct H_t(d)—works analytically but may poorly describe actual political processes where bundling and log-rolling across domains determine outcomes. How should we calculate α(d) when a legislative package addresses multiple domains simultaneously and representatives vote on the bundle rather than components?

Temporal evolution is modeled simply through discrete-time transitions H_t(d) → H_t+1(d) responding to accumulated friction, but actual institutional change exhibits complex path dependence, threshold effects, and non-monotonic dynamics. The framework predicts that rising F(d) should trigger consent expansion raising α(d), yet history shows long periods where high friction persists without institutional adaptation, followed by rapid punctuated change once thresholds are crossed. Why do suffrage movements sometimes succeed quickly while other times require decades of sustained mobilization despite comparable friction levels? The framework currently lacks endogenous theory of institutional change, treating H_t(d) as exogenous then analyzing consequences rather than modeling how friction translates into consent structure reform. This limits predictive power regarding timing and form of institutional adaptation.

**Normative limitations** constrain the framework's prescriptive capacity. By design, consent-holding theory describes structural properties of decision-making systems rather than prescribing optimal institutions. This analytical modesty is simultaneously a strength—enabling application across diverse normative frameworks—and a limitation—providing no guidance about which consent allocation is "best." The framework can diagnose that α(d) = 0.3 generates higher friction than α(d) = 0.7, but cannot say whether society should prioritize raising α versus accepting friction to preserve other values (efficiency, expertise, cultural tradition). The legitimacy function L(d) = w₁·α + w₂·P makes this explicit: optimal institutional design depends on weights w₁, w₂ reflecting society-specific tradeoffs between voice and performance. But how should these weights be determined? The framework suggests empirical estimation through revealed preference (regressing friction on α and P), but this assumes existing institutions reveal legitimate preferences rather than path-dependent accidents or elite manipulation.

This normative limitation becomes acute when consent structures themselves are contested. The framework says to include agents with high sᵢ(d) in H_t(d), but what if current consent-holders deny that excluded groups have legitimate stakes? Antebellum slaveowners denied that enslaved people possessed stakes warranting voice in governance—their preferences were deemed irrelevant by those controlling H_t(d). The framework can retrospectively diagnose that α(d_slavery) → 0 because people with extreme stakes (enslaved populations facing life-or-death impacts) held zero consent power, and predict this generated friction (rebellion, escape, ultimately civil war). But the framework provides no independent normative argument that slavery was wrong—it simply measures alignment between stakes and consent then predicts consequences of misalignment. For those seeking moral condemnation of slavery rather than structural analysis, this neutrality proves unsatisfying.

The framework also cannot resolve deep value conflicts where stakeholders agree that α(d) matters but disagree fundamentally about which outcomes are legitimate. Consider abortion policy: pro-choice advocates argue that pregnant people hold extreme stakes (bodily autonomy, life trajectory) warranting decisive Cᵢ, while pro-life advocates argue that fetuses hold extreme stakes (life itself) warranting protection even if pregnant people disagree. Both positions invoke stakes-based legitimacy claims; the framework can clarify the structure of disagreement (who is counted in S_d, how are stakes measured, what consent power follows from stakes) but cannot resolve whose stakes count more absent external normative theory. The framework thus serves as analytical infrastructure for normative debates rather than replacement for substantive moral philosophy.

**Empirical testing gaps** constrain confidence in the framework's predictions pending systematic quantitative validation. The historical case studies in Section 7 provide narrative support for predicted dynamics—groups with high stakes and zero consent generate friction until incorporation or suppression—but these remain qualitative illustrations rather than rigorous econometric tests. No formal statistical validation has yet estimated α(d,t) and F(d,t) across a panel of countries, domains, and time periods, then tested predicted relationships controlling for confounds. Such analysis would require heroic data collection measuring consent power distributions, stakes allocations, and friction metrics across diverse institutional contexts with sufficient variation for identification. The Monte Carlo simulations in Section 6 demonstrate that stakes-weighted mechanisms outperform alternatives under specified preference distributions, but these results remain illustrative rather than fitted to real-world data.

Cross-national panel studies could test core predictions—does higher α(d) correlate with lower F(d) controlling for economic conditions, political culture, and external shocks? Does variation in α(d) explain differences in institutional stability beyond traditional measures like GDP per capita, regime type, or ethnic fractionalization? Within-country variation across policy domains enables tests of domain-specificity: do constitutional amendments indeed exhibit higher required α thresholds than routine legislation? Does climate policy exhibit lower temporal α due to future generation exclusion? Such tests require measuring α(d,t) systematically, which in turn requires solving the measurement challenges discussed above—operationalizing stakes, consent power, and effective voice across diverse contexts with comparable metrics.

Quasi-experimental designs exploiting institutional reforms could enable causal identification. When voting rights expand, does observed friction decline in subsequent periods? When labor codetermination mandates increase worker board representation, do workplace strikes and litigation rates fall? When platforms introduce participatory governance mechanisms, does user satisfaction and retention improve? Natural experiments are rare but valuable: countries adopting citizens' assemblies for climate policy provide treatment variation in α(d_climate); corporate governance reforms mandating stakeholder representation provide treatment variation in α(d_corporate); platform policy changes provide treatment variation in α(d_platform). Difference-in-differences, synthetic control methods, or regression discontinuity designs could identify causal effects if reforms exhibit sufficient exogeneity.

**Plutocracy risk** represents a persistent concern despite safeguards proposed in Objection 8. If stakes-weighting means those affected more should have greater voice, and wealth affects how much policies impact wellbeing, do rich people legitimately deserve disproportionate Cᵢ? A billionaire losing 10% of wealth to taxation might suffer absolute dollar losses exceeding a poor person's entire wealth, seemingly warranting higher stakes measurement. The framework's response—measure stakes relative to capability baselines, use concave transformations preventing wealth from dominating, and focus on capability impacts rather than monetary losses—addresses this theoretically but implementation remains challenging. Wealthy elites possess superior capacity to manipulate stakes measurement through accounting techniques, political influence over definition of metrics, and legal strategies maximizing claimed exposure. Even "revealed preference" approaches face gaming: the rich can afford expensive campaigns exaggerating regulatory burdens, while the poor lack resources for comparable mobilization despite equivalent stakes.

This concern becomes especially acute in domains where wealth concentration is extreme. If the top 1% hold 40% of wealth, even carefully designed stakes-weighting might grant them 10-20% of consent power in economic policy domains, far exceeding their population share. Is this legitimate or plutocratic? The framework cannot answer without specifying threshold α beyond which concentrated wealth threatens legitimacy—but determining such thresholds requires normative commitments about acceptable inequality that the structural approach cannot provide endogenously. Vigilance in institutional design remains necessary: progressive weighting schemes, floors on minimal voice regardless of stakes, domain restrictions preventing wealth-stakes from dominating all decisions, and democratic oversight of measurement procedures all help mitigate plutocracy risk. But the fundamental tension between stakes-weighting and wealth inequality persists as a challenge requiring ongoing attention.

### 9.4 Future Research Directions

The consent-holding framework opens multiple research agendas spanning empirical validation, theoretical extension, domain application, and normative development. These directions range from immediate next steps using existing data and methods to long-term research programs requiring substantial methodological innovation.

**Empirical agenda**: The most pressing research priority is systematic empirical validation of core predictions through quantitative analysis beyond the qualitative historical cases presented in Section 7. A cross-national panel study measuring consent alignment α(d,t) and friction F(d,t) across countries, policy domains, and time periods would test the framework's central claim that misalignment generates instability. Such a study requires several components. First, construct α(d,t) measures using institutional data on consent power distributions (voting rules, board composition, regulatory authority, algorithmic control) and stakes proxies (survey data on perceived impacts, material exposure measures, capability assessments). Second, construct F(d,t) measures using protest event data (count and intensity of demonstrations, strikes, riots), non-compliance metrics (tax evasion rates, regulatory violation frequencies, litigation rates), and institutional instability indicators (policy reversals, leadership turnover, regime breakdown). Third, estimate panel regressions testing predictions: H1 that α(d,t) correlates negatively with F(d,t) controlling for economic conditions, external shocks, and unobserved heterogeneity; H2 that changes in α(d,t) predict subsequent changes in F(d,t+k) with measurable lags reflecting institutional adjustment; H3 that the relationship varies across domains with higher sensitivity in high-stakes value domains than low-stakes technical domains.

Quasi-experimental designs exploiting institutional reforms as natural experiments would enable stronger causal identification than cross-sectional or panel correlations. Several promising opportunities exist. Voting rights expansions—including suffrage for women, racial minorities, immigrants, or youth—provide temporal variation in α(d_electoral) within countries, allowing difference-in-differences estimation comparing political friction before and after reform relative to control countries without expansions. Corporate governance mandates requiring worker board representation (German codetermination, Nordic stakeholder models, proposed EU directives) provide cross-country and temporal variation in α(d_corporate), enabling tests of whether increased worker voice reduces labor friction (strikes, turnover, conflicts) while maintaining performance. Platform policy changes introducing participatory governance mechanisms (Facebook's Oversight Board, YouTube Creator Councils, Reddit's moderator system) provide treatment variation in α(d_platform), allowing tests of user satisfaction, retention, and compliance changes.

Survey instruments measuring perceived legitimacy could validate that structural α(d) correlates with subjective acceptance. Existing surveys ask about institutional trust or democratic satisfaction, but these confound legitimacy with performance and ideology. The framework suggests alternative question formats: "How much voice do people like you have in [domain] decisions compared to how much those decisions affect you?" directly elicits perceived α(d). "How often do [domain] decisions go against what people like you want?" approximates friction F(d). Cross-validating such subjective measures against institutional α(d,t) calculations would test whether structural alignment tracks normative acceptance, a key assumption of legitimacy theory. If subjective and objective measures diverge systematically—say, high institutional α but low perceived legitimacy—this signals measurement error, framing effects, or misinformation requiring investigation.

**Theoretical extensions**: Several theoretical developments would enhance the framework's analytical power and address current limitations. Dynamic models of institutional evolution would endogenize consent structure changes currently treated as exogenous. How does accumulated friction F(d) translate into pressure for consent expansion? Under what conditions do elites respond through incorporation (raising α) versus suppression (maintaining concentrated consent through coercion)? Formal models could specify how rising F(d) increases expected costs of suppression relative to incorporation, generating threshold effects where gradual friction accumulation triggers rapid institutional change once costs cross. Such models might explain punctuated equilibrium patterns in democratic transitions—long periods of authoritarian stability followed by rapid liberalization—as emergent properties of friction dynamics rather than exogenous shocks.

Network structure extensions would model agents embedded in social networks rather than isolated individuals. An agent's stakes sᵢ(d) might depend partly on network position—how many connections are affected by decisions—while consent power Cᵢ might depend on centrality enabling coalition formation. Network diffusion models could show how friction spreads: individuals with high sᵢ(d) and zero Cᵢ mobilize their networks, generating cascades that amplify F(d) beyond direct stakeholders. This would formalize how excluded groups build coalitions with sympathetic allies possessing consent power, a key mechanism in historical suffrage and civil rights movements. Graph-theoretic tools could identify structural positions maximizing influence over consent-holding evolution.

Incomplete information extensions would model realistic uncertainty about stakes distributions, preference intensities, and consent power allocations. Perfect information assumptions—all agents know all sᵢ(d), x*ᵢ,d, and Cᵢ—rarely hold in practice. Elites may genuinely not know excluded groups' stakes, while subordinated populations may not know their own potential political power until mobilization reveals latent strength. Bayesian updating models could show how friction signals stakes-consent misalignment, causing consent-holders to learn α(d) is lower than believed and triggering reforms. Signaling models could examine how excluded groups communicate stakes intensity to persuade consent-holders that incorporation is necessary, or how elites manipulate perceived stakes to justify concentrated control.

Behavioral micro-foundations would relax full rationality assumptions, incorporating bounded rationality, prospect theory, and reference-dependent preferences into consent-holding dynamics. Real agents exhibit loss aversion, status quo bias, and framing effects that affect both stake perception and friction generation. Prospect theory suggests that agents weight losses more heavily than gains, potentially amplifying friction F(d) when outcomes fall below reference points even if absolute welfare remains high. Reference-dependent preferences mean that rising α(d) might reset expectations upward, generating friction if subsequent policies fail to maintain participation levels—a ratchet effect explaining why democratic backsliding generates intense resistance even when returning to previously accepted α levels. Such behavioral extensions would improve descriptive accuracy while maintaining tractable analytical structure.

**Domain applications**: Applying the framework to emerging governance challenges would demonstrate practical value while identifying domain-specific measurement issues requiring methodological innovation. Algorithmic governance represents the most pressing application: as consequential decisions increasingly rely on automated systems, developing participatory AI mechanisms maintaining α(d_algorithm) > τ becomes critical for social stability. The framework prescribes specific interventions: participatory algorithm design bringing affected communities into fairness criterion selection and training data curation; algorithmic impact assessments with public consultation before high-stakes deployment; human oversight proportional to decision stakes (more human review for criminal sentencing than music recommendations); contestability through explanation rights and appeals processes. Empirical research could evaluate such interventions: do participatory design processes raise perceived legitimacy? Do impact assessments reduce friction through stakeholder incorporation? Do appeals mechanisms reduce user resistance even when outcome rates remain constant?

Climate institutions present unique challenges due to temporal misalignment: future generations possess extreme stakes but zero current consent power. The framework clarifies that conventional democratic institutions produce α(d_climate) → 0 when stakeholder sets include future people, predicting catastrophic friction (civilizational collapse) or institutional innovation. Potential consent-holding mechanisms include futures committees (dedicated bodies representing long-term interests with veto power over shortsighted policies), intergenerational equity audits (requiring justification when policies burden future generations), constitutional climate provisions (embedding obligations to future people in fundamental law immune to electoral cycles), or weighted youth voting in climate domains (giving younger cohorts with longer time horizons greater Cᵢ). Comparative institutional analysis could evaluate which mechanisms successfully raise temporal α while maintaining sufficient present-generation voice for political sustainability.

Platform cooperatives offer alternative organizational forms attempting to raise α(d_platform) through worker and user ownership. Unlike conventional platforms where consent concentrates in executives and investors, cooperatives distribute Cᵢ to stakeholders through democratic governance (worker co-ops), multi-stakeholder boards (platform co-ops including users, workers, and investors), or hybrid models. The framework predicts such structures should exhibit lower friction than conventional platforms if governance actually empowers high-stakes agents. Empirical research could test this: do platform cooperatives experience lower user churn, fewer labor conflicts, less regulatory pressure than comparable conventional platforms? If not, why—is α(d) actually lower than formal governance suggests due to elite capture, or is friction driven by factors beyond consent alignment?

Global governance institutions—UN, WTO, IMF, World Bank, climate agreements—exhibit severe legitimacy deficits diagnosable through α(d_global). Decisions affecting billions concentrate consent in handful of powerful states, producing low stakes-weighted alignment especially for Global South populations with high sᵢ(d) but minimal Cᵢ. The framework predicts such institutions face mounting friction through non-compliance (Paris Agreement enforcement gaps), exit threats (Brexit, potential US withdrawals), and alternative institution formation (BRICS development bank, regional trade agreements). Institutional reforms raising α(d) might include weighted voting by population rather than state (though sovereignty concerns limit feasibility), stakeholder representation for affected non-state actors, or polycentric governance enabling local adaptation while maintaining coordination benefits. Comparative institutional analysis across global governance domains could identify which reforms successfully expand voice without paralyzing decision-making.

**Normative development**: While the framework intentionally avoids prescribing optimal consent allocations, several normative questions require attention for practical application. Optimal stake measurement remains contested: should stakes reflect material exposure, capability impacts, subjective intensity, or some weighted combination? Different measurement approaches generate different α(d) calculations and thus different legitimacy assessments. Material exposure seems most objective but ignores psychological and dignity interests; capability impacts capture broader welfare effects but require normative judgments about which capabilities matter; subjective intensity respects agent assessments but enables strategic exaggeration. Normative analysis could examine trade-offs, perhaps arguing that domain characteristics determine appropriate measurement—material stakes for economic policy, capability stakes for health and education policy, subjective stakes for cultural and identity domains.

Threshold calibration questions how to determine society-specific legitimacy thresholds τ separating acceptable from problematic consent alignment. The empirical observation that τ ≈ 0.6 appears historically, but should this guide institutional design? Perhaps τ should vary across domains (higher for constitutional amendments than routine legislation) or across societies (higher in individualist cultures valuing autonomy, lower in collectivist cultures prioritizing harmony). Normative analysis could develop principled arguments for threshold setting: capability-based approaches might set τ to ensure minimum effective voice for all stakeholders; stability-based approaches might set τ to minimize expected friction costs; fairness-based approaches might require α above population-weighted equality. Empirical research identifying τ through revealed preference complements but cannot replace normative argument about what thresholds should be.

Progressive weighting questions how to structure stakes-to-consent transformations preventing plutocracy while respecting differential impacts. Linear weighting Cᵢ ∝ sᵢ(d) grants proportional influence, but this might concentrate power excessively when stakes distributions are extremely unequal. Concave transformations like Cᵢ ∝ √sᵢ(d) or Cᵢ ∝ log(sᵢ(d)) compress influence disparities, ensuring those with twice the stakes don't receive twice the consent power. Normative analysis could examine which transformations achieve legitimate balance between stakes-responsiveness and power concentration, perhaps drawing on inequality measurement (Gini coefficients for consent power distributions) or social welfare functions (prioritarian weighting favoring worse-off stakeholders).

Rights integration questions how fundamental rights constrain permissible Cᵢ distributions even when optimizing α(d). Liberal political philosophy holds that certain rights are inalienable—freedom of conscience, bodily autonomy, basic security—regardless of aggregate welfare or consent alignment. The framework currently treats rights as external constraints on feasible x_d rather than integral to legitimacy measurement, but this may be inadequate. Perhaps rights should enter directly: violations reduce effective voice for affected parties (making eff_voiceᵢ → 0 when rights are denied), or rights-respecting consent structures receive lexicographic priority over optimization (satisfying rights thresholds before maximizing α). Normative development could integrate deontological constraints while preserving the framework's structural approach to legitimacy measurement.

### 9.5 Concluding Reflection

The consent-holding framework offers political theory what thermodynamics offered physics: not a substantive claim about which outcomes are desirable, but a structural analysis of what arrangements are possible and which generate instability. Just as thermodynamics does not prescribe what machines engineers should build but predicts which designs will fail to conserve energy, consent-holding theory does not prescribe ideal governance but predicts which configurations generate friction requiring either reform or suppression. This analogy clarifies the framework's intellectual contribution and practical value while highlighting important disanalogies that constrain direct application.

Thermodynamics established that certain physical processes are impossible—perpetual motion machines violate energy conservation, perfect efficiency violates entropy increase—without specifying which possible designs are optimal. Similarly, consent-holding theory establishes that certain institutional arrangements are unstable—concentrating decision power among those unaffected by outcomes generates friction wherever stakes-bearing populations can mobilize—without specifying which stable arrangements are most just, efficient, or desirable. The framework provides diagnostic tools rather than prescriptive blueprints: measure who holds consent Cᵢ, who bears stakes sᵢ(d), calculate alignment α(d), predict friction F(d), then design institutions that achieve sufficient legitimacy given society-specific weights w₁, w₂ on voice and performance.

For practitioners—policymakers designing governance institutions, corporate boards structuring stakeholder relations, platform designers building algorithmic systems—the framework provides actionable guidance despite its analytical modesty. Low α(d) combined with high stakes dispersion predicts instability requiring preventive action: either expand consent-holding to incorporate high-stakes populations, or prepare for friction through monitoring, responsiveness, and adaptive capacity. High F(d) signals existing consent misalignment requiring diagnosis: identify excluded groups with high sᵢ(d), design voice mechanisms raising their eff_voiceᵢ, monitor whether friction declines following institutional adjustment. Performance gains through concentrated expertise cannot indefinitely substitute for stakeholder voice when w₁ > 0, meaning technocratic solutions face inherent legitimacy limits unless designed to preserve meaningful participation.

For researchers, the framework offers unified analytical language across domains that political theory has traditionally analyzed separately. Democratic theorists study electoral systems, corporate governance scholars study stakeholder capitalism, environmental scholars study commons management, and platform governance researchers study algorithmic accountability—yet all address fundamentally similar questions about who should decide when decisions affect multiple parties unequally. The consent-holding framework demonstrates that these are special cases of a general problem: how to allocate decision authority Cᵢ among agents with heterogeneous stakes sᵢ(d) to achieve legitimacy L combining consent alignment α and performance P through weights w₁, w₂ reflecting normative priorities. This generalization enables knowledge transfer across domains: lessons from commons governance about polycentric nested consent-holding apply to platform governance; insights from labor codetermination about stakeholder voice apply to algorithmic fairness; discoveries about deliberative democracy apply to corporate boards.

The framework's larger significance lies in making consent-holding structures visible, measurable, and comparable across contexts that obscure such comparisons when analyzed through domain-specific lenses. Every institutional arrangement—from hunter-gatherer band consensus to parliamentary democracy to algorithmic content moderation—instantiates some distribution of consent over decision domains. These distributions are not normatively neutral background conditions but consequential structural features generating legitimacy or friction depending on alignment with stakes. Understanding consent-holding distributions, measuring their correspondence to impact distributions, and predicting friction from misalignment may be the most fundamental task of political analysis, whether conducted by democratic theorists assessing electoral systems, organizational scholars evaluating corporate governance, or computer scientists designing automated decision-making platforms.

Consent-holding is not optional. The question facing any institution is never whether someone holds decision authority, but who holds it, how authority is allocated across domains, and whether allocation aligns sufficiently with stakes to sustain stability given society's tolerance for friction. Hierarchical organizations concentrate consent in executives; democratic polities distribute it through suffrage; algorithmic systems concentrate it in designers and code; anarchist communes pursue consensus. Each approach produces different α(d) depending on how well Cᵢ tracks sᵢ(d), and each generates different F(d) depending on how far outcomes deviate from stakeholder ideal points. The framework cannot adjudicate which arrangement is morally superior—utilitarians favor whatever maximizes welfare, deontologists favor whatever respects rights, virtue ethicists favor whatever exemplifies good character—but it can measure consequences of any arrangement through empirically observable legitimacy and friction.

This structural approach proves especially valuable in pluralistic societies where substantive moral consensus remains elusive. When citizens disagree deeply about justice, the good life, or legitimate authority—reasonable disagreements that persist even among informed, rational, good-faith interlocutors—structural analysis provides common ground. We may disagree about whether expanding suffrage to non-citizens is just, but we can measure whether doing so would raise α(d_electoral) and reduce F(d) through decreased political alienation. We may disagree about whether worker board representation is fair, but we can test whether codetermination achieves higher α(d_workplace) and lower friction than shareholder primacy. We may disagree about whether algorithmic content moderation should prioritize free speech or user safety, but we can measure whether participatory design raises α(d_moderation) by incorporating affected communities. Structural analysis does not resolve value conflicts, but it clarifies their implications, enabling empirically informed choices rather than purely philosophical debate.

The consent-holding framework thus serves democratic deliberation by providing measurement tools that make legitimacy discussable across value frameworks. Rather than arguing abstractly about whether democracy is intrinsically valuable or merely instrumentally useful, we can measure α(d) in existing institutions, test whether higher alignment reduces friction, and design reforms targeting specific legitimacy deficits. Rather than debating whether stakeholder capitalism or shareholder primacy is ethically superior, we can calculate α(d_corporate) under different governance models and evaluate trade-offs between voice and performance empirically. Rather than asserting that algorithmic governance threatens democracy or dismissing such concerns as Luddite, we can operationalize legitimacy in automated systems and test which institutional safeguards successfully maintain α above stability thresholds.

The framework's final contribution may be its demonstration that legitimacy measurement is possible—not easy, not uncontroversial, but tractable through systematic operationalization and empirical validation. Political philosophy has debated consent, representation, and democratic authority for centuries while lacking agreed metrics for assessing whether institutions achieve legitimacy in practice. Public opinion surveys measure subjective attitudes but confound legitimacy with performance and ideology. Regime classifications measure formal institutions but miss effective power distributions. The consent-holding framework proposes that legitimacy is stakes-weighted consent alignment: α(d,t) = (Σᵢ∈S_d sᵢ(d)·eff_voiceᵢ(d,t))/(Σᵢ∈S_d sᵢ(d)). This formula is not perfect—measurement challenges abound, normative judgments remain necessary, domain boundaries prove contested—but it provides a starting point for rigorous empirical analysis previously impossible.

As algorithmic governance expands, climate crisis intensifies, and platform power concentrates, understanding consent-holding dynamics becomes increasingly urgent. Institutions making consequential decisions affecting billions must grapple with legitimacy deficits when authority concentrates among those bearing minimal consequences while excluding those most affected. The consent-holding framework provides tools for diagnosing such deficits, predicting resulting friction, and designing institutional reforms that align decision power with stakes bearing. Whether such reforms actually occur depends on political will, mobilization capacity, and resistance from entrenched elites—factors beyond analytical frameworks. But clear diagnosis of legitimacy problems, precise measurement of misalignment, and rigorous prediction of consequences may help reformers build coalitions, persuade opponents, and design institutions sustaining both stability and justice.

Consent-holding theory provides the analytical architecture for this work. The framework establishes that someone always holds authority wherever decisions produce shared outcomes, that legitimacy measures how well authority aligns with consequences, and that persistent misalignment generates friction requiring institutional response. These structural insights apply across democratic, technocratic, and algorithmic governance, across suffrage movements and labor organizing and climate justice, across corporate boards and platform moderation and constitutional design. Understanding consent-holding—who decides, for whom, and with what consequences—may be the fundamental question of political analysis. This framework provides tools to answer it rigorously.
